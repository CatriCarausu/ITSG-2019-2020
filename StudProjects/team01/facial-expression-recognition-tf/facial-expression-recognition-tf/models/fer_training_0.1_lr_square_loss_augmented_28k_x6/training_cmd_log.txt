PS C:\Users\Bogdan\Documents\GitHub\Intelligent-Tools-for-Social-Good\facial-expression-recognition-tf\facial-expression-recognition-tf> python .\tf_model.py                                                                                   Using TensorFlow backend.
2019-12-08 03:01:13.558879: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
C:\Users\Bogdan\Documents\GitHub\Intelligent-Tools-for-Social-Good\facial-expression-recognition-tf\facial-expression-recognition-tf\data_loader.py:25: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.
  emotions = pd.get_dummies(data['emotion']).as_matrix()
WARNING:tensorflow:From C:\Users\Bogdan\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\ops\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2019-12-08 03:03:35.694739: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2019-12-08 03:03:36.219174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:
name: Quadro M520 major: 5 minor: 0 memoryClockRate(GHz): 1.176
pciBusID: 0000:02:00.0
2019-12-08 03:03:36.225927: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2019-12-08 03:03:36.233882: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll
2019-12-08 03:03:36.242185: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_100.dll
2019-12-08 03:03:36.249341: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_100.dll
2019-12-08 03:03:36.257890: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_100.dll
2019-12-08 03:03:36.265945: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_100.dll
2019-12-08 03:03:36.292634: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2019-12-08 03:03:36.299277: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-12-08 03:03:36.302772: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2019-12-08 03:03:36.309731: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:
name: Quadro M520 major: 5 minor: 0 memoryClockRate(GHz): 1.176
pciBusID: 0000:02:00.0
2019-12-08 03:03:36.316079: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2019-12-08 03:03:36.320062: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll
2019-12-08 03:03:36.324028: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_100.dll
2019-12-08 03:03:36.328569: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_100.dll
2019-12-08 03:03:36.333083: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_100.dll
2019-12-08 03:03:36.337515: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_100.dll
2019-12-08 03:03:36.342092: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2019-12-08 03:03:36.352371: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-12-08 03:03:37.657369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-12-08 03:03:37.661773: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0
2019-12-08 03:03:37.664773: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N
2019-12-08 03:03:37.668287: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1384 MB memory) -> physical GPU (device: 0, name: Quadro M520, pci bus id: 0000:02:00.0, compute capability: 5.0)
WARNING:tensorflow:From C:\Users\Bogdan\AppData\Local\Programs\Python\Python37\lib\site-packages\keras\backend\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.

Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, 48, 48, 1)    0
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 46, 46, 8)    72          input_1[0][0]
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 46, 46, 8)    32          conv2d_1[0][0]
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 46, 46, 8)    0           batch_normalization_1[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 44, 44, 8)    576         activation_1[0][0]
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 44, 44, 8)    32          conv2d_2[0][0]
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 44, 44, 8)    0           batch_normalization_2[0][0]
__________________________________________________________________________________________________
separable_conv2d_1 (SeparableCo (None, 44, 44, 16)   200         activation_2[0][0]
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 44, 44, 16)   64          separable_conv2d_1[0][0]
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 44, 44, 16)   0           batch_normalization_4[0][0]
__________________________________________________________________________________________________
separable_conv2d_2 (SeparableCo (None, 44, 44, 16)   400         activation_3[0][0]
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 44, 44, 16)   64          separable_conv2d_2[0][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 22, 22, 16)   128         activation_2[0][0]
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 22, 22, 16)   0           batch_normalization_5[0][0]
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 22, 22, 16)   64          conv2d_3[0][0]
__________________________________________________________________________________________________
add_1 (Add)                     (None, 22, 22, 16)   0           max_pooling2d_1[0][0]
                                                                 batch_normalization_3[0][0]
__________________________________________________________________________________________________
separable_conv2d_3 (SeparableCo (None, 22, 22, 32)   656         add_1[0][0]
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 22, 22, 32)   128         separable_conv2d_3[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 22, 22, 32)   0           batch_normalization_7[0][0]
__________________________________________________________________________________________________
separable_conv2d_4 (SeparableCo (None, 22, 22, 32)   1312        activation_4[0][0]
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 22, 22, 32)   128         separable_conv2d_4[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 11, 11, 32)   512         add_1[0][0]
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 11, 11, 32)   0           batch_normalization_8[0][0]
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 11, 11, 32)   128         conv2d_4[0][0]
__________________________________________________________________________________________________
add_2 (Add)                     (None, 11, 11, 32)   0           max_pooling2d_2[0][0]
                                                                 batch_normalization_6[0][0]
__________________________________________________________________________________________________
separable_conv2d_5 (SeparableCo (None, 11, 11, 64)   2336        add_2[0][0]
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 11, 11, 64)   256         separable_conv2d_5[0][0]
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 11, 11, 64)   0           batch_normalization_10[0][0]
__________________________________________________________________________________________________
separable_conv2d_6 (SeparableCo (None, 11, 11, 64)   4672        activation_5[0][0]
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 11, 11, 64)   256         separable_conv2d_6[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 6, 6, 64)     2048        add_2[0][0]
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 6, 6, 64)     0           batch_normalization_11[0][0]
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 6, 6, 64)     256         conv2d_5[0][0]
__________________________________________________________________________________________________
add_3 (Add)                     (None, 6, 6, 64)     0           max_pooling2d_3[0][0]
                                                                 batch_normalization_9[0][0]
__________________________________________________________________________________________________
separable_conv2d_7 (SeparableCo (None, 6, 6, 128)    8768        add_3[0][0]
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 6, 6, 128)    512         separable_conv2d_7[0][0]
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 6, 6, 128)    0           batch_normalization_13[0][0]
__________________________________________________________________________________________________
separable_conv2d_8 (SeparableCo (None, 6, 6, 128)    17536       activation_6[0][0]
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 6, 6, 128)    512         separable_conv2d_8[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 3, 3, 128)    8192        add_3[0][0]
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 3, 3, 128)    0           batch_normalization_14[0][0]
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 3, 3, 128)    512         conv2d_6[0][0]
__________________________________________________________________________________________________
add_4 (Add)                     (None, 3, 3, 128)    0           max_pooling2d_4[0][0]
                                                                 batch_normalization_12[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 3, 3, 7)      8071        add_4[0][0]
__________________________________________________________________________________________________
global_average_pooling2d_1 (Glo (None, 7)            0           conv2d_7[0][0]
__________________________________________________________________________________________________
predictions (Activation)        (None, 7)            0           global_average_pooling2d_1[0][0]
==================================================================================================
Total params: 58,423
Trainable params: 56,951
Non-trainable params: 1,472
__________________________________________________________________________________________________
WARNING:tensorflow:From C:\Users\Bogdan\AppData\Local\Programs\Python\Python37\lib\site-packages\keras\backend\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

Epoch 1/110
2019-12-08 03:03:42.470035: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2019-12-08 03:03:43.440823: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Internal: Invoking ptxas not supported on Windows
Relying on driver to perform ptx compilation. This message will be only logged once.
2019-12-08 03:03:43.497444: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll
4307/4306 [==============================] - 307s 71ms/step - loss: 0.1094 - accuracy: 0.3760 - val_loss: 0.0955 - val_accuracy: 0.4645

Epoch 00001: val_loss improved from inf to 0.09550, saving model to models/_mini_XCEPTION.01-0.46.hdf5
Epoch 2/110
4307/4306 [==============================] - 303s 70ms/step - loss: 0.0938 - accuracy: 0.4768 - val_loss: 0.0910 - val_accuracy: 0.4938

Epoch 00002: val_loss improved from 0.09550 to 0.09097, saving model to models/_mini_XCEPTION.02-0.49.hdf5
Epoch 3/110
4307/4306 [==============================] - 302s 70ms/step - loss: 0.0892 - accuracy: 0.5074 - val_loss: 0.0874 - val_accuracy: 0.5208

Epoch 00003: val_loss improved from 0.09097 to 0.08740, saving model to models/_mini_XCEPTION.03-0.52.hdf5
Epoch 4/110
4307/4306 [==============================] - 303s 70ms/step - loss: 0.0864 - accuracy: 0.5263 - val_loss: 0.0858 - val_accuracy: 0.5281

Epoch 00004: val_loss improved from 0.08740 to 0.08583, saving model to models/_mini_XCEPTION.04-0.53.hdf5
Epoch 5/110
4307/4306 [==============================] - 304s 71ms/step - loss: 0.0847 - accuracy: 0.5374 - val_loss: 0.0838 - val_accuracy: 0.5398

Epoch 00005: val_loss improved from 0.08583 to 0.08382, saving model to models/_mini_XCEPTION.05-0.54.hdf5
Epoch 6/110
4307/4306 [==============================] - 305s 71ms/step - loss: 0.0832 - accuracy: 0.5467 - val_loss: 0.0809 - val_accuracy: 0.5604

Epoch 00006: val_loss improved from 0.08382 to 0.08095, saving model to models/_mini_XCEPTION.06-0.56.hdf5
Epoch 7/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0820 - accuracy: 0.5560 - val_loss: 0.0798 - val_accuracy: 0.5683

Epoch 00007: val_loss improved from 0.08095 to 0.07976, saving model to models/_mini_XCEPTION.07-0.57.hdf5
Epoch 8/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0811 - accuracy: 0.5622 - val_loss: 0.0799 - val_accuracy: 0.5700

Epoch 00008: val_loss did not improve from 0.07976
Epoch 9/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0802 - accuracy: 0.5672 - val_loss: 0.0810 - val_accuracy: 0.5612

Epoch 00009: val_loss did not improve from 0.07976
Epoch 10/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0799 - accuracy: 0.5688 - val_loss: 0.0798 - val_accuracy: 0.5715

Epoch 00010: val_loss improved from 0.07976 to 0.07975, saving model to models/_mini_XCEPTION.10-0.57.hdf5
Epoch 11/110
4307/4306 [==============================] - 307s 71ms/step - loss: 0.0791 - accuracy: 0.5746 - val_loss: 0.0794 - val_accuracy: 0.5723

Epoch 00011: val_loss improved from 0.07975 to 0.07944, saving model to models/_mini_XCEPTION.11-0.57.hdf5
Epoch 12/110
4307/4306 [==============================] - 305s 71ms/step - loss: 0.0785 - accuracy: 0.5789 - val_loss: 0.0777 - val_accuracy: 0.5876

Epoch 00012: val_loss improved from 0.07944 to 0.07769, saving model to models/_mini_XCEPTION.12-0.59.hdf5
Epoch 13/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0780 - accuracy: 0.5830 - val_loss: 0.0804 - val_accuracy: 0.5715

Epoch 00013: val_loss did not improve from 0.07769
Epoch 14/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0775 - accuracy: 0.5860 - val_loss: 0.0780 - val_accuracy: 0.5816

Epoch 00014: val_loss did not improve from 0.07769
Epoch 15/110
4307/4306 [==============================] - 305s 71ms/step - loss: 0.0771 - accuracy: 0.5889 - val_loss: 0.0772 - val_accuracy: 0.5881

Epoch 00015: val_loss improved from 0.07769 to 0.07719, saving model to models/_mini_XCEPTION.15-0.59.hdf5
Epoch 16/110
4307/4306 [==============================] - 305s 71ms/step - loss: 0.0767 - accuracy: 0.5912 - val_loss: 0.0765 - val_accuracy: 0.5957

Epoch 00016: val_loss improved from 0.07719 to 0.07652, saving model to models/_mini_XCEPTION.16-0.60.hdf5
Epoch 17/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0762 - accuracy: 0.5944 - val_loss: 0.0847 - val_accuracy: 0.5429

Epoch 00017: val_loss did not improve from 0.07652
Epoch 18/110
4307/4306 [==============================] - 305s 71ms/step - loss: 0.0760 - accuracy: 0.5968 - val_loss: 0.0799 - val_accuracy: 0.5769

Epoch 00018: val_loss did not improve from 0.07652
Epoch 19/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0756 - accuracy: 0.5986 - val_loss: 0.0821 - val_accuracy: 0.5613

Epoch 00019: val_loss did not improve from 0.07652
Epoch 20/110
4307/4306 [==============================] - 305s 71ms/step - loss: 0.0754 - accuracy: 0.5993 - val_loss: 0.0763 - val_accuracy: 0.5950

Epoch 00020: val_loss improved from 0.07652 to 0.07629, saving model to models/_mini_XCEPTION.20-0.59.hdf5
Epoch 21/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0752 - accuracy: 0.6008 - val_loss: 0.0756 - val_accuracy: 0.5995

Epoch 00021: val_loss improved from 0.07629 to 0.07557, saving model to models/_mini_XCEPTION.21-0.60.hdf5
Epoch 22/110
4307/4306 [==============================] - 311s 72ms/step - loss: 0.0748 - accuracy: 0.6039 - val_loss: 0.0786 - val_accuracy: 0.5814

Epoch 00022: val_loss did not improve from 0.07557
Epoch 23/110
4307/4306 [==============================] - 307s 71ms/step - loss: 0.0745 - accuracy: 0.6058 - val_loss: 0.0755 - val_accuracy: 0.5977

Epoch 00023: val_loss improved from 0.07557 to 0.07551, saving model to models/_mini_XCEPTION.23-0.60.hdf5
Epoch 24/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0743 - accuracy: 0.6087 - val_loss: 0.0729 - val_accuracy: 0.6160

Epoch 00024: val_loss improved from 0.07551 to 0.07290, saving model to models/_mini_XCEPTION.24-0.62.hdf5
Epoch 25/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0741 - accuracy: 0.6087 - val_loss: 0.0765 - val_accuracy: 0.5897

Epoch 00025: val_loss did not improve from 0.07290
Epoch 26/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0740 - accuracy: 0.6091 - val_loss: 0.0743 - val_accuracy: 0.6073

Epoch 00026: val_loss did not improve from 0.07290
Epoch 27/110
4307/4306 [==============================] - 305s 71ms/step - loss: 0.0738 - accuracy: 0.6108 - val_loss: 0.0716 - val_accuracy: 0.6254

Epoch 00027: val_loss improved from 0.07290 to 0.07159, saving model to models/_mini_XCEPTION.27-0.63.hdf5
Epoch 28/110
4307/4306 [==============================] - 305s 71ms/step - loss: 0.0733 - accuracy: 0.6125 - val_loss: 0.0728 - val_accuracy: 0.6162

Epoch 00028: val_loss did not improve from 0.07159
Epoch 29/110
4307/4306 [==============================] - 305s 71ms/step - loss: 0.0731 - accuracy: 0.6145 - val_loss: 0.0765 - val_accuracy: 0.5923

Epoch 00029: val_loss did not improve from 0.07159
Epoch 30/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0730 - accuracy: 0.6151 - val_loss: 0.0747 - val_accuracy: 0.6078

Epoch 00030: val_loss did not improve from 0.07159
Epoch 31/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0730 - accuracy: 0.6159 - val_loss: 0.0738 - val_accuracy: 0.6111

Epoch 00031: val_loss did not improve from 0.07159
Epoch 32/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0728 - accuracy: 0.6169 - val_loss: 0.0731 - val_accuracy: 0.6175

Epoch 00032: val_loss did not improve from 0.07159
Epoch 33/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0726 - accuracy: 0.6201 - val_loss: 0.0729 - val_accuracy: 0.6184

Epoch 00033: val_loss did not improve from 0.07159
Epoch 34/110
4307/4306 [==============================] - 305s 71ms/step - loss: 0.0724 - accuracy: 0.6199 - val_loss: 0.0813 - val_accuracy: 0.5745

Epoch 00034: val_loss did not improve from 0.07159
Epoch 35/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0722 - accuracy: 0.6212 - val_loss: 0.0710 - val_accuracy: 0.6277

Epoch 00035: val_loss improved from 0.07159 to 0.07103, saving model to models/_mini_XCEPTION.35-0.63.hdf5
Epoch 36/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0720 - accuracy: 0.6225 - val_loss: 0.0777 - val_accuracy: 0.5914

Epoch 00036: val_loss did not improve from 0.07103
Epoch 37/110
4307/4306 [==============================] - 305s 71ms/step - loss: 0.0719 - accuracy: 0.6237 - val_loss: 0.0815 - val_accuracy: 0.5701

Epoch 00037: val_loss did not improve from 0.07103
Epoch 38/110
4307/4306 [==============================] - 305s 71ms/step - loss: 0.0718 - accuracy: 0.6244 - val_loss: 0.0721 - val_accuracy: 0.6196

Epoch 00038: val_loss did not improve from 0.07103
Epoch 39/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0717 - accuracy: 0.6249 - val_loss: 0.0710 - val_accuracy: 0.6288

Epoch 00039: val_loss improved from 0.07103 to 0.07100, saving model to models/_mini_XCEPTION.39-0.63.hdf5
Epoch 40/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0715 - accuracy: 0.6247 - val_loss: 0.0718 - val_accuracy: 0.6226

Epoch 00040: val_loss did not improve from 0.07100
Epoch 41/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0715 - accuracy: 0.6257 - val_loss: 0.0703 - val_accuracy: 0.6341

Epoch 00041: val_loss improved from 0.07100 to 0.07027, saving model to models/_mini_XCEPTION.41-0.63.hdf5
Epoch 42/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0713 - accuracy: 0.6275 - val_loss: 0.0788 - val_accuracy: 0.5850

Epoch 00042: val_loss did not improve from 0.07027
Epoch 43/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0712 - accuracy: 0.6277 - val_loss: 0.0719 - val_accuracy: 0.6246

Epoch 00043: val_loss did not improve from 0.07027
Epoch 44/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0710 - accuracy: 0.6291 - val_loss: 0.0777 - val_accuracy: 0.5932

Epoch 00044: val_loss did not improve from 0.07027
Epoch 45/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0710 - accuracy: 0.6292 - val_loss: 0.0734 - val_accuracy: 0.6130

Epoch 00045: val_loss did not improve from 0.07027
Epoch 46/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0710 - accuracy: 0.6287 - val_loss: 0.0707 - val_accuracy: 0.6314

Epoch 00046: val_loss did not improve from 0.07027
Epoch 47/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0708 - accuracy: 0.6301 - val_loss: 0.0704 - val_accuracy: 0.6328

Epoch 00047: val_loss did not improve from 0.07027
Epoch 48/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0705 - accuracy: 0.6314 - val_loss: 0.0701 - val_accuracy: 0.6328

Epoch 00048: val_loss improved from 0.07027 to 0.07014, saving model to models/_mini_XCEPTION.48-0.63.hdf5
Epoch 49/110
4307/4306 [==============================] - 305s 71ms/step - loss: 0.0703 - accuracy: 0.6337 - val_loss: 0.0763 - val_accuracy: 0.6010

Epoch 00049: val_loss did not improve from 0.07014
Epoch 50/110
4307/4306 [==============================] - 305s 71ms/step - loss: 0.0704 - accuracy: 0.6333 - val_loss: 0.0718 - val_accuracy: 0.6229

Epoch 00050: val_loss did not improve from 0.07014
Epoch 51/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0703 - accuracy: 0.6336 - val_loss: 0.0742 - val_accuracy: 0.6090

Epoch 00051: val_loss did not improve from 0.07014
Epoch 52/110
4307/4306 [==============================] - 305s 71ms/step - loss: 0.0701 - accuracy: 0.6336 - val_loss: 0.0786 - val_accuracy: 0.5787

Epoch 00052: val_loss did not improve from 0.07014
Epoch 53/110
4307/4306 [==============================] - 305s 71ms/step - loss: 0.0701 - accuracy: 0.6341 - val_loss: 0.0716 - val_accuracy: 0.6263

Epoch 00053: val_loss did not improve from 0.07014
Epoch 54/110
4307/4306 [==============================] - 305s 71ms/step - loss: 0.0700 - accuracy: 0.6352 - val_loss: 0.0730 - val_accuracy: 0.6203

Epoch 00054: val_loss did not improve from 0.07014
Epoch 55/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0699 - accuracy: 0.6353 - val_loss: 0.0744 - val_accuracy: 0.6087

Epoch 00055: val_loss did not improve from 0.07014
Epoch 56/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0695 - accuracy: 0.6398 - val_loss: 0.0694 - val_accuracy: 0.6391

Epoch 00056: val_loss improved from 0.07014 to 0.06938, saving model to models/_mini_XCEPTION.56-0.64.hdf5
Epoch 57/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0697 - accuracy: 0.6375 - val_loss: 0.0786 - val_accuracy: 0.5849

Epoch 00057: val_loss did not improve from 0.06938
Epoch 58/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0697 - accuracy: 0.6359 - val_loss: 0.0731 - val_accuracy: 0.6162

Epoch 00058: val_loss did not improve from 0.06938
Epoch 59/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0696 - accuracy: 0.6385 - val_loss: 0.0686 - val_accuracy: 0.6454

Epoch 00059: val_loss improved from 0.06938 to 0.06860, saving model to models/_mini_XCEPTION.59-0.65.hdf5
Epoch 60/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0695 - accuracy: 0.6381 - val_loss: 0.0713 - val_accuracy: 0.6281

Epoch 00060: val_loss did not improve from 0.06860
Epoch 61/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0695 - accuracy: 0.6386 - val_loss: 0.0709 - val_accuracy: 0.6314

Epoch 00061: val_loss did not improve from 0.06860
Epoch 62/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0694 - accuracy: 0.6386 - val_loss: 0.0760 - val_accuracy: 0.5971

Epoch 00062: val_loss did not improve from 0.06860
Epoch 63/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0692 - accuracy: 0.6400 - val_loss: 0.0736 - val_accuracy: 0.6096

Epoch 00063: val_loss did not improve from 0.06860
Epoch 64/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0691 - accuracy: 0.6405 - val_loss: 0.0712 - val_accuracy: 0.6311

Epoch 00064: val_loss did not improve from 0.06860
Epoch 65/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0690 - accuracy: 0.6419 - val_loss: 0.0690 - val_accuracy: 0.6413

Epoch 00065: val_loss did not improve from 0.06860
Epoch 66/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0690 - accuracy: 0.6419 - val_loss: 0.0715 - val_accuracy: 0.6233

Epoch 00066: val_loss did not improve from 0.06860
Epoch 67/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0687 - accuracy: 0.6432 - val_loss: 0.0702 - val_accuracy: 0.6348

Epoch 00067: val_loss did not improve from 0.06860
Epoch 68/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0687 - accuracy: 0.6436 - val_loss: 0.0681 - val_accuracy: 0.6454

Epoch 00068: val_loss improved from 0.06860 to 0.06812, saving model to models/_mini_XCEPTION.68-0.65.hdf5
Epoch 69/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0688 - accuracy: 0.6422 - val_loss: 0.0686 - val_accuracy: 0.6445

Epoch 00069: val_loss did not improve from 0.06812
Epoch 70/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0688 - accuracy: 0.6426 - val_loss: 0.0673 - val_accuracy: 0.6538

Epoch 00070: val_loss improved from 0.06812 to 0.06726, saving model to models/_mini_XCEPTION.70-0.65.hdf5
Epoch 71/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0685 - accuracy: 0.6462 - val_loss: 0.0679 - val_accuracy: 0.6505

Epoch 00071: val_loss did not improve from 0.06726
Epoch 72/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0683 - accuracy: 0.6473 - val_loss: 0.0730 - val_accuracy: 0.6168

Epoch 00072: val_loss did not improve from 0.06726
Epoch 73/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0684 - accuracy: 0.6460 - val_loss: 0.0703 - val_accuracy: 0.6334

Epoch 00073: val_loss did not improve from 0.06726
Epoch 74/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0684 - accuracy: 0.6454 - val_loss: 0.0688 - val_accuracy: 0.6420

Epoch 00074: val_loss did not improve from 0.06726
Epoch 75/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0683 - accuracy: 0.6464 - val_loss: 0.0690 - val_accuracy: 0.6422

Epoch 00075: val_loss did not improve from 0.06726
Epoch 76/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0683 - accuracy: 0.6465 - val_loss: 0.0693 - val_accuracy: 0.6401

Epoch 00076: val_loss did not improve from 0.06726
Epoch 77/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0683 - accuracy: 0.6452 - val_loss: 0.0684 - val_accuracy: 0.6490

Epoch 00077: val_loss did not improve from 0.06726
Epoch 78/110
4307/4306 [==============================] - 305s 71ms/step - loss: 0.0682 - accuracy: 0.6465 - val_loss: 0.0692 - val_accuracy: 0.6398

Epoch 00078: val_loss did not improve from 0.06726
Epoch 79/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0682 - accuracy: 0.6474 - val_loss: 0.0680 - val_accuracy: 0.6501

Epoch 00079: val_loss did not improve from 0.06726
Epoch 80/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0681 - accuracy: 0.6467 - val_loss: 0.0688 - val_accuracy: 0.6427

Epoch 00080: val_loss did not improve from 0.06726
Epoch 81/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0678 - accuracy: 0.6495 - val_loss: 0.0689 - val_accuracy: 0.6419

Epoch 00081: val_loss did not improve from 0.06726
Epoch 82/110
4307/4306 [==============================] - 305s 71ms/step - loss: 0.0677 - accuracy: 0.6494 - val_loss: 0.0697 - val_accuracy: 0.6368

Epoch 00082: val_loss did not improve from 0.06726

Epoch 00082: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
Epoch 83/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0651 - accuracy: 0.6650 - val_loss: 0.0637 - val_accuracy: 0.6740

Epoch 00083: val_loss improved from 0.06726 to 0.06368, saving model to models/_mini_XCEPTION.83-0.67.hdf5
Epoch 84/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0641 - accuracy: 0.6712 - val_loss: 0.0635 - val_accuracy: 0.6736

Epoch 00084: val_loss improved from 0.06368 to 0.06352, saving model to models/_mini_XCEPTION.84-0.67.hdf5
Epoch 85/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0641 - accuracy: 0.6698 - val_loss: 0.0639 - val_accuracy: 0.6702

Epoch 00085: val_loss did not improve from 0.06352
Epoch 86/110
4307/4306 [==============================] - 305s 71ms/step - loss: 0.0638 - accuracy: 0.6717 - val_loss: 0.0648 - val_accuracy: 0.6665

Epoch 00086: val_loss did not improve from 0.06352
Epoch 87/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0637 - accuracy: 0.6720 - val_loss: 0.0627 - val_accuracy: 0.6788

Epoch 00087: val_loss improved from 0.06352 to 0.06266, saving model to models/_mini_XCEPTION.87-0.68.hdf5
Epoch 88/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0635 - accuracy: 0.6741 - val_loss: 0.0641 - val_accuracy: 0.6708

Epoch 00088: val_loss did not improve from 0.06266
Epoch 89/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0634 - accuracy: 0.6748 - val_loss: 0.0626 - val_accuracy: 0.6806

Epoch 00089: val_loss improved from 0.06266 to 0.06258, saving model to models/_mini_XCEPTION.89-0.68.hdf5
Epoch 90/110
4307/4306 [==============================] - 305s 71ms/step - loss: 0.0634 - accuracy: 0.6735 - val_loss: 0.0629 - val_accuracy: 0.6771

Epoch 00090: val_loss did not improve from 0.06258
Epoch 91/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0632 - accuracy: 0.6745 - val_loss: 0.0632 - val_accuracy: 0.6756

Epoch 00091: val_loss did not improve from 0.06258
Epoch 92/110
4307/4306 [==============================] - 305s 71ms/step - loss: 0.0632 - accuracy: 0.6748 - val_loss: 0.0628 - val_accuracy: 0.6785

Epoch 00092: val_loss did not improve from 0.06258
Epoch 93/110
4307/4306 [==============================] - 305s 71ms/step - loss: 0.0632 - accuracy: 0.6745 - val_loss: 0.0628 - val_accuracy: 0.6790

Epoch 00093: val_loss did not improve from 0.06258
Epoch 94/110
4307/4306 [==============================] - 305s 71ms/step - loss: 0.0631 - accuracy: 0.6755 - val_loss: 0.0634 - val_accuracy: 0.6739

Epoch 00094: val_loss did not improve from 0.06258
Epoch 95/110
4307/4306 [==============================] - 305s 71ms/step - loss: 0.0631 - accuracy: 0.6758 - val_loss: 0.0626 - val_accuracy: 0.6778

Epoch 00095: val_loss did not improve from 0.06258
Epoch 96/110
4307/4306 [==============================] - 305s 71ms/step - loss: 0.0631 - accuracy: 0.6763 - val_loss: 0.0638 - val_accuracy: 0.6728

Epoch 00096: val_loss did not improve from 0.06258
Epoch 97/110
4307/4306 [==============================] - 305s 71ms/step - loss: 0.0631 - accuracy: 0.6756 - val_loss: 0.0624 - val_accuracy: 0.6794

Epoch 00097: val_loss improved from 0.06258 to 0.06242, saving model to models/_mini_XCEPTION.97-0.68.hdf5
Epoch 98/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0630 - accuracy: 0.6758 - val_loss: 0.0635 - val_accuracy: 0.6735

Epoch 00098: val_loss did not improve from 0.06242
Epoch 99/110
4307/4306 [==============================] - 305s 71ms/step - loss: 0.0629 - accuracy: 0.6769 - val_loss: 0.0633 - val_accuracy: 0.6750

Epoch 00099: val_loss did not improve from 0.06242
Epoch 100/110
4307/4306 [==============================] - 305s 71ms/step - loss: 0.0627 - accuracy: 0.6783 - val_loss: 0.0635 - val_accuracy: 0.6741

Epoch 00100: val_loss did not improve from 0.06242
Epoch 101/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0629 - accuracy: 0.6764 - val_loss: 0.0625 - val_accuracy: 0.6803

Epoch 00101: val_loss did not improve from 0.06242
Epoch 102/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0628 - accuracy: 0.6763 - val_loss: 0.0629 - val_accuracy: 0.6785

Epoch 00102: val_loss did not improve from 0.06242
Epoch 103/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0629 - accuracy: 0.6773 - val_loss: 0.0640 - val_accuracy: 0.6712

Epoch 00103: val_loss did not improve from 0.06242
Epoch 104/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0628 - accuracy: 0.6778 - val_loss: 0.0623 - val_accuracy: 0.6809

Epoch 00104: val_loss improved from 0.06242 to 0.06226, saving model to models/_mini_XCEPTION.104-0.68.hdf5
Epoch 105/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0626 - accuracy: 0.6780 - val_loss: 0.0625 - val_accuracy: 0.6799

Epoch 00105: val_loss did not improve from 0.06226
Epoch 106/110
4307/4306 [==============================] - 306s 71ms/step - loss: 0.0628 - accuracy: 0.6767 - val_loss: 0.0628 - val_accuracy: 0.6787

Epoch 00106: val_loss did not improve from 0.06226
Epoch 107/110
4307/4306 [==============================] - 309s 72ms/step - loss: 0.0627 - accuracy: 0.6774 - val_loss: 0.0622 - val_accuracy: 0.6817

Epoch 00107: val_loss improved from 0.06226 to 0.06223, saving model to models/_mini_XCEPTION.107-0.68.hdf5
Epoch 108/110
4307/4306 [==============================] - 307s 71ms/step - loss: 0.0627 - accuracy: 0.6777 - val_loss: 0.0624 - val_accuracy: 0.6813

Epoch 00108: val_loss did not improve from 0.06223
Epoch 109/110
4307/4306 [==============================] - 310s 72ms/step - loss: 0.0627 - accuracy: 0.6784 - val_loss: 0.0635 - val_accuracy: 0.6727

Epoch 00109: val_loss did not improve from 0.06223
Epoch 110/110
4307/4306 [==============================] - 307s 71ms/step - loss: 0.0625 - accuracy: 0.6788 - val_loss: 0.0625 - val_accuracy: 0.6801

Epoch 00110: val_loss did not improve from 0.06223