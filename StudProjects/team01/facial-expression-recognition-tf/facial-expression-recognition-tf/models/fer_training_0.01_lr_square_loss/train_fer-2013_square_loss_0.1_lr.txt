Using TensorFlow backend.
2019-11-10 10:45:46.635251: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
C:\Users\Bogdan\Documents\GitHub\Intelligent-Tools-for-Social-Good\facial-expression-recognition-tf\facial-expression-recognition-tf\data_loader.py:23: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.
  emotions = pd.get_dummies(data['emotion']).as_matrix()
WARNING:tensorflow:From C:\Users\Bogdan\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\ops\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2019-11-10 10:46:15.834042: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2019-11-10 10:46:16.355950: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:
name: Quadro M520 major: 5 minor: 0 memoryClockRate(GHz): 1.176
pciBusID: 0000:02:00.0
2019-11-10 10:46:16.362301: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2019-11-10 10:46:16.368625: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll
2019-11-10 10:46:16.376068: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_100.dll
2019-11-10 10:46:16.381685: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_100.dll
2019-11-10 10:46:16.389535: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_100.dll
2019-11-10 10:46:16.396607: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_100.dll
2019-11-10 10:46:16.410287: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2019-11-10 10:46:16.416104: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-11-10 10:46:16.420256: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2019-11-10 10:46:16.427984: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:
name: Quadro M520 major: 5 minor: 0 memoryClockRate(GHz): 1.176
pciBusID: 0000:02:00.0
2019-11-10 10:46:16.433805: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2019-11-10 10:46:16.439419: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll
2019-11-10 10:46:16.444400: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_100.dll
2019-11-10 10:46:16.448807: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_100.dll
2019-11-10 10:46:16.454059: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_100.dll
2019-11-10 10:46:16.458972: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_100.dll
2019-11-10 10:46:16.463292: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2019-11-10 10:46:16.469484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-11-10 10:46:17.741458: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-11-10 10:46:17.745586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0
2019-11-10 10:46:17.748167: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N
2019-11-10 10:46:17.751692: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1384 MB memory) -> physical GPU (device: 0, name: Quadro M520, pci bus id: 0000:02:00.0, compute capability: 5.0)
WARNING:tensorflow:From C:\Users\Bogdan\AppData\Local\Programs\Python\Python37\lib\site-packages\keras\backend\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.

Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, 48, 48, 1)    0
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 46, 46, 8)    72          input_1[0][0]
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 46, 46, 8)    32          conv2d_1[0][0]
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 46, 46, 8)    0           batch_normalization_1[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 44, 44, 8)    576         activation_1[0][0]
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 44, 44, 8)    32          conv2d_2[0][0]
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 44, 44, 8)    0           batch_normalization_2[0][0]
__________________________________________________________________________________________________
separable_conv2d_1 (SeparableCo (None, 44, 44, 16)   200         activation_2[0][0]
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 44, 44, 16)   64          separable_conv2d_1[0][0]
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 44, 44, 16)   0           batch_normalization_4[0][0]
__________________________________________________________________________________________________
separable_conv2d_2 (SeparableCo (None, 44, 44, 16)   400         activation_3[0][0]
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 44, 44, 16)   64          separable_conv2d_2[0][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 22, 22, 16)   128         activation_2[0][0]
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 22, 22, 16)   0           batch_normalization_5[0][0]
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 22, 22, 16)   64          conv2d_3[0][0]
__________________________________________________________________________________________________
add_1 (Add)                     (None, 22, 22, 16)   0           max_pooling2d_1[0][0]
                                                                 batch_normalization_3[0][0]
__________________________________________________________________________________________________
separable_conv2d_3 (SeparableCo (None, 22, 22, 32)   656         add_1[0][0]
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 22, 22, 32)   128         separable_conv2d_3[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 22, 22, 32)   0           batch_normalization_7[0][0]
__________________________________________________________________________________________________
separable_conv2d_4 (SeparableCo (None, 22, 22, 32)   1312        activation_4[0][0]
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 22, 22, 32)   128         separable_conv2d_4[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 11, 11, 32)   512         add_1[0][0]
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 11, 11, 32)   0           batch_normalization_8[0][0]
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 11, 11, 32)   128         conv2d_4[0][0]
__________________________________________________________________________________________________
add_2 (Add)                     (None, 11, 11, 32)   0           max_pooling2d_2[0][0]
                                                                 batch_normalization_6[0][0]
__________________________________________________________________________________________________
separable_conv2d_5 (SeparableCo (None, 11, 11, 64)   2336        add_2[0][0]
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 11, 11, 64)   256         separable_conv2d_5[0][0]
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 11, 11, 64)   0           batch_normalization_10[0][0]
__________________________________________________________________________________________________
separable_conv2d_6 (SeparableCo (None, 11, 11, 64)   4672        activation_5[0][0]
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 11, 11, 64)   256         separable_conv2d_6[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 6, 6, 64)     2048        add_2[0][0]
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 6, 6, 64)     0           batch_normalization_11[0][0]
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 6, 6, 64)     256         conv2d_5[0][0]
__________________________________________________________________________________________________
add_3 (Add)                     (None, 6, 6, 64)     0           max_pooling2d_3[0][0]
                                                                 batch_normalization_9[0][0]
__________________________________________________________________________________________________
separable_conv2d_7 (SeparableCo (None, 6, 6, 128)    8768        add_3[0][0]
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 6, 6, 128)    512         separable_conv2d_7[0][0]
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 6, 6, 128)    0           batch_normalization_13[0][0]
__________________________________________________________________________________________________
separable_conv2d_8 (SeparableCo (None, 6, 6, 128)    17536       activation_6[0][0]
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 6, 6, 128)    512         separable_conv2d_8[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 3, 3, 128)    8192        add_3[0][0]
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 3, 3, 128)    0           batch_normalization_14[0][0]
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 3, 3, 128)    512         conv2d_6[0][0]
__________________________________________________________________________________________________
add_4 (Add)                     (None, 3, 3, 128)    0           max_pooling2d_4[0][0]
                                                                 batch_normalization_12[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 3, 3, 7)      8071        add_4[0][0]
__________________________________________________________________________________________________
global_average_pooling2d_1 (Glo (None, 7)            0           conv2d_7[0][0]
__________________________________________________________________________________________________
predictions (Activation)        (None, 7)            0           global_average_pooling2d_1[0][0]
==================================================================================================
Total params: 58,423
Trainable params: 56,951
Non-trainable params: 1,472
__________________________________________________________________________________________________
WARNING:tensorflow:From C:\Users\Bogdan\AppData\Local\Programs\Python\Python37\lib\site-packages\keras\backend\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

Epoch 1/110
2019-11-10 10:46:28.290801: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2019-11-10 10:46:29.274717: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Internal: Invoking ptxas not supported on Windows
Relying on driver to perform ptx compilation. This message will be only logged once.
2019-11-10 10:46:29.329062: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll
718/717 [==============================] - 59s 83ms/step - loss: 0.1398 - accuracy: 0.2964 - val_loss: 0.1131 - val_accuracy: 0.3673

Epoch 00001: val_loss improved from inf to 0.11310, saving model to models/_mini_XCEPTION.01-0.37.hdf5
Epoch 2/110
718/717 [==============================] - 53s 74ms/step - loss: 0.1073 - accuracy: 0.3953 - val_loss: 0.1031 - val_accuracy: 0.4305

Epoch 00002: val_loss improved from 0.11310 to 0.10306, saving model to models/_mini_XCEPTION.02-0.43.hdf5
Epoch 3/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0999 - accuracy: 0.4422 - val_loss: 0.1022 - val_accuracy: 0.4511

Epoch 00003: val_loss improved from 0.10306 to 0.10219, saving model to models/_mini_XCEPTION.03-0.45.hdf5
Epoch 4/110
718/717 [==============================] - 53s 73ms/step - loss: 0.0961 - accuracy: 0.4696 - val_loss: 0.1027 - val_accuracy: 0.4357

Epoch 00004: val_loss did not improve from 0.10219
Epoch 5/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0937 - accuracy: 0.4854 - val_loss: 0.1011 - val_accuracy: 0.4265

Epoch 00005: val_loss improved from 0.10219 to 0.10109, saving model to models/_mini_XCEPTION.05-0.43.hdf5
Epoch 6/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0917 - accuracy: 0.4974 - val_loss: 0.1062 - val_accuracy: 0.4326

Epoch 00006: val_loss did not improve from 0.10109
Epoch 7/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0892 - accuracy: 0.5124 - val_loss: 0.0945 - val_accuracy: 0.5031

Epoch 00007: val_loss improved from 0.10109 to 0.09454, saving model to models/_mini_XCEPTION.07-0.50.hdf5
Epoch 8/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0878 - accuracy: 0.5271 - val_loss: 0.1009 - val_accuracy: 0.4460

Epoch 00008: val_loss did not improve from 0.09454
Epoch 9/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0866 - accuracy: 0.5302 - val_loss: 0.0964 - val_accuracy: 0.4730

Epoch 00009: val_loss did not improve from 0.09454
Epoch 10/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0852 - accuracy: 0.5376 - val_loss: 0.0907 - val_accuracy: 0.5094

Epoch 00010: val_loss improved from 0.09454 to 0.09068, saving model to models/_mini_XCEPTION.10-0.51.hdf5
Epoch 11/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0841 - accuracy: 0.5461 - val_loss: 0.0855 - val_accuracy: 0.5394

Epoch 00011: val_loss improved from 0.09068 to 0.08547, saving model to models/_mini_XCEPTION.11-0.54.hdf5
Epoch 12/110
718/717 [==============================] - 54s 75ms/step - loss: 0.0843 - accuracy: 0.5443 - val_loss: 0.0876 - val_accuracy: 0.5308

Epoch 00012: val_loss did not improve from 0.08547
Epoch 13/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0832 - accuracy: 0.5517 - val_loss: 0.0932 - val_accuracy: 0.5111

Epoch 00013: val_loss did not improve from 0.08547
Epoch 14/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0821 - accuracy: 0.5555 - val_loss: 0.0893 - val_accuracy: 0.5296

Epoch 00014: val_loss did not improve from 0.08547
Epoch 15/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0812 - accuracy: 0.5645 - val_loss: 0.0837 - val_accuracy: 0.5509

Epoch 00015: val_loss improved from 0.08547 to 0.08368, saving model to models/_mini_XCEPTION.15-0.55.hdf5
Epoch 16/110
718/717 [==============================] - 54s 75ms/step - loss: 0.0809 - accuracy: 0.5665 - val_loss: 0.0855 - val_accuracy: 0.5448

Epoch 00016: val_loss did not improve from 0.08368
Epoch 17/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0803 - accuracy: 0.5685 - val_loss: 0.0905 - val_accuracy: 0.5094

Epoch 00017: val_loss did not improve from 0.08368
Epoch 18/110
718/717 [==============================] - 54s 75ms/step - loss: 0.0798 - accuracy: 0.5699 - val_loss: 0.0872 - val_accuracy: 0.5333

Epoch 00018: val_loss did not improve from 0.08368
Epoch 19/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0792 - accuracy: 0.5782 - val_loss: 0.0855 - val_accuracy: 0.5449

Epoch 00019: val_loss did not improve from 0.08368
Epoch 20/110
718/717 [==============================] - 54s 75ms/step - loss: 0.0791 - accuracy: 0.5761 - val_loss: 0.0831 - val_accuracy: 0.5578

Epoch 00020: val_loss improved from 0.08368 to 0.08315, saving model to models/_mini_XCEPTION.20-0.56.hdf5
Epoch 21/110
718/717 [==============================] - 54s 75ms/step - loss: 0.0781 - accuracy: 0.5839 - val_loss: 0.0829 - val_accuracy: 0.5547

Epoch 00021: val_loss improved from 0.08315 to 0.08294, saving model to models/_mini_XCEPTION.21-0.55.hdf5
Epoch 22/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0779 - accuracy: 0.5862 - val_loss: 0.0797 - val_accuracy: 0.5785

Epoch 00022: val_loss improved from 0.08294 to 0.07970, saving model to models/_mini_XCEPTION.22-0.58.hdf5
Epoch 23/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0775 - accuracy: 0.5923 - val_loss: 0.0831 - val_accuracy: 0.5677

Epoch 00023: val_loss did not improve from 0.07970
Epoch 24/110
718/717 [==============================] - 54s 75ms/step - loss: 0.0774 - accuracy: 0.5906 - val_loss: 0.0823 - val_accuracy: 0.5529

Epoch 00024: val_loss did not improve from 0.07970
Epoch 25/110
718/717 [==============================] - 54s 75ms/step - loss: 0.0767 - accuracy: 0.5935 - val_loss: 0.0830 - val_accuracy: 0.5531

Epoch 00025: val_loss did not improve from 0.07970
Epoch 26/110
718/717 [==============================] - 54s 75ms/step - loss: 0.0766 - accuracy: 0.5966 - val_loss: 0.0866 - val_accuracy: 0.5394

Epoch 00026: val_loss did not improve from 0.07970
Epoch 27/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0763 - accuracy: 0.5946 - val_loss: 0.0794 - val_accuracy: 0.5808

Epoch 00027: val_loss improved from 0.07970 to 0.07936, saving model to models/_mini_XCEPTION.27-0.58.hdf5
Epoch 28/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0757 - accuracy: 0.6004 - val_loss: 0.0891 - val_accuracy: 0.5352

Epoch 00028: val_loss did not improve from 0.07936
Epoch 29/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0756 - accuracy: 0.6010 - val_loss: 0.0838 - val_accuracy: 0.5603

Epoch 00029: val_loss did not improve from 0.07936
Epoch 30/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0756 - accuracy: 0.5991 - val_loss: 0.0818 - val_accuracy: 0.5700

Epoch 00030: val_loss did not improve from 0.07936
Epoch 31/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0749 - accuracy: 0.6086 - val_loss: 0.0796 - val_accuracy: 0.5845

Epoch 00031: val_loss did not improve from 0.07936
Epoch 32/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0749 - accuracy: 0.6042 - val_loss: 0.0835 - val_accuracy: 0.5592

Epoch 00032: val_loss did not improve from 0.07936
Epoch 33/110
718/717 [==============================] - 54s 75ms/step - loss: 0.0744 - accuracy: 0.6097 - val_loss: 0.0831 - val_accuracy: 0.5737

Epoch 00033: val_loss did not improve from 0.07936
Epoch 34/110
718/717 [==============================] - 54s 75ms/step - loss: 0.0741 - accuracy: 0.6107 - val_loss: 0.0808 - val_accuracy: 0.5718

Epoch 00034: val_loss did not improve from 0.07936
Epoch 35/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0738 - accuracy: 0.6117 - val_loss: 0.0960 - val_accuracy: 0.5153

Epoch 00035: val_loss did not improve from 0.07936
Epoch 36/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0737 - accuracy: 0.6107 - val_loss: 0.0877 - val_accuracy: 0.5239

Epoch 00036: val_loss did not improve from 0.07936
Epoch 37/110
718/717 [==============================] - 54s 75ms/step - loss: 0.0733 - accuracy: 0.6180 - val_loss: 0.0777 - val_accuracy: 0.5921

Epoch 00037: val_loss improved from 0.07936 to 0.07769, saving model to models/_mini_XCEPTION.37-0.59.hdf5
Epoch 38/110
718/717 [==============================] - 55s 76ms/step - loss: 0.0731 - accuracy: 0.6180 - val_loss: 0.0787 - val_accuracy: 0.5944

Epoch 00038: val_loss did not improve from 0.07769
Epoch 39/110
718/717 [==============================] - 54s 75ms/step - loss: 0.0729 - accuracy: 0.6187 - val_loss: 0.0820 - val_accuracy: 0.5603

Epoch 00039: val_loss did not improve from 0.07769
Epoch 40/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0726 - accuracy: 0.6212 - val_loss: 0.0817 - val_accuracy: 0.5585

Epoch 00040: val_loss did not improve from 0.07769
Epoch 41/110
718/717 [==============================] - 53s 75ms/step - loss: 0.0726 - accuracy: 0.6169 - val_loss: 0.0785 - val_accuracy: 0.5824

Epoch 00041: val_loss did not improve from 0.07769
Epoch 42/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0726 - accuracy: 0.6173 - val_loss: 0.0788 - val_accuracy: 0.5886

Epoch 00042: val_loss did not improve from 0.07769
Epoch 43/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0720 - accuracy: 0.6206 - val_loss: 0.0799 - val_accuracy: 0.5827

Epoch 00043: val_loss did not improve from 0.07769
Epoch 44/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0716 - accuracy: 0.6270 - val_loss: 0.0785 - val_accuracy: 0.5827

Epoch 00044: val_loss did not improve from 0.07769
Epoch 45/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0714 - accuracy: 0.6253 - val_loss: 0.0765 - val_accuracy: 0.6047

Epoch 00045: val_loss improved from 0.07769 to 0.07648, saving model to models/_mini_XCEPTION.45-0.60.hdf5
Epoch 46/110
718/717 [==============================] - 53s 75ms/step - loss: 0.0713 - accuracy: 0.6277 - val_loss: 0.0785 - val_accuracy: 0.5770

Epoch 00046: val_loss did not improve from 0.07648
Epoch 47/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0714 - accuracy: 0.6259 - val_loss: 0.0778 - val_accuracy: 0.5878

Epoch 00047: val_loss did not improve from 0.07648
Epoch 48/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0712 - accuracy: 0.6276 - val_loss: 0.0830 - val_accuracy: 0.5547

Epoch 00048: val_loss did not improve from 0.07648
Epoch 49/110
718/717 [==============================] - 54s 75ms/step - loss: 0.0706 - accuracy: 0.6320 - val_loss: 0.0794 - val_accuracy: 0.5838

Epoch 00049: val_loss did not improve from 0.07648
Epoch 50/110
718/717 [==============================] - 54s 75ms/step - loss: 0.0710 - accuracy: 0.6300 - val_loss: 0.0797 - val_accuracy: 0.5808

Epoch 00050: val_loss did not improve from 0.07648
Epoch 51/110
718/717 [==============================] - 54s 75ms/step - loss: 0.0707 - accuracy: 0.6335 - val_loss: 0.0790 - val_accuracy: 0.5782

Epoch 00051: val_loss did not improve from 0.07648
Epoch 52/110
718/717 [==============================] - 54s 75ms/step - loss: 0.0704 - accuracy: 0.6306 - val_loss: 0.0778 - val_accuracy: 0.5954

Epoch 00052: val_loss did not improve from 0.07648
Epoch 53/110
718/717 [==============================] - 54s 75ms/step - loss: 0.0698 - accuracy: 0.6354 - val_loss: 0.0794 - val_accuracy: 0.5897

Epoch 00053: val_loss did not improve from 0.07648
Epoch 54/110
718/717 [==============================] - 54s 75ms/step - loss: 0.0699 - accuracy: 0.6336 - val_loss: 0.0780 - val_accuracy: 0.5845

Epoch 00054: val_loss did not improve from 0.07648
Epoch 55/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0697 - accuracy: 0.6396 - val_loss: 0.0803 - val_accuracy: 0.5843

Epoch 00055: val_loss did not improve from 0.07648
Epoch 56/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0695 - accuracy: 0.6391 - val_loss: 0.0771 - val_accuracy: 0.5940

Epoch 00056: val_loss did not improve from 0.07648
Epoch 57/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0699 - accuracy: 0.6381 - val_loss: 0.0792 - val_accuracy: 0.5979

Epoch 00057: val_loss did not improve from 0.07648

Epoch 00057: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
Epoch 58/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0663 - accuracy: 0.6593 - val_loss: 0.0733 - val_accuracy: 0.6223

Epoch 00058: val_loss improved from 0.07648 to 0.07335, saving model to models/_mini_XCEPTION.58-0.62.hdf5
Epoch 59/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0652 - accuracy: 0.6630 - val_loss: 0.0729 - val_accuracy: 0.6214

Epoch 00059: val_loss improved from 0.07335 to 0.07295, saving model to models/_mini_XCEPTION.59-0.62.hdf5
Epoch 60/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0646 - accuracy: 0.6687 - val_loss: 0.0732 - val_accuracy: 0.6207

Epoch 00060: val_loss did not improve from 0.07295
Epoch 61/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0648 - accuracy: 0.6667 - val_loss: 0.0734 - val_accuracy: 0.6183

Epoch 00061: val_loss did not improve from 0.07295
Epoch 62/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0645 - accuracy: 0.6675 - val_loss: 0.0733 - val_accuracy: 0.6179

Epoch 00062: val_loss did not improve from 0.07295
Epoch 63/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0643 - accuracy: 0.6703 - val_loss: 0.0734 - val_accuracy: 0.6177

Epoch 00063: val_loss did not improve from 0.07295
Epoch 64/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0639 - accuracy: 0.6719 - val_loss: 0.0731 - val_accuracy: 0.6207

Epoch 00064: val_loss did not improve from 0.07295
Epoch 65/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0640 - accuracy: 0.6711 - val_loss: 0.0728 - val_accuracy: 0.6216

Epoch 00065: val_loss improved from 0.07295 to 0.07278, saving model to models/_mini_XCEPTION.65-0.62.hdf5
Epoch 66/110
718/717 [==============================] - 54s 75ms/step - loss: 0.0638 - accuracy: 0.6720 - val_loss: 0.0731 - val_accuracy: 0.6224

Epoch 00066: val_loss did not improve from 0.07278
Epoch 67/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0635 - accuracy: 0.6731 - val_loss: 0.0730 - val_accuracy: 0.6202

Epoch 00067: val_loss did not improve from 0.07278
Epoch 68/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0640 - accuracy: 0.6706 - val_loss: 0.0733 - val_accuracy: 0.6214

Epoch 00068: val_loss did not improve from 0.07278
Epoch 69/110
718/717 [==============================] - 54s 75ms/step - loss: 0.0634 - accuracy: 0.6756 - val_loss: 0.0732 - val_accuracy: 0.6209

Epoch 00069: val_loss did not improve from 0.07278
Epoch 70/110
718/717 [==============================] - 54s 75ms/step - loss: 0.0635 - accuracy: 0.6754 - val_loss: 0.0725 - val_accuracy: 0.6237

Epoch 00070: val_loss improved from 0.07278 to 0.07251, saving model to models/_mini_XCEPTION.70-0.62.hdf5
Epoch 71/110
718/717 [==============================] - 54s 75ms/step - loss: 0.0634 - accuracy: 0.6714 - val_loss: 0.0732 - val_accuracy: 0.6226

Epoch 00071: val_loss did not improve from 0.07251
Epoch 72/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0635 - accuracy: 0.6773 - val_loss: 0.0734 - val_accuracy: 0.6200

Epoch 00072: val_loss did not improve from 0.07251
Epoch 73/110
718/717 [==============================] - 54s 75ms/step - loss: 0.0634 - accuracy: 0.6752 - val_loss: 0.0728 - val_accuracy: 0.6243

Epoch 00073: val_loss did not improve from 0.07251
Epoch 74/110
718/717 [==============================] - 54s 76ms/step - loss: 0.0631 - accuracy: 0.6755 - val_loss: 0.0729 - val_accuracy: 0.6259

Epoch 00074: val_loss did not improve from 0.07251
Epoch 75/110
718/717 [==============================] - 54s 75ms/step - loss: 0.0631 - accuracy: 0.6763 - val_loss: 0.0729 - val_accuracy: 0.6198

Epoch 00075: val_loss did not improve from 0.07251
Epoch 76/110
718/717 [==============================] - 54s 75ms/step - loss: 0.0628 - accuracy: 0.6784 - val_loss: 0.0730 - val_accuracy: 0.6200

Epoch 00076: val_loss did not improve from 0.07251
Epoch 77/110
718/717 [==============================] - 54s 75ms/step - loss: 0.0629 - accuracy: 0.6799 - val_loss: 0.0730 - val_accuracy: 0.6237

Epoch 00077: val_loss did not improve from 0.07251
Epoch 78/110
718/717 [==============================] - 54s 75ms/step - loss: 0.0630 - accuracy: 0.6774 - val_loss: 0.0728 - val_accuracy: 0.6238

Epoch 00078: val_loss did not improve from 0.07251
Epoch 79/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0627 - accuracy: 0.6780 - val_loss: 0.0737 - val_accuracy: 0.6193

Epoch 00079: val_loss did not improve from 0.07251
Epoch 80/110
718/717 [==============================] - 54s 75ms/step - loss: 0.0629 - accuracy: 0.6795 - val_loss: 0.0736 - val_accuracy: 0.6177

Epoch 00080: val_loss did not improve from 0.07251
Epoch 81/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0626 - accuracy: 0.6800 - val_loss: 0.0730 - val_accuracy: 0.6195

Epoch 00081: val_loss did not improve from 0.07251
Epoch 82/110
718/717 [==============================] - 53s 75ms/step - loss: 0.0627 - accuracy: 0.6794 - val_loss: 0.0728 - val_accuracy: 0.6240

Epoch 00082: val_loss did not improve from 0.07251

Epoch 00082: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
Epoch 83/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0621 - accuracy: 0.6833 - val_loss: 0.0727 - val_accuracy: 0.6264

Epoch 00083: val_loss did not improve from 0.07251
Epoch 84/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0622 - accuracy: 0.6832 - val_loss: 0.0726 - val_accuracy: 0.6259

Epoch 00084: val_loss did not improve from 0.07251
Epoch 85/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0623 - accuracy: 0.6826 - val_loss: 0.0725 - val_accuracy: 0.6270

Epoch 00085: val_loss did not improve from 0.07251
Epoch 86/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0621 - accuracy: 0.6848 - val_loss: 0.0726 - val_accuracy: 0.6243

Epoch 00086: val_loss did not improve from 0.07251
Epoch 87/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0621 - accuracy: 0.6827 - val_loss: 0.0725 - val_accuracy: 0.6257

Epoch 00087: val_loss did not improve from 0.07251
Epoch 88/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0619 - accuracy: 0.6853 - val_loss: 0.0725 - val_accuracy: 0.6252

Epoch 00088: val_loss did not improve from 0.07251
Epoch 89/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0619 - accuracy: 0.6825 - val_loss: 0.0725 - val_accuracy: 0.6254

Epoch 00089: val_loss did not improve from 0.07251
Epoch 90/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0619 - accuracy: 0.6852 - val_loss: 0.0726 - val_accuracy: 0.6256

Epoch 00090: val_loss did not improve from 0.07251
Epoch 91/110
718/717 [==============================] - 54s 75ms/step - loss: 0.0620 - accuracy: 0.6853 - val_loss: 0.0725 - val_accuracy: 0.6268

Epoch 00091: val_loss improved from 0.07251 to 0.07250, saving model to models/_mini_XCEPTION.91-0.63.hdf5
Epoch 92/110
718/717 [==============================] - 54s 75ms/step - loss: 0.0617 - accuracy: 0.6845 - val_loss: 0.0725 - val_accuracy: 0.6237

Epoch 00092: val_loss did not improve from 0.07250
Epoch 93/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0622 - accuracy: 0.6819 - val_loss: 0.0726 - val_accuracy: 0.6256

Epoch 00093: val_loss did not improve from 0.07250
Epoch 94/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0618 - accuracy: 0.6851 - val_loss: 0.0725 - val_accuracy: 0.6263

Epoch 00094: val_loss did not improve from 0.07250

Epoch 00094: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.
Epoch 95/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0622 - accuracy: 0.6822 - val_loss: 0.0726 - val_accuracy: 0.6254

Epoch 00095: val_loss did not improve from 0.07250
Epoch 96/110
718/717 [==============================] - 54s 75ms/step - loss: 0.0621 - accuracy: 0.6845 - val_loss: 0.0726 - val_accuracy: 0.6257

Epoch 00096: val_loss did not improve from 0.07250
Epoch 97/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0616 - accuracy: 0.6879 - val_loss: 0.0726 - val_accuracy: 0.6259

Epoch 00097: val_loss did not improve from 0.07250
Epoch 98/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0621 - accuracy: 0.6837 - val_loss: 0.0726 - val_accuracy: 0.6259

Epoch 00098: val_loss did not improve from 0.07250
Epoch 99/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0623 - accuracy: 0.6819 - val_loss: 0.0725 - val_accuracy: 0.6266

Epoch 00099: val_loss did not improve from 0.07250
Epoch 100/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0619 - accuracy: 0.6840 - val_loss: 0.0725 - val_accuracy: 0.6263

Epoch 00100: val_loss did not improve from 0.07250
Epoch 101/110
718/717 [==============================] - 53s 74ms/step - loss: 0.0623 - accuracy: 0.6804 - val_loss: 0.0725 - val_accuracy: 0.6270

Epoch 00101: val_loss did not improve from 0.07250
Epoch 102/110
718/717 [==============================] - 54s 75ms/step - loss: 0.0623 - accuracy: 0.6817 - val_loss: 0.0725 - val_accuracy: 0.6257

Epoch 00102: val_loss did not improve from 0.07250
Epoch 103/110
718/717 [==============================] - 54s 75ms/step - loss: 0.0619 - accuracy: 0.6858 - val_loss: 0.0725 - val_accuracy: 0.6257

Epoch 00103: val_loss did not improve from 0.07250
Epoch 104/110
718/717 [==============================] - 54s 75ms/step - loss: 0.0616 - accuracy: 0.6879 - val_loss: 0.0725 - val_accuracy: 0.6264

Epoch 00104: val_loss improved from 0.07250 to 0.07249, saving model to models/_mini_XCEPTION.104-0.63.hdf5
Epoch 105/110
718/717 [==============================] - 54s 75ms/step - loss: 0.0622 - accuracy: 0.6804 - val_loss: 0.0725 - val_accuracy: 0.6261

Epoch 00105: val_loss did not improve from 0.07249
Epoch 106/110
718/717 [==============================] - 54s 75ms/step - loss: 0.0616 - accuracy: 0.6875 - val_loss: 0.0725 - val_accuracy: 0.6256

Epoch 00106: val_loss did not improve from 0.07249

Epoch 00106: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.
Epoch 107/110
718/717 [==============================] - 54s 75ms/step - loss: 0.0618 - accuracy: 0.6861 - val_loss: 0.0725 - val_accuracy: 0.6270

Epoch 00107: val_loss improved from 0.07249 to 0.07247, saving model to models/_mini_XCEPTION.107-0.63.hdf5
Epoch 108/110
718/717 [==============================] - 54s 75ms/step - loss: 0.0619 - accuracy: 0.6866 - val_loss: 0.0725 - val_accuracy: 0.6261

Epoch 00108: val_loss did not improve from 0.07247
Epoch 109/110
718/717 [==============================] - 54s 75ms/step - loss: 0.0618 - accuracy: 0.6839 - val_loss: 0.0725 - val_accuracy: 0.6270

Epoch 00109: val_loss did not improve from 0.07247
Epoch 110/110
718/717 [==============================] - 54s 75ms/step - loss: 0.0621 - accuracy: 0.6835 - val_loss: 0.0725 - val_accuracy: 0.6275

Epoch 00110: val_loss did not improve from 0.07247
Press any key to continue . . .