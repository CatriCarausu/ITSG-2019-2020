Using TensorFlow backend.
2019-10-27 17:55:14.875215: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
C:\Users\Bogdan\source\repos\facial-expression-recognition-tf\facial-expression-recognition-tf\data_loader.py:22: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.
  emotions = pd.get_dummies(data['emotion']).as_matrix()
WARNING:tensorflow:From C:\Users\Bogdan\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\ops\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2019-10-27 17:56:21.586079: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2019-10-27 17:56:22.097791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:
name: Quadro M520 major: 5 minor: 0 memoryClockRate(GHz): 1.176
pciBusID: 0000:02:00.0
2019-10-27 17:56:22.105045: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2019-10-27 17:56:22.115096: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll
2019-10-27 17:56:22.124880: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_100.dll
2019-10-27 17:56:22.130254: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_100.dll
2019-10-27 17:56:22.141951: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_100.dll
2019-10-27 17:56:22.150337: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_100.dll
2019-10-27 17:56:22.168395: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2019-10-27 17:56:22.176120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-10-27 17:56:22.181297: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2019-10-27 17:56:22.191583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:
name: Quadro M520 major: 5 minor: 0 memoryClockRate(GHz): 1.176
pciBusID: 0000:02:00.0
2019-10-27 17:56:22.198406: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2019-10-27 17:56:22.204214: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll
2019-10-27 17:56:22.209214: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_100.dll
2019-10-27 17:56:22.213782: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_100.dll
2019-10-27 17:56:22.220385: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_100.dll
2019-10-27 17:56:22.226154: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_100.dll
2019-10-27 17:56:22.230778: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2019-10-27 17:56:22.245835: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-10-27 17:56:24.014855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-10-27 17:56:24.019833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0
2019-10-27 17:56:24.022291: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N
2019-10-27 17:56:24.030053: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1384 MB memory) -> physical GPU (device: 0, name: Quadro M520, pci bus id: 0000:02:00.0, compute capability: 5.0)
WARNING:tensorflow:From C:\Users\Bogdan\AppData\Local\Programs\Python\Python37\lib\site-packages\keras\backend\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.

Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, 48, 48, 1)    0
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 46, 46, 8)    72          input_1[0][0]
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 46, 46, 8)    32          conv2d_1[0][0]
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 46, 46, 8)    0           batch_normalization_1[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 44, 44, 8)    576         activation_1[0][0]
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 44, 44, 8)    32          conv2d_2[0][0]
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 44, 44, 8)    0           batch_normalization_2[0][0]
__________________________________________________________________________________________________
separable_conv2d_1 (SeparableCo (None, 44, 44, 16)   200         activation_2[0][0]
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 44, 44, 16)   64          separable_conv2d_1[0][0]
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 44, 44, 16)   0           batch_normalization_4[0][0]
__________________________________________________________________________________________________
separable_conv2d_2 (SeparableCo (None, 44, 44, 16)   400         activation_3[0][0]
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 44, 44, 16)   64          separable_conv2d_2[0][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 22, 22, 16)   128         activation_2[0][0]
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 22, 22, 16)   0           batch_normalization_5[0][0]
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 22, 22, 16)   64          conv2d_3[0][0]
__________________________________________________________________________________________________
add_1 (Add)                     (None, 22, 22, 16)   0           max_pooling2d_1[0][0]
                                                                 batch_normalization_3[0][0]
__________________________________________________________________________________________________
separable_conv2d_3 (SeparableCo (None, 22, 22, 32)   656         add_1[0][0]
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 22, 22, 32)   128         separable_conv2d_3[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 22, 22, 32)   0           batch_normalization_7[0][0]
__________________________________________________________________________________________________
separable_conv2d_4 (SeparableCo (None, 22, 22, 32)   1312        activation_4[0][0]
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 22, 22, 32)   128         separable_conv2d_4[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 11, 11, 32)   512         add_1[0][0]
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 11, 11, 32)   0           batch_normalization_8[0][0]
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 11, 11, 32)   128         conv2d_4[0][0]
__________________________________________________________________________________________________
add_2 (Add)                     (None, 11, 11, 32)   0           max_pooling2d_2[0][0]
                                                                 batch_normalization_6[0][0]
__________________________________________________________________________________________________
separable_conv2d_5 (SeparableCo (None, 11, 11, 64)   2336        add_2[0][0]
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 11, 11, 64)   256         separable_conv2d_5[0][0]
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 11, 11, 64)   0           batch_normalization_10[0][0]
__________________________________________________________________________________________________
separable_conv2d_6 (SeparableCo (None, 11, 11, 64)   4672        activation_5[0][0]
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 11, 11, 64)   256         separable_conv2d_6[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 6, 6, 64)     2048        add_2[0][0]
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 6, 6, 64)     0           batch_normalization_11[0][0]
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 6, 6, 64)     256         conv2d_5[0][0]
__________________________________________________________________________________________________
add_3 (Add)                     (None, 6, 6, 64)     0           max_pooling2d_3[0][0]
                                                                 batch_normalization_9[0][0]
__________________________________________________________________________________________________
separable_conv2d_7 (SeparableCo (None, 6, 6, 128)    8768        add_3[0][0]
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 6, 6, 128)    512         separable_conv2d_7[0][0]
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 6, 6, 128)    0           batch_normalization_13[0][0]
__________________________________________________________________________________________________
separable_conv2d_8 (SeparableCo (None, 6, 6, 128)    17536       activation_6[0][0]
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 6, 6, 128)    512         separable_conv2d_8[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 3, 3, 128)    8192        add_3[0][0]
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 3, 3, 128)    0           batch_normalization_14[0][0]
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 3, 3, 128)    512         conv2d_6[0][0]
__________________________________________________________________________________________________
add_4 (Add)                     (None, 3, 3, 128)    0           max_pooling2d_4[0][0]
                                                                 batch_normalization_12[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 3, 3, 7)      8071        add_4[0][0]
__________________________________________________________________________________________________
global_average_pooling2d_1 (Glo (None, 7)            0           conv2d_7[0][0]
__________________________________________________________________________________________________
predictions (Activation)        (None, 7)            0           global_average_pooling2d_1[0][0]
==================================================================================================
Total params: 58,423
Trainable params: 56,951
Non-trainable params: 1,472
__________________________________________________________________________________________________
WARNING:tensorflow:From C:\Users\Bogdan\AppData\Local\Programs\Python\Python37\lib\site-packages\keras\backend\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

Epoch 1/110
2019-10-27 17:56:39.895292: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2019-10-27 17:56:41.080092: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Internal: Invoking ptxas not supported on Windows
Relying on driver to perform ptx compilation. This message will be only logged once.
2019-10-27 17:56:41.146736: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll
898/897 [==============================] - 113s 126ms/step - loss: 1.7663 - accuracy: 0.3292 - val_loss: 1.6231 - val_accuracy: 0.4199

Epoch 00001: val_loss improved from inf to 1.62310, saving model to models/_mini_XCEPTION.01-0.42.hdf5
Epoch 2/110
898/897 [==============================] - 91s 101ms/step - loss: 1.5114 - accuracy: 0.4332 - val_loss: 1.5582 - val_accuracy: 0.4252

Epoch 00002: val_loss improved from 1.62310 to 1.55823, saving model to models/_mini_XCEPTION.02-0.43.hdf5
Epoch 3/110
898/897 [==============================] - 86s 96ms/step - loss: 1.3955 - accuracy: 0.4795 - val_loss: 1.4497 - val_accuracy: 0.4533

Epoch 00003: val_loss improved from 1.55823 to 1.44975, saving model to models/_mini_XCEPTION.03-0.45.hdf5
Epoch 4/110
898/897 [==============================] - 75s 83ms/step - loss: 1.3225 - accuracy: 0.5037 - val_loss: 1.2677 - val_accuracy: 0.5100

Epoch 00004: val_loss improved from 1.44975 to 1.26773, saving model to models/_mini_XCEPTION.04-0.51.hdf5
Epoch 5/110
898/897 [==============================] - 76s 84ms/step - loss: 1.2733 - accuracy: 0.5245 - val_loss: 1.3086 - val_accuracy: 0.5210

Epoch 00005: val_loss did not improve from 1.26773
Epoch 6/110
898/897 [==============================] - 74s 83ms/step - loss: 1.2470 - accuracy: 0.5281 - val_loss: 1.2084 - val_accuracy: 0.5497

Epoch 00006: val_loss improved from 1.26773 to 1.20836, saving model to models/_mini_XCEPTION.06-0.55.hdf5
Epoch 7/110
898/897 [==============================] - 76s 84ms/step - loss: 1.2216 - accuracy: 0.5414 - val_loss: 1.3544 - val_accuracy: 0.5103

Epoch 00007: val_loss did not improve from 1.20836
Epoch 8/110
898/897 [==============================] - 74s 83ms/step - loss: 1.1978 - accuracy: 0.5507 - val_loss: 1.3215 - val_accuracy: 0.5096

Epoch 00008: val_loss did not improve from 1.20836
Epoch 9/110
898/897 [==============================] - 75s 84ms/step - loss: 1.1849 - accuracy: 0.5523 - val_loss: 1.2925 - val_accuracy: 0.5173

Epoch 00009: val_loss did not improve from 1.20836
Epoch 10/110
898/897 [==============================] - 75s 83ms/step - loss: 1.1640 - accuracy: 0.5607 - val_loss: 1.1091 - val_accuracy: 0.5887

Epoch 00010: val_loss improved from 1.20836 to 1.10910, saving model to models/_mini_XCEPTION.10-0.59.hdf5
Epoch 11/110
898/897 [==============================] - 74s 83ms/step - loss: 1.1532 - accuracy: 0.5659 - val_loss: 1.1518 - val_accuracy: 0.5683

Epoch 00011: val_loss did not improve from 1.10910
Epoch 12/110
898/897 [==============================] - 75s 83ms/step - loss: 1.1319 - accuracy: 0.5734 - val_loss: 1.2007 - val_accuracy: 0.5571

Epoch 00012: val_loss did not improve from 1.10910
Epoch 13/110
898/897 [==============================] - 75s 83ms/step - loss: 1.1278 - accuracy: 0.5794 - val_loss: 1.1379 - val_accuracy: 0.5673

Epoch 00013: val_loss did not improve from 1.10910
Epoch 14/110
898/897 [==============================] - 76s 85ms/step - loss: 1.1162 - accuracy: 0.5824 - val_loss: 1.1762 - val_accuracy: 0.5600

Epoch 00014: val_loss did not improve from 1.10910
Epoch 15/110
898/897 [==============================] - 76s 85ms/step - loss: 1.1094 - accuracy: 0.5862 - val_loss: 1.1027 - val_accuracy: 0.5931

Epoch 00015: val_loss improved from 1.10910 to 1.10265, saving model to models/_mini_XCEPTION.15-0.59.hdf5
Epoch 16/110
898/897 [==============================] - 75s 83ms/step - loss: 1.1038 - accuracy: 0.5886 - val_loss: 1.0776 - val_accuracy: 0.5929

Epoch 00016: val_loss improved from 1.10265 to 1.07760, saving model to models/_mini_XCEPTION.16-0.59.hdf5
Epoch 17/110
898/897 [==============================] - 76s 85ms/step - loss: 1.0863 - accuracy: 0.5936 - val_loss: 1.0980 - val_accuracy: 0.5960

Epoch 00017: val_loss did not improve from 1.07760
Epoch 18/110
898/897 [==============================] - 74s 83ms/step - loss: 1.0853 - accuracy: 0.5928 - val_loss: 1.0945 - val_accuracy: 0.5815

Epoch 00018: val_loss did not improve from 1.07760
Epoch 19/110
898/897 [==============================] - 74s 83ms/step - loss: 1.0718 - accuracy: 0.5987 - val_loss: 1.1220 - val_accuracy: 0.5847

Epoch 00019: val_loss did not improve from 1.07760
Epoch 20/110
898/897 [==============================] - 74s 83ms/step - loss: 1.0664 - accuracy: 0.6026 - val_loss: 1.0447 - val_accuracy: 0.6117

Epoch 00020: val_loss improved from 1.07760 to 1.04473, saving model to models/_mini_XCEPTION.20-0.61.hdf5
Epoch 21/110
898/897 [==============================] - 75s 83ms/step - loss: 1.0594 - accuracy: 0.6035 - val_loss: 1.0617 - val_accuracy: 0.6096

Epoch 00021: val_loss did not improve from 1.04473
Epoch 22/110
898/897 [==============================] - 74s 83ms/step - loss: 1.0576 - accuracy: 0.6037 - val_loss: 1.0531 - val_accuracy: 0.6135

Epoch 00022: val_loss did not improve from 1.04473
Epoch 23/110
898/897 [==============================] - 75s 84ms/step - loss: 1.0460 - accuracy: 0.6104 - val_loss: 1.1213 - val_accuracy: 0.5776

Epoch 00023: val_loss did not improve from 1.04473
Epoch 24/110
898/897 [==============================] - 75s 83ms/step - loss: 1.0406 - accuracy: 0.6112 - val_loss: 1.1083 - val_accuracy: 0.5947

Epoch 00024: val_loss did not improve from 1.04473
Epoch 25/110
898/897 [==============================] - 76s 85ms/step - loss: 1.0369 - accuracy: 0.6125 - val_loss: 1.0601 - val_accuracy: 0.5999

Epoch 00025: val_loss did not improve from 1.04473
Epoch 26/110
898/897 [==============================] - 75s 84ms/step - loss: 1.0330 - accuracy: 0.6120 - val_loss: 1.0284 - val_accuracy: 0.6158

Epoch 00026: val_loss improved from 1.04473 to 1.02839, saving model to models/_mini_XCEPTION.26-0.62.hdf5
Epoch 27/110
898/897 [==============================] - 77s 86ms/step - loss: 1.0260 - accuracy: 0.6191 - val_loss: 1.0425 - val_accuracy: 0.6177

Epoch 00027: val_loss did not improve from 1.02839
Epoch 28/110
898/897 [==============================] - 73s 82ms/step - loss: 1.0176 - accuracy: 0.6157 - val_loss: 1.0604 - val_accuracy: 0.6145

Epoch 00028: val_loss did not improve from 1.02839
Epoch 29/110
898/897 [==============================] - 72s 81ms/step - loss: 1.0202 - accuracy: 0.6205 - val_loss: 1.0718 - val_accuracy: 0.6071

Epoch 00029: val_loss did not improve from 1.02839
Epoch 30/110
898/897 [==============================] - 72s 80ms/step - loss: 1.0103 - accuracy: 0.6238 - val_loss: 1.0492 - val_accuracy: 0.6108

Epoch 00030: val_loss did not improve from 1.02839
Epoch 31/110
898/897 [==============================] - 72s 80ms/step - loss: 1.0131 - accuracy: 0.6235 - val_loss: 1.0699 - val_accuracy: 0.6073

Epoch 00031: val_loss did not improve from 1.02839
Epoch 32/110
898/897 [==============================] - 72s 80ms/step - loss: 1.0111 - accuracy: 0.6267 - val_loss: 1.0552 - val_accuracy: 0.6098

Epoch 00032: val_loss did not improve from 1.02839
Epoch 33/110
898/897 [==============================] - 72s 80ms/step - loss: 1.0025 - accuracy: 0.6243 - val_loss: 1.0539 - val_accuracy: 0.6123

Epoch 00033: val_loss did not improve from 1.02839
Epoch 34/110
898/897 [==============================] - 72s 81ms/step - loss: 1.0006 - accuracy: 0.6231 - val_loss: 1.0634 - val_accuracy: 0.6137

Epoch 00034: val_loss did not improve from 1.02839
Epoch 35/110
898/897 [==============================] - 72s 80ms/step - loss: 0.9928 - accuracy: 0.6275 - val_loss: 1.0686 - val_accuracy: 0.6144

Epoch 00035: val_loss did not improve from 1.02839
Epoch 36/110
898/897 [==============================] - 71s 79ms/step - loss: 0.9883 - accuracy: 0.6305 - val_loss: 1.0754 - val_accuracy: 0.6039

Epoch 00036: val_loss did not improve from 1.02839
Epoch 37/110
898/897 [==============================] - 72s 80ms/step - loss: 0.9869 - accuracy: 0.6278 - val_loss: 1.0158 - val_accuracy: 0.6278

Epoch 00037: val_loss improved from 1.02839 to 1.01577, saving model to models/_mini_XCEPTION.37-0.63.hdf5
Epoch 38/110
898/897 [==============================] - 72s 80ms/step - loss: 0.9825 - accuracy: 0.6342 - val_loss: 1.0457 - val_accuracy: 0.6209

Epoch 00038: val_loss did not improve from 1.01577
Epoch 39/110
898/897 [==============================] - 71s 79ms/step - loss: 0.9812 - accuracy: 0.6335 - val_loss: 1.0644 - val_accuracy: 0.6177

Epoch 00039: val_loss did not improve from 1.01577
Epoch 40/110
898/897 [==============================] - 71s 79ms/step - loss: 0.9776 - accuracy: 0.6357 - val_loss: 1.0337 - val_accuracy: 0.6167

Epoch 00040: val_loss did not improve from 1.01577
Epoch 41/110
898/897 [==============================] - 71s 79ms/step - loss: 0.9744 - accuracy: 0.6367 - val_loss: 1.0583 - val_accuracy: 0.6140

Epoch 00041: val_loss did not improve from 1.01577
Epoch 42/110
898/897 [==============================] - 72s 80ms/step - loss: 0.9726 - accuracy: 0.6375 - val_loss: 1.0204 - val_accuracy: 0.6369

Epoch 00042: val_loss did not improve from 1.01577
Epoch 43/110
898/897 [==============================] - 71s 79ms/step - loss: 0.9717 - accuracy: 0.6380 - val_loss: 1.0108 - val_accuracy: 0.6354

Epoch 00043: val_loss improved from 1.01577 to 1.01082, saving model to models/_mini_XCEPTION.43-0.64.hdf5
Epoch 44/110
898/897 [==============================] - 71s 79ms/step - loss: 0.9682 - accuracy: 0.6369 - val_loss: 1.0527 - val_accuracy: 0.6195

Epoch 00044: val_loss did not improve from 1.01082
Epoch 45/110
898/897 [==============================] - 71s 79ms/step - loss: 0.9684 - accuracy: 0.6381 - val_loss: 1.0147 - val_accuracy: 0.6314

Epoch 00045: val_loss did not improve from 1.01082
Epoch 46/110
898/897 [==============================] - 71s 79ms/step - loss: 0.9580 - accuracy: 0.6438 - val_loss: 1.0554 - val_accuracy: 0.6117

Epoch 00046: val_loss did not improve from 1.01082
Epoch 47/110
898/897 [==============================] - 71s 79ms/step - loss: 0.9588 - accuracy: 0.6434 - val_loss: 1.0079 - val_accuracy: 0.6339

Epoch 00047: val_loss improved from 1.01082 to 1.00792, saving model to models/_mini_XCEPTION.47-0.63.hdf5
Epoch 48/110
898/897 [==============================] - 72s 80ms/step - loss: 0.9582 - accuracy: 0.6416 - val_loss: 1.0359 - val_accuracy: 0.6293

Epoch 00048: val_loss did not improve from 1.00792
Epoch 49/110
898/897 [==============================] - 71s 79ms/step - loss: 0.9487 - accuracy: 0.6500 - val_loss: 1.0825 - val_accuracy: 0.6048

Epoch 00049: val_loss did not improve from 1.00792
Epoch 50/110
898/897 [==============================] - 71s 79ms/step - loss: 0.9534 - accuracy: 0.6447 - val_loss: 1.0272 - val_accuracy: 0.6234

Epoch 00050: val_loss did not improve from 1.00792
Epoch 51/110
898/897 [==============================] - 70s 78ms/step - loss: 0.9484 - accuracy: 0.6478 - val_loss: 1.0769 - val_accuracy: 0.6219

Epoch 00051: val_loss did not improve from 1.00792
Epoch 52/110
898/897 [==============================] - 70s 78ms/step - loss: 0.9475 - accuracy: 0.6474 - val_loss: 1.0463 - val_accuracy: 0.6163

Epoch 00052: val_loss did not improve from 1.00792
Epoch 53/110
898/897 [==============================] - 70s 78ms/step - loss: 0.9494 - accuracy: 0.6440 - val_loss: 1.0418 - val_accuracy: 0.6225

Epoch 00053: val_loss did not improve from 1.00792
Epoch 54/110
898/897 [==============================] - 70s 78ms/step - loss: 0.9426 - accuracy: 0.6478 - val_loss: 1.0630 - val_accuracy: 0.6163

Epoch 00054: val_loss did not improve from 1.00792
Epoch 55/110
898/897 [==============================] - 71s 79ms/step - loss: 0.9420 - accuracy: 0.6497 - val_loss: 1.0203 - val_accuracy: 0.6258

Epoch 00055: val_loss did not improve from 1.00792
Epoch 56/110
898/897 [==============================] - 70s 78ms/step - loss: 0.9401 - accuracy: 0.6541 - val_loss: 1.0020 - val_accuracy: 0.6315

Epoch 00056: val_loss improved from 1.00792 to 1.00203, saving model to models/_mini_XCEPTION.56-0.63.hdf5
Epoch 57/110
898/897 [==============================] - 70s 78ms/step - loss: 0.9364 - accuracy: 0.6488 - val_loss: 0.9994 - val_accuracy: 0.6346

Epoch 00057: val_loss improved from 1.00203 to 0.99944, saving model to models/_mini_XCEPTION.57-0.63.hdf5
Epoch 58/110
898/897 [==============================] - 70s 78ms/step - loss: 0.9331 - accuracy: 0.6516 - val_loss: 1.0222 - val_accuracy: 0.6282

Epoch 00058: val_loss did not improve from 0.99944
Epoch 59/110
898/897 [==============================] - 70s 78ms/step - loss: 0.9335 - accuracy: 0.6501 - val_loss: 1.0170 - val_accuracy: 0.6335

Epoch 00059: val_loss did not improve from 0.99944
Epoch 60/110
898/897 [==============================] - 70s 78ms/step - loss: 0.9248 - accuracy: 0.6560 - val_loss: 1.0096 - val_accuracy: 0.6278

Epoch 00060: val_loss did not improve from 0.99944
Epoch 61/110
898/897 [==============================] - 70s 78ms/step - loss: 0.9278 - accuracy: 0.6540 - val_loss: 1.0096 - val_accuracy: 0.6276

Epoch 00061: val_loss did not improve from 0.99944
Epoch 62/110
898/897 [==============================] - 70s 78ms/step - loss: 0.9315 - accuracy: 0.6528 - val_loss: 1.0530 - val_accuracy: 0.6170

Epoch 00062: val_loss did not improve from 0.99944
Epoch 63/110
898/897 [==============================] - 70s 78ms/step - loss: 0.9290 - accuracy: 0.6545 - val_loss: 1.0022 - val_accuracy: 0.6332

Epoch 00063: val_loss did not improve from 0.99944
Epoch 64/110
898/897 [==============================] - 70s 78ms/step - loss: 0.9250 - accuracy: 0.6568 - val_loss: 0.9973 - val_accuracy: 0.6360

Epoch 00064: val_loss improved from 0.99944 to 0.99733, saving model to models/_mini_XCEPTION.64-0.64.hdf5
Epoch 65/110
898/897 [==============================] - 70s 78ms/step - loss: 0.9265 - accuracy: 0.6553 - val_loss: 1.0399 - val_accuracy: 0.6229

Epoch 00065: val_loss did not improve from 0.99733
Epoch 66/110
898/897 [==============================] - 70s 78ms/step - loss: 0.9207 - accuracy: 0.6591 - val_loss: 1.0544 - val_accuracy: 0.6186

Epoch 00066: val_loss did not improve from 0.99733
Epoch 67/110
898/897 [==============================] - 70s 78ms/step - loss: 0.9180 - accuracy: 0.6586 - val_loss: 1.0458 - val_accuracy: 0.6162

Epoch 00067: val_loss did not improve from 0.99733
Epoch 68/110
898/897 [==============================] - 70s 78ms/step - loss: 0.9139 - accuracy: 0.6597 - val_loss: 1.0028 - val_accuracy: 0.6321

Epoch 00068: val_loss did not improve from 0.99733
Epoch 69/110
898/897 [==============================] - 70s 78ms/step - loss: 0.9128 - accuracy: 0.6581 - val_loss: 1.0085 - val_accuracy: 0.6325

Epoch 00069: val_loss did not improve from 0.99733
Epoch 70/110
898/897 [==============================] - 70s 78ms/step - loss: 0.9166 - accuracy: 0.6616 - val_loss: 1.0464 - val_accuracy: 0.6261

Epoch 00070: val_loss did not improve from 0.99733
Epoch 71/110
898/897 [==============================] - 70s 78ms/step - loss: 0.9182 - accuracy: 0.6608 - val_loss: 1.0405 - val_accuracy: 0.6258

Epoch 00071: val_loss did not improve from 0.99733
Epoch 72/110
898/897 [==============================] - 70s 78ms/step - loss: 0.9111 - accuracy: 0.6616 - val_loss: 1.0492 - val_accuracy: 0.6219

Epoch 00072: val_loss did not improve from 0.99733
Epoch 73/110
898/897 [==============================] - 71s 79ms/step - loss: 0.9091 - accuracy: 0.6630 - val_loss: 1.0667 - val_accuracy: 0.6121

Epoch 00073: val_loss did not improve from 0.99733
Epoch 74/110
898/897 [==============================] - 70s 78ms/step - loss: 0.9106 - accuracy: 0.6637 - val_loss: 1.0233 - val_accuracy: 0.6293

Epoch 00074: val_loss did not improve from 0.99733
Epoch 75/110
898/897 [==============================] - 70s 78ms/step - loss: 0.9052 - accuracy: 0.6647 - val_loss: 1.0190 - val_accuracy: 0.6318

Epoch 00075: val_loss did not improve from 0.99733
Epoch 76/110
898/897 [==============================] - 71s 79ms/step - loss: 0.9063 - accuracy: 0.6633 - val_loss: 1.0065 - val_accuracy: 0.6346

Epoch 00076: val_loss did not improve from 0.99733

Epoch 00076: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
Epoch 77/110
898/897 [==============================] - 70s 78ms/step - loss: 0.8621 - accuracy: 0.6815 - val_loss: 0.9649 - val_accuracy: 0.6510

Epoch 00077: val_loss improved from 0.99733 to 0.96489, saving model to models/_mini_XCEPTION.77-0.65.hdf5
Epoch 78/110
898/897 [==============================] - 70s 78ms/step - loss: 0.8455 - accuracy: 0.6880 - val_loss: 0.9611 - val_accuracy: 0.6527

Epoch 00078: val_loss improved from 0.96489 to 0.96111, saving model to models/_mini_XCEPTION.78-0.65.hdf5
Epoch 79/110
898/897 [==============================] - 70s 78ms/step - loss: 0.8394 - accuracy: 0.6900 - val_loss: 0.9642 - val_accuracy: 0.6506

Epoch 00079: val_loss did not improve from 0.96111
Epoch 80/110
898/897 [==============================] - 70s 78ms/step - loss: 0.8349 - accuracy: 0.6900 - val_loss: 0.9612 - val_accuracy: 0.6521

Epoch 00080: val_loss did not improve from 0.96111
Epoch 81/110
898/897 [==============================] - 70s 78ms/step - loss: 0.8328 - accuracy: 0.6918 - val_loss: 0.9678 - val_accuracy: 0.6482

Epoch 00081: val_loss did not improve from 0.96111
Epoch 82/110
898/897 [==============================] - 70s 78ms/step - loss: 0.8278 - accuracy: 0.6932 - val_loss: 0.9677 - val_accuracy: 0.6523

Epoch 00082: val_loss did not improve from 0.96111
Epoch 83/110
898/897 [==============================] - 70s 78ms/step - loss: 0.8288 - accuracy: 0.6928 - val_loss: 0.9624 - val_accuracy: 0.6513

Epoch 00083: val_loss did not improve from 0.96111
Epoch 84/110
898/897 [==============================] - 70s 78ms/step - loss: 0.8252 - accuracy: 0.6958 - val_loss: 0.9650 - val_accuracy: 0.6506

Epoch 00084: val_loss did not improve from 0.96111
Epoch 85/110
898/897 [==============================] - 70s 78ms/step - loss: 0.8280 - accuracy: 0.6933 - val_loss: 0.9661 - val_accuracy: 0.6512

Epoch 00085: val_loss did not improve from 0.96111
Epoch 86/110
898/897 [==============================] - 70s 78ms/step - loss: 0.8240 - accuracy: 0.6961 - val_loss: 0.9600 - val_accuracy: 0.6521

Epoch 00086: val_loss improved from 0.96111 to 0.96004, saving model to models/_mini_XCEPTION.86-0.65.hdf5
Epoch 87/110
898/897 [==============================] - 70s 78ms/step - loss: 0.8263 - accuracy: 0.6957 - val_loss: 0.9702 - val_accuracy: 0.6457

Epoch 00087: val_loss did not improve from 0.96004
Epoch 88/110
898/897 [==============================] - 70s 78ms/step - loss: 0.8198 - accuracy: 0.6986 - val_loss: 0.9672 - val_accuracy: 0.6496

Epoch 00088: val_loss did not improve from 0.96004
Epoch 89/110
898/897 [==============================] - 71s 80ms/step - loss: 0.8254 - accuracy: 0.6958 - val_loss: 0.9687 - val_accuracy: 0.6478

Epoch 00089: val_loss did not improve from 0.96004
Epoch 90/110
898/897 [==============================] - 70s 78ms/step - loss: 0.8178 - accuracy: 0.6997 - val_loss: 0.9644 - val_accuracy: 0.6513

Epoch 00090: val_loss did not improve from 0.96004
Epoch 91/110
898/897 [==============================] - 70s 78ms/step - loss: 0.8181 - accuracy: 0.6971 - val_loss: 0.9657 - val_accuracy: 0.6468

Epoch 00091: val_loss did not improve from 0.96004
Epoch 92/110
898/897 [==============================] - 70s 78ms/step - loss: 0.8149 - accuracy: 0.6970 - val_loss: 0.9626 - val_accuracy: 0.6488

Epoch 00092: val_loss did not improve from 0.96004
Epoch 93/110
898/897 [==============================] - 71s 79ms/step - loss: 0.8104 - accuracy: 0.6993 - val_loss: 0.9594 - val_accuracy: 0.6538

Epoch 00093: val_loss improved from 0.96004 to 0.95937, saving model to models/_mini_XCEPTION.93-0.65.hdf5
Epoch 94/110
898/897 [==============================] - 70s 78ms/step - loss: 0.8158 - accuracy: 0.6990 - val_loss: 0.9624 - val_accuracy: 0.6491

Epoch 00094: val_loss did not improve from 0.95937
Epoch 95/110
898/897 [==============================] - 70s 78ms/step - loss: 0.8169 - accuracy: 0.6994 - val_loss: 0.9636 - val_accuracy: 0.6528

Epoch 00095: val_loss did not improve from 0.95937
Epoch 96/110
898/897 [==============================] - 70s 78ms/step - loss: 0.8157 - accuracy: 0.6991 - val_loss: 0.9622 - val_accuracy: 0.6539

Epoch 00096: val_loss did not improve from 0.95937
Epoch 97/110
898/897 [==============================] - 70s 78ms/step - loss: 0.8099 - accuracy: 0.7023 - val_loss: 0.9589 - val_accuracy: 0.6498

Epoch 00097: val_loss improved from 0.95937 to 0.95891, saving model to models/_mini_XCEPTION.97-0.65.hdf5
Epoch 98/110
898/897 [==============================] - 71s 79ms/step - loss: 0.8083 - accuracy: 0.6995 - val_loss: 0.9634 - val_accuracy: 0.6517

Epoch 00098: val_loss did not improve from 0.95891
Epoch 99/110
898/897 [==============================] - 70s 78ms/step - loss: 0.8116 - accuracy: 0.6989 - val_loss: 0.9644 - val_accuracy: 0.6519

Epoch 00099: val_loss did not improve from 0.95891
Epoch 100/110
898/897 [==============================] - 70s 78ms/step - loss: 0.8106 - accuracy: 0.7018 - val_loss: 0.9650 - val_accuracy: 0.6542

Epoch 00100: val_loss did not improve from 0.95891
Epoch 101/110
898/897 [==============================] - 70s 78ms/step - loss: 0.8076 - accuracy: 0.7015 - val_loss: 0.9651 - val_accuracy: 0.6527

Epoch 00101: val_loss did not improve from 0.95891
Epoch 102/110
898/897 [==============================] - 70s 78ms/step - loss: 0.8087 - accuracy: 0.7028 - val_loss: 0.9714 - val_accuracy: 0.6477

Epoch 00102: val_loss did not improve from 0.95891
Epoch 103/110
898/897 [==============================] - 70s 78ms/step - loss: 0.8105 - accuracy: 0.7008 - val_loss: 0.9614 - val_accuracy: 0.6516

Epoch 00103: val_loss did not improve from 0.95891
Epoch 104/110
898/897 [==============================] - 70s 78ms/step - loss: 0.8074 - accuracy: 0.6999 - val_loss: 0.9665 - val_accuracy: 0.6521

Epoch 00104: val_loss did not improve from 0.95891
Epoch 105/110
898/897 [==============================] - 70s 78ms/step - loss: 0.8030 - accuracy: 0.7039 - val_loss: 0.9645 - val_accuracy: 0.6503

Epoch 00105: val_loss did not improve from 0.95891
Epoch 106/110
898/897 [==============================] - 70s 78ms/step - loss: 0.8083 - accuracy: 0.6997 - val_loss: 0.9694 - val_accuracy: 0.6475

Epoch 00106: val_loss did not improve from 0.95891
Epoch 107/110
898/897 [==============================] - 70s 78ms/step - loss: 0.8082 - accuracy: 0.7016 - val_loss: 0.9737 - val_accuracy: 0.6485

Epoch 00107: val_loss did not improve from 0.95891
Epoch 108/110
898/897 [==============================] - 71s 79ms/step - loss: 0.7986 - accuracy: 0.7035 - val_loss: 0.9645 - val_accuracy: 0.6516

Epoch 00108: val_loss did not improve from 0.95891
Epoch 109/110
898/897 [==============================] - 70s 78ms/step - loss: 0.7968 - accuracy: 0.7056 - val_loss: 0.9699 - val_accuracy: 0.6512

Epoch 00109: val_loss did not improve from 0.95891

Epoch 00109: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
Epoch 110/110
898/897 [==============================] - 72s 80ms/step - loss: 0.7945 - accuracy: 0.7089 - val_loss: 0.9661 - val_accuracy: 0.6531

Epoch 00110: val_loss did not improve from 0.95891
Press any key to continue . . .