PS C:\Users\Bogdan\Documents\GitHub\Intelligent-Tools-for-Social-Good\facial-expression-recognition-tf\facial-expression-recognition-tf> python .\tf_model.py
Using TensorFlow backend.
2019-12-08 21:34:46.264388: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
C:\Users\Bogdan\Documents\GitHub\Intelligent-Tools-for-Social-Good\facial-expression-recognition-tf\facial-expression-recognition-tf\data_loader.py:29: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.
  emotions = pd.get_dummies(data['emotion']).as_matrix()
WARNING:tensorflow:From C:\Users\Bogdan\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\ops\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2019-12-08 21:37:06.847733: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2019-12-08 21:37:07.381743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:
name: Quadro M520 major: 5 minor: 0 memoryClockRate(GHz): 1.176
pciBusID: 0000:02:00.0
2019-12-08 21:37:07.388043: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2019-12-08 21:37:07.394935: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll
2019-12-08 21:37:07.403052: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_100.dll
2019-12-08 21:37:07.408895: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_100.dll
2019-12-08 21:37:07.417028: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_100.dll
2019-12-08 21:37:07.424493: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_100.dll
2019-12-08 21:37:07.438806: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2019-12-08 21:37:07.444405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-12-08 21:37:07.449826: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2019-12-08 21:37:07.456456: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:
name: Quadro M520 major: 5 minor: 0 memoryClockRate(GHz): 1.176
pciBusID: 0000:02:00.0
2019-12-08 21:37:07.462005: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2019-12-08 21:37:07.466690: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll
2019-12-08 21:37:07.470566: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_100.dll
2019-12-08 21:37:07.474328: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_100.dll
2019-12-08 21:37:07.479141: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_100.dll
2019-12-08 21:37:07.483308: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_100.dll
2019-12-08 21:37:07.488074: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2019-12-08 21:37:07.493680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-12-08 21:37:08.797438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-12-08 21:37:08.802469: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0
2019-12-08 21:37:08.805393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N
2019-12-08 21:37:08.809698: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1384 MB memory) -> physical GPU (device: 0, name: Quadro M520, pci bus id: 0000:02:00.0, compute capability: 5.0)
WARNING:tensorflow:From C:\Users\Bogdan\AppData\Local\Programs\Python\Python37\lib\site-packages\keras\backend\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.

Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, 48, 48, 1)    0
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 46, 46, 8)    72          input_1[0][0]
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 46, 46, 8)    32          conv2d_1[0][0]
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 46, 46, 8)    0           batch_normalization_1[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 44, 44, 8)    576         activation_1[0][0]
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 44, 44, 8)    32          conv2d_2[0][0]
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 44, 44, 8)    0           batch_normalization_2[0][0]
__________________________________________________________________________________________________
separable_conv2d_1 (SeparableCo (None, 44, 44, 16)   200         activation_2[0][0]
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 44, 44, 16)   64          separable_conv2d_1[0][0]
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 44, 44, 16)   0           batch_normalization_4[0][0]
__________________________________________________________________________________________________
separable_conv2d_2 (SeparableCo (None, 44, 44, 16)   400         activation_3[0][0]
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 44, 44, 16)   64          separable_conv2d_2[0][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 22, 22, 16)   128         activation_2[0][0]
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 22, 22, 16)   0           batch_normalization_5[0][0]
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 22, 22, 16)   64          conv2d_3[0][0]
__________________________________________________________________________________________________
add_1 (Add)                     (None, 22, 22, 16)   0           max_pooling2d_1[0][0]
                                                                 batch_normalization_3[0][0]
__________________________________________________________________________________________________
separable_conv2d_3 (SeparableCo (None, 22, 22, 32)   656         add_1[0][0]
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 22, 22, 32)   128         separable_conv2d_3[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 22, 22, 32)   0           batch_normalization_7[0][0]
__________________________________________________________________________________________________
separable_conv2d_4 (SeparableCo (None, 22, 22, 32)   1312        activation_4[0][0]
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 22, 22, 32)   128         separable_conv2d_4[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 11, 11, 32)   512         add_1[0][0]
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 11, 11, 32)   0           batch_normalization_8[0][0]
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 11, 11, 32)   128         conv2d_4[0][0]
__________________________________________________________________________________________________
add_2 (Add)                     (None, 11, 11, 32)   0           max_pooling2d_2[0][0]
                                                                 batch_normalization_6[0][0]
__________________________________________________________________________________________________
separable_conv2d_5 (SeparableCo (None, 11, 11, 64)   2336        add_2[0][0]
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 11, 11, 64)   256         separable_conv2d_5[0][0]
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 11, 11, 64)   0           batch_normalization_10[0][0]
__________________________________________________________________________________________________
separable_conv2d_6 (SeparableCo (None, 11, 11, 64)   4672        activation_5[0][0]
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 11, 11, 64)   256         separable_conv2d_6[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 6, 6, 64)     2048        add_2[0][0]
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 6, 6, 64)     0           batch_normalization_11[0][0]
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 6, 6, 64)     256         conv2d_5[0][0]
__________________________________________________________________________________________________
add_3 (Add)                     (None, 6, 6, 64)     0           max_pooling2d_3[0][0]
                                                                 batch_normalization_9[0][0]
__________________________________________________________________________________________________
separable_conv2d_7 (SeparableCo (None, 6, 6, 128)    8768        add_3[0][0]
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 6, 6, 128)    512         separable_conv2d_7[0][0]
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 6, 6, 128)    0           batch_normalization_13[0][0]
__________________________________________________________________________________________________
separable_conv2d_8 (SeparableCo (None, 6, 6, 128)    17536       activation_6[0][0]
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 6, 6, 128)    512         separable_conv2d_8[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 3, 3, 128)    8192        add_3[0][0]
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 3, 3, 128)    0           batch_normalization_14[0][0]
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 3, 3, 128)    512         conv2d_6[0][0]
__________________________________________________________________________________________________
add_4 (Add)                     (None, 3, 3, 128)    0           max_pooling2d_4[0][0]
                                                                 batch_normalization_12[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 3, 3, 6)      6918        add_4[0][0]
__________________________________________________________________________________________________
global_average_pooling2d_1 (Glo (None, 6)            0           conv2d_7[0][0]
__________________________________________________________________________________________________
predictions (Activation)        (None, 6)            0           global_average_pooling2d_1[0][0]
==================================================================================================
Total params: 57,270
Trainable params: 55,798
Non-trainable params: 1,472
__________________________________________________________________________________________________
WARNING:tensorflow:From C:\Users\Bogdan\AppData\Local\Programs\Python\Python37\lib\site-packages\keras\backend\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

Epoch 1/110
2019-12-08 21:37:13.902865: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2019-12-08 21:37:14.909993: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Internal: Invoking ptxas not supported on Windows
Relying on driver to perform ptx compilation. This message will be only logged once.
2019-12-08 21:37:14.972281: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll
4241/4240 [==============================] - 304s 72ms/step - loss: 0.1260 - accuracy: 0.3867 - val_loss: 0.1129 - val_accuracy: 0.4597

Epoch 00001: val_loss improved from inf to 0.11285, saving model to models/_mini_XCEPTION.01-0.46.hdf5
Epoch 2/110
4241/4240 [==============================] - 299s 71ms/step - loss: 0.1066 - accuracy: 0.4904 - val_loss: 0.1029 - val_accuracy: 0.5199

Epoch 00002: val_loss improved from 0.11285 to 0.10291, saving model to models/_mini_XCEPTION.02-0.52.hdf5
Epoch 3/110
4241/4240 [==============================] - 300s 71ms/step - loss: 0.1017 - accuracy: 0.5190 - val_loss: 0.0983 - val_accuracy: 0.5395

Epoch 00003: val_loss improved from 0.10291 to 0.09826, saving model to models/_mini_XCEPTION.03-0.54.hdf5
Epoch 4/110
4241/4240 [==============================] - 299s 70ms/step - loss: 0.0992 - accuracy: 0.5334 - val_loss: 0.0945 - val_accuracy: 0.5597

Epoch 00004: val_loss improved from 0.09826 to 0.09448, saving model to models/_mini_XCEPTION.04-0.56.hdf5
Epoch 5/110
4241/4240 [==============================] - 301s 71ms/step - loss: 0.0969 - accuracy: 0.5463 - val_loss: 0.0972 - val_accuracy: 0.5456

Epoch 00005: val_loss did not improve from 0.09448
Epoch 6/110
4241/4240 [==============================] - 300s 71ms/step - loss: 0.0954 - accuracy: 0.5543 - val_loss: 0.0945 - val_accuracy: 0.5599

Epoch 00006: val_loss did not improve from 0.09448
Epoch 7/110
4241/4240 [==============================] - 300s 71ms/step - loss: 0.0944 - accuracy: 0.5623 - val_loss: 0.1004 - val_accuracy: 0.5276

Epoch 00007: val_loss did not improve from 0.09448
Epoch 8/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0934 - accuracy: 0.5668 - val_loss: 0.0918 - val_accuracy: 0.5754

Epoch 00008: val_loss improved from 0.09448 to 0.09181, saving model to models/_mini_XCEPTION.08-0.58.hdf5
Epoch 9/110
4241/4240 [==============================] - 300s 71ms/step - loss: 0.0923 - accuracy: 0.5736 - val_loss: 0.0910 - val_accuracy: 0.5770

Epoch 00009: val_loss improved from 0.09181 to 0.09105, saving model to models/_mini_XCEPTION.09-0.58.hdf5
Epoch 10/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0916 - accuracy: 0.5786 - val_loss: 0.0950 - val_accuracy: 0.5599

Epoch 00010: val_loss did not improve from 0.09105
Epoch 11/110
4241/4240 [==============================] - 301s 71ms/step - loss: 0.0908 - accuracy: 0.5819 - val_loss: 0.0931 - val_accuracy: 0.5678

Epoch 00011: val_loss did not improve from 0.09105
Epoch 12/110
4241/4240 [==============================] - 303s 71ms/step - loss: 0.0902 - accuracy: 0.5871 - val_loss: 0.0924 - val_accuracy: 0.5768

Epoch 00012: val_loss did not improve from 0.09105
Epoch 13/110
4241/4240 [==============================] - 301s 71ms/step - loss: 0.0898 - accuracy: 0.5880 - val_loss: 0.0880 - val_accuracy: 0.5976

Epoch 00013: val_loss improved from 0.09105 to 0.08800, saving model to models/_mini_XCEPTION.13-0.60.hdf5
Epoch 14/110
4241/4240 [==============================] - 303s 71ms/step - loss: 0.0891 - accuracy: 0.5930 - val_loss: 0.0871 - val_accuracy: 0.6043

Epoch 00014: val_loss improved from 0.08800 to 0.08713, saving model to models/_mini_XCEPTION.14-0.60.hdf5
Epoch 15/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0888 - accuracy: 0.5939 - val_loss: 0.0879 - val_accuracy: 0.6000

Epoch 00015: val_loss did not improve from 0.08713
Epoch 16/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0884 - accuracy: 0.5962 - val_loss: 0.0853 - val_accuracy: 0.6127

Epoch 00016: val_loss improved from 0.08713 to 0.08533, saving model to models/_mini_XCEPTION.16-0.61.hdf5
Epoch 17/110
4241/4240 [==============================] - 303s 71ms/step - loss: 0.0877 - accuracy: 0.6004 - val_loss: 0.0913 - val_accuracy: 0.5862

Epoch 00017: val_loss did not improve from 0.08533
Epoch 18/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0876 - accuracy: 0.6021 - val_loss: 0.0852 - val_accuracy: 0.6142

Epoch 00018: val_loss improved from 0.08533 to 0.08520, saving model to models/_mini_XCEPTION.18-0.61.hdf5
Epoch 19/110
4241/4240 [==============================] - 304s 72ms/step - loss: 0.0872 - accuracy: 0.6047 - val_loss: 0.0846 - val_accuracy: 0.6157

Epoch 00019: val_loss improved from 0.08520 to 0.08457, saving model to models/_mini_XCEPTION.19-0.62.hdf5
Epoch 20/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0868 - accuracy: 0.6066 - val_loss: 0.0857 - val_accuracy: 0.6117

Epoch 00020: val_loss did not improve from 0.08457
Epoch 21/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0866 - accuracy: 0.6078 - val_loss: 0.0842 - val_accuracy: 0.6212

Epoch 00021: val_loss improved from 0.08457 to 0.08424, saving model to models/_mini_XCEPTION.21-0.62.hdf5
Epoch 22/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0862 - accuracy: 0.6096 - val_loss: 0.0826 - val_accuracy: 0.6308

Epoch 00022: val_loss improved from 0.08424 to 0.08264, saving model to models/_mini_XCEPTION.22-0.63.hdf5
Epoch 23/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0858 - accuracy: 0.6123 - val_loss: 0.0860 - val_accuracy: 0.6092

Epoch 00023: val_loss did not improve from 0.08264
Epoch 24/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0858 - accuracy: 0.6106 - val_loss: 0.0841 - val_accuracy: 0.6237

Epoch 00024: val_loss did not improve from 0.08264
Epoch 25/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0855 - accuracy: 0.6125 - val_loss: 0.0835 - val_accuracy: 0.6277

Epoch 00025: val_loss did not improve from 0.08264
Epoch 26/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0850 - accuracy: 0.6157 - val_loss: 0.0832 - val_accuracy: 0.6252

Epoch 00026: val_loss did not improve from 0.08264
Epoch 27/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0850 - accuracy: 0.6163 - val_loss: 0.0841 - val_accuracy: 0.6217

Epoch 00027: val_loss did not improve from 0.08264
Epoch 28/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0845 - accuracy: 0.6198 - val_loss: 0.0834 - val_accuracy: 0.6231

Epoch 00028: val_loss did not improve from 0.08264
Epoch 29/110
4241/4240 [==============================] - 301s 71ms/step - loss: 0.0845 - accuracy: 0.6189 - val_loss: 0.0833 - val_accuracy: 0.6269

Epoch 00029: val_loss did not improve from 0.08264
Epoch 30/110
4241/4240 [==============================] - 301s 71ms/step - loss: 0.0843 - accuracy: 0.6201 - val_loss: 0.0828 - val_accuracy: 0.6270

Epoch 00030: val_loss did not improve from 0.08264
Epoch 31/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0843 - accuracy: 0.6202 - val_loss: 0.0844 - val_accuracy: 0.6145

Epoch 00031: val_loss did not improve from 0.08264
Epoch 32/110
4241/4240 [==============================] - 301s 71ms/step - loss: 0.0837 - accuracy: 0.6242 - val_loss: 0.0826 - val_accuracy: 0.6314

Epoch 00032: val_loss improved from 0.08264 to 0.08260, saving model to models/_mini_XCEPTION.32-0.63.hdf5
Epoch 33/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0836 - accuracy: 0.6237 - val_loss: 0.0835 - val_accuracy: 0.6260

Epoch 00033: val_loss did not improve from 0.08260
Epoch 34/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0836 - accuracy: 0.6241 - val_loss: 0.0820 - val_accuracy: 0.6314

Epoch 00034: val_loss improved from 0.08260 to 0.08204, saving model to models/_mini_XCEPTION.34-0.63.hdf5
Epoch 35/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0834 - accuracy: 0.6248 - val_loss: 0.0831 - val_accuracy: 0.6298

Epoch 00035: val_loss did not improve from 0.08204
Epoch 36/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0830 - accuracy: 0.6267 - val_loss: 0.0850 - val_accuracy: 0.6217

Epoch 00036: val_loss did not improve from 0.08204
Epoch 37/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0828 - accuracy: 0.6283 - val_loss: 0.0798 - val_accuracy: 0.6463

Epoch 00037: val_loss improved from 0.08204 to 0.07978, saving model to models/_mini_XCEPTION.37-0.65.hdf5
Epoch 38/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0828 - accuracy: 0.6290 - val_loss: 0.0818 - val_accuracy: 0.6320

Epoch 00038: val_loss did not improve from 0.07978
Epoch 39/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0825 - accuracy: 0.6301 - val_loss: 0.0829 - val_accuracy: 0.6305

Epoch 00039: val_loss did not improve from 0.07978
Epoch 40/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0823 - accuracy: 0.6314 - val_loss: 0.0798 - val_accuracy: 0.6430

Epoch 00040: val_loss did not improve from 0.07978
Epoch 41/110
4241/4240 [==============================] - 303s 71ms/step - loss: 0.0823 - accuracy: 0.6320 - val_loss: 0.0815 - val_accuracy: 0.6341

Epoch 00041: val_loss did not improve from 0.07978
Epoch 42/110
4241/4240 [==============================] - 301s 71ms/step - loss: 0.0822 - accuracy: 0.6322 - val_loss: 0.0805 - val_accuracy: 0.6394

Epoch 00042: val_loss did not improve from 0.07978
Epoch 43/110
4241/4240 [==============================] - 301s 71ms/step - loss: 0.0821 - accuracy: 0.6328 - val_loss: 0.0835 - val_accuracy: 0.6242

Epoch 00043: val_loss did not improve from 0.07978
Epoch 44/110
4241/4240 [==============================] - 301s 71ms/step - loss: 0.0820 - accuracy: 0.6333 - val_loss: 0.0845 - val_accuracy: 0.6158

Epoch 00044: val_loss did not improve from 0.07978
Epoch 45/110
4241/4240 [==============================] - 301s 71ms/step - loss: 0.0818 - accuracy: 0.6344 - val_loss: 0.0816 - val_accuracy: 0.6346

Epoch 00045: val_loss did not improve from 0.07978
Epoch 46/110
4241/4240 [==============================] - 301s 71ms/step - loss: 0.0816 - accuracy: 0.6349 - val_loss: 0.0821 - val_accuracy: 0.6340

Epoch 00046: val_loss did not improve from 0.07978
Epoch 47/110
4241/4240 [==============================] - 301s 71ms/step - loss: 0.0815 - accuracy: 0.6363 - val_loss: 0.0805 - val_accuracy: 0.6407

Epoch 00047: val_loss did not improve from 0.07978
Epoch 48/110
4241/4240 [==============================] - 301s 71ms/step - loss: 0.0814 - accuracy: 0.6362 - val_loss: 0.0803 - val_accuracy: 0.6396

Epoch 00048: val_loss did not improve from 0.07978
Epoch 49/110
4241/4240 [==============================] - 301s 71ms/step - loss: 0.0812 - accuracy: 0.6377 - val_loss: 0.0824 - val_accuracy: 0.6328

Epoch 00049: val_loss did not improve from 0.07978

Epoch 00049: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
Epoch 50/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0779 - accuracy: 0.6548 - val_loss: 0.0754 - val_accuracy: 0.6666

Epoch 00050: val_loss improved from 0.07978 to 0.07543, saving model to models/_mini_XCEPTION.50-0.67.hdf5
Epoch 51/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0769 - accuracy: 0.6590 - val_loss: 0.0752 - val_accuracy: 0.6663

Epoch 00051: val_loss improved from 0.07543 to 0.07525, saving model to models/_mini_XCEPTION.51-0.67.hdf5
Epoch 52/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0764 - accuracy: 0.6617 - val_loss: 0.0746 - val_accuracy: 0.6708

Epoch 00052: val_loss improved from 0.07525 to 0.07456, saving model to models/_mini_XCEPTION.52-0.67.hdf5
Epoch 53/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0762 - accuracy: 0.6624 - val_loss: 0.0744 - val_accuracy: 0.6707

Epoch 00053: val_loss improved from 0.07456 to 0.07444, saving model to models/_mini_XCEPTION.53-0.67.hdf5
Epoch 54/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0761 - accuracy: 0.6625 - val_loss: 0.0741 - val_accuracy: 0.6722

Epoch 00054: val_loss improved from 0.07444 to 0.07412, saving model to models/_mini_XCEPTION.54-0.67.hdf5
Epoch 55/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0758 - accuracy: 0.6629 - val_loss: 0.0745 - val_accuracy: 0.6712

Epoch 00055: val_loss did not improve from 0.07412
Epoch 56/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0756 - accuracy: 0.6655 - val_loss: 0.0741 - val_accuracy: 0.6739

Epoch 00056: val_loss improved from 0.07412 to 0.07406, saving model to models/_mini_XCEPTION.56-0.67.hdf5
Epoch 57/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0754 - accuracy: 0.6649 - val_loss: 0.0753 - val_accuracy: 0.6665

Epoch 00057: val_loss did not improve from 0.07406
Epoch 58/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0755 - accuracy: 0.6652 - val_loss: 0.0741 - val_accuracy: 0.6720

Epoch 00058: val_loss did not improve from 0.07406
Epoch 59/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0756 - accuracy: 0.6652 - val_loss: 0.0737 - val_accuracy: 0.6738

Epoch 00059: val_loss improved from 0.07406 to 0.07367, saving model to models/_mini_XCEPTION.59-0.67.hdf5
Epoch 60/110
4241/4240 [==============================] - 301s 71ms/step - loss: 0.0752 - accuracy: 0.6680 - val_loss: 0.0740 - val_accuracy: 0.6720

Epoch 00060: val_loss did not improve from 0.07367
Epoch 61/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0753 - accuracy: 0.6663 - val_loss: 0.0738 - val_accuracy: 0.6750

Epoch 00061: val_loss did not improve from 0.07367
Epoch 62/110
4241/4240 [==============================] - 301s 71ms/step - loss: 0.0751 - accuracy: 0.6671 - val_loss: 0.0745 - val_accuracy: 0.6702

Epoch 00062: val_loss did not improve from 0.07367
Epoch 63/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0751 - accuracy: 0.6670 - val_loss: 0.0741 - val_accuracy: 0.6706

Epoch 00063: val_loss did not improve from 0.07367
Epoch 64/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0748 - accuracy: 0.6694 - val_loss: 0.0736 - val_accuracy: 0.6748

Epoch 00064: val_loss improved from 0.07367 to 0.07362, saving model to models/_mini_XCEPTION.64-0.67.hdf5
Epoch 65/110
4241/4240 [==============================] - 301s 71ms/step - loss: 0.0750 - accuracy: 0.6676 - val_loss: 0.0737 - val_accuracy: 0.6756

Epoch 00065: val_loss did not improve from 0.07362
Epoch 66/110
4241/4240 [==============================] - 301s 71ms/step - loss: 0.0749 - accuracy: 0.6688 - val_loss: 0.0739 - val_accuracy: 0.6733

Epoch 00066: val_loss did not improve from 0.07362
Epoch 67/110
4241/4240 [==============================] - 301s 71ms/step - loss: 0.0749 - accuracy: 0.6677 - val_loss: 0.0734 - val_accuracy: 0.6754

Epoch 00067: val_loss improved from 0.07362 to 0.07341, saving model to models/_mini_XCEPTION.67-0.68.hdf5
Epoch 68/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0748 - accuracy: 0.6695 - val_loss: 0.0732 - val_accuracy: 0.6760

Epoch 00068: val_loss improved from 0.07341 to 0.07322, saving model to models/_mini_XCEPTION.68-0.68.hdf5
Epoch 69/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0746 - accuracy: 0.6708 - val_loss: 0.0736 - val_accuracy: 0.6737

Epoch 00069: val_loss did not improve from 0.07322
Epoch 70/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0747 - accuracy: 0.6692 - val_loss: 0.0739 - val_accuracy: 0.6728

Epoch 00070: val_loss did not improve from 0.07322
Epoch 71/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0745 - accuracy: 0.6695 - val_loss: 0.0747 - val_accuracy: 0.6701

Epoch 00071: val_loss did not improve from 0.07322
Epoch 72/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0747 - accuracy: 0.6703 - val_loss: 0.0731 - val_accuracy: 0.6780

Epoch 00072: val_loss improved from 0.07322 to 0.07313, saving model to models/_mini_XCEPTION.72-0.68.hdf5
Epoch 73/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0746 - accuracy: 0.6688 - val_loss: 0.0733 - val_accuracy: 0.6767

Epoch 00073: val_loss did not improve from 0.07313
Epoch 74/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0745 - accuracy: 0.6693 - val_loss: 0.0731 - val_accuracy: 0.6751

Epoch 00074: val_loss improved from 0.07313 to 0.07308, saving model to models/_mini_XCEPTION.74-0.68.hdf5
Epoch 75/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0747 - accuracy: 0.6686 - val_loss: 0.0735 - val_accuracy: 0.6746

Epoch 00075: val_loss did not improve from 0.07308
Epoch 76/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0745 - accuracy: 0.6699 - val_loss: 0.0730 - val_accuracy: 0.6776

Epoch 00076: val_loss improved from 0.07308 to 0.07299, saving model to models/_mini_XCEPTION.76-0.68.hdf5
Epoch 77/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0745 - accuracy: 0.6695 - val_loss: 0.0738 - val_accuracy: 0.6718

Epoch 00077: val_loss did not improve from 0.07299
Epoch 78/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0744 - accuracy: 0.6707 - val_loss: 0.0731 - val_accuracy: 0.6750

Epoch 00078: val_loss did not improve from 0.07299
Epoch 79/110
4241/4240 [==============================] - 305s 72ms/step - loss: 0.0744 - accuracy: 0.6709 - val_loss: 0.0734 - val_accuracy: 0.6751

Epoch 00079: val_loss did not improve from 0.07299
Epoch 80/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0743 - accuracy: 0.6707 - val_loss: 0.0731 - val_accuracy: 0.6766

Epoch 00080: val_loss did not improve from 0.07299
Epoch 81/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0743 - accuracy: 0.6710 - val_loss: 0.0730 - val_accuracy: 0.6776

Epoch 00081: val_loss did not improve from 0.07299
Epoch 82/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0741 - accuracy: 0.6722 - val_loss: 0.0745 - val_accuracy: 0.6697

Epoch 00082: val_loss did not improve from 0.07299
Epoch 83/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0743 - accuracy: 0.6714 - val_loss: 0.0733 - val_accuracy: 0.6732

Epoch 00083: val_loss did not improve from 0.07299
Epoch 84/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0743 - accuracy: 0.6707 - val_loss: 0.0733 - val_accuracy: 0.6752

Epoch 00084: val_loss did not improve from 0.07299
Epoch 85/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0742 - accuracy: 0.6711 - val_loss: 0.0734 - val_accuracy: 0.6744

Epoch 00085: val_loss did not improve from 0.07299
Epoch 86/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0742 - accuracy: 0.6721 - val_loss: 0.0732 - val_accuracy: 0.6767

Epoch 00086: val_loss did not improve from 0.07299

Epoch 00086: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
Epoch 87/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0738 - accuracy: 0.6739 - val_loss: 0.0727 - val_accuracy: 0.6794

Epoch 00087: val_loss improved from 0.07299 to 0.07270, saving model to models/_mini_XCEPTION.87-0.68.hdf5
Epoch 88/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0736 - accuracy: 0.6749 - val_loss: 0.0726 - val_accuracy: 0.6795

Epoch 00088: val_loss improved from 0.07270 to 0.07265, saving model to models/_mini_XCEPTION.88-0.68.hdf5
Epoch 89/110
4241/4240 [==============================] - 301s 71ms/step - loss: 0.0736 - accuracy: 0.6749 - val_loss: 0.0726 - val_accuracy: 0.6807

Epoch 00089: val_loss improved from 0.07265 to 0.07260, saving model to models/_mini_XCEPTION.89-0.68.hdf5
Epoch 90/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0735 - accuracy: 0.6749 - val_loss: 0.0727 - val_accuracy: 0.6802

Epoch 00090: val_loss did not improve from 0.07260
Epoch 91/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0735 - accuracy: 0.6763 - val_loss: 0.0725 - val_accuracy: 0.6800

Epoch 00091: val_loss improved from 0.07260 to 0.07255, saving model to models/_mini_XCEPTION.91-0.68.hdf5
Epoch 92/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0733 - accuracy: 0.6775 - val_loss: 0.0726 - val_accuracy: 0.6794

Epoch 00092: val_loss did not improve from 0.07255
Epoch 93/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0735 - accuracy: 0.6755 - val_loss: 0.0725 - val_accuracy: 0.6804

Epoch 00093: val_loss improved from 0.07255 to 0.07247, saving model to models/_mini_XCEPTION.93-0.68.hdf5
Epoch 94/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0735 - accuracy: 0.6746 - val_loss: 0.0725 - val_accuracy: 0.6798

Epoch 00094: val_loss did not improve from 0.07247
Epoch 95/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0737 - accuracy: 0.6739 - val_loss: 0.0727 - val_accuracy: 0.6790

Epoch 00095: val_loss did not improve from 0.07247
Epoch 96/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0735 - accuracy: 0.6750 - val_loss: 0.0725 - val_accuracy: 0.6795

Epoch 00096: val_loss did not improve from 0.07247
Epoch 97/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0734 - accuracy: 0.6750 - val_loss: 0.0725 - val_accuracy: 0.6809

Epoch 00097: val_loss did not improve from 0.07247
Epoch 98/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0736 - accuracy: 0.6744 - val_loss: 0.0724 - val_accuracy: 0.6809

Epoch 00098: val_loss improved from 0.07247 to 0.07245, saving model to models/_mini_XCEPTION.98-0.68.hdf5
Epoch 99/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0735 - accuracy: 0.6754 - val_loss: 0.0725 - val_accuracy: 0.6796

Epoch 00099: val_loss did not improve from 0.07245
Epoch 100/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0734 - accuracy: 0.6763 - val_loss: 0.0724 - val_accuracy: 0.6791

Epoch 00100: val_loss did not improve from 0.07245
Epoch 101/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0734 - accuracy: 0.6760 - val_loss: 0.0724 - val_accuracy: 0.6797

Epoch 00101: val_loss improved from 0.07245 to 0.07243, saving model to models/_mini_XCEPTION.101-0.68.hdf5
Epoch 102/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0734 - accuracy: 0.6761 - val_loss: 0.0726 - val_accuracy: 0.6793

Epoch 00102: val_loss did not improve from 0.07243
Epoch 103/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0733 - accuracy: 0.6760 - val_loss: 0.0725 - val_accuracy: 0.6807

Epoch 00103: val_loss did not improve from 0.07243
Epoch 104/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0733 - accuracy: 0.6764 - val_loss: 0.0726 - val_accuracy: 0.6793

Epoch 00104: val_loss did not improve from 0.07243
Epoch 105/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0733 - accuracy: 0.6762 - val_loss: 0.0724 - val_accuracy: 0.6792

Epoch 00105: val_loss improved from 0.07243 to 0.07242, saving model to models/_mini_XCEPTION.105-0.68.hdf5
Epoch 106/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0734 - accuracy: 0.6763 - val_loss: 0.0725 - val_accuracy: 0.6798

Epoch 00106: val_loss did not improve from 0.07242
Epoch 107/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0734 - accuracy: 0.6763 - val_loss: 0.0725 - val_accuracy: 0.6803

Epoch 00107: val_loss did not improve from 0.07242
Epoch 108/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0731 - accuracy: 0.6763 - val_loss: 0.0724 - val_accuracy: 0.6804

Epoch 00108: val_loss improved from 0.07242 to 0.07241, saving model to models/_mini_XCEPTION.108-0.68.hdf5
Epoch 109/110
4241/4240 [==============================] - 301s 71ms/step - loss: 0.0734 - accuracy: 0.6752 - val_loss: 0.0725 - val_accuracy: 0.6791

Epoch 00109: val_loss did not improve from 0.07241
Epoch 110/110
4241/4240 [==============================] - 302s 71ms/step - loss: 0.0731 - accuracy: 0.6776 - val_loss: 0.0723 - val_accuracy: 0.6804

Epoch 00110: val_loss improved from 0.07241 to 0.07231, saving model to models/_mini_XCEPTION.110-0.68.hdf5