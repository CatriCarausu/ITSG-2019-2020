Using TensorFlow backend.
2019-11-10 17:49:31.775342: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
C:\Users\Bogdan\Documents\GitHub\Intelligent-Tools-for-Social-Good\facial-expression-recognition-tf\facial-expression-recognition-tf\data_loader.py:23: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.
  emotions = pd.get_dummies(data['emotion']).as_matrix()
WARNING:tensorflow:From C:\Users\Bogdan\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\ops\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2019-11-10 17:50:07.184314: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2019-11-10 17:50:07.703149: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:
name: Quadro M520 major: 5 minor: 0 memoryClockRate(GHz): 1.176
pciBusID: 0000:02:00.0
2019-11-10 17:50:07.709287: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2019-11-10 17:50:07.717661: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll
2019-11-10 17:50:07.724889: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_100.dll
2019-11-10 17:50:07.729787: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_100.dll
2019-11-10 17:50:07.738009: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_100.dll
2019-11-10 17:50:07.745605: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_100.dll
2019-11-10 17:50:07.763309: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2019-11-10 17:50:07.769156: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-11-10 17:50:07.773219: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2019-11-10 17:50:07.780414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:
name: Quadro M520 major: 5 minor: 0 memoryClockRate(GHz): 1.176
pciBusID: 0000:02:00.0
2019-11-10 17:50:07.787726: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2019-11-10 17:50:07.792308: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll
2019-11-10 17:50:07.797363: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_100.dll
2019-11-10 17:50:07.802921: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_100.dll
2019-11-10 17:50:07.807416: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_100.dll
2019-11-10 17:50:07.812667: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_100.dll
2019-11-10 17:50:07.818053: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2019-11-10 17:50:07.823872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-11-10 17:50:09.110117: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-11-10 17:50:09.114305: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0
2019-11-10 17:50:09.116933: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N
2019-11-10 17:50:09.120905: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1384 MB memory) -> physical GPU (device: 0, name: Quadro M520, pci bus id: 0000:02:00.0, compute capability: 5.0)
WARNING:tensorflow:From C:\Users\Bogdan\AppData\Local\Programs\Python\Python37\lib\site-packages\keras\backend\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.

Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, 48, 48, 1)    0
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 46, 46, 8)    72          input_1[0][0]
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 46, 46, 8)    32          conv2d_1[0][0]
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 46, 46, 8)    0           batch_normalization_1[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 44, 44, 8)    576         activation_1[0][0]
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 44, 44, 8)    32          conv2d_2[0][0]
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 44, 44, 8)    0           batch_normalization_2[0][0]
__________________________________________________________________________________________________
separable_conv2d_1 (SeparableCo (None, 44, 44, 16)   200         activation_2[0][0]
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 44, 44, 16)   64          separable_conv2d_1[0][0]
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 44, 44, 16)   0           batch_normalization_4[0][0]
__________________________________________________________________________________________________
separable_conv2d_2 (SeparableCo (None, 44, 44, 16)   400         activation_3[0][0]
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 44, 44, 16)   64          separable_conv2d_2[0][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 22, 22, 16)   128         activation_2[0][0]
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 22, 22, 16)   0           batch_normalization_5[0][0]
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 22, 22, 16)   64          conv2d_3[0][0]
__________________________________________________________________________________________________
add_1 (Add)                     (None, 22, 22, 16)   0           max_pooling2d_1[0][0]
                                                                 batch_normalization_3[0][0]
__________________________________________________________________________________________________
separable_conv2d_3 (SeparableCo (None, 22, 22, 32)   656         add_1[0][0]
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 22, 22, 32)   128         separable_conv2d_3[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 22, 22, 32)   0           batch_normalization_7[0][0]
__________________________________________________________________________________________________
separable_conv2d_4 (SeparableCo (None, 22, 22, 32)   1312        activation_4[0][0]
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 22, 22, 32)   128         separable_conv2d_4[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 11, 11, 32)   512         add_1[0][0]
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 11, 11, 32)   0           batch_normalization_8[0][0]
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 11, 11, 32)   128         conv2d_4[0][0]
__________________________________________________________________________________________________
add_2 (Add)                     (None, 11, 11, 32)   0           max_pooling2d_2[0][0]
                                                                 batch_normalization_6[0][0]
__________________________________________________________________________________________________
separable_conv2d_5 (SeparableCo (None, 11, 11, 64)   2336        add_2[0][0]
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 11, 11, 64)   256         separable_conv2d_5[0][0]
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 11, 11, 64)   0           batch_normalization_10[0][0]
__________________________________________________________________________________________________
separable_conv2d_6 (SeparableCo (None, 11, 11, 64)   4672        activation_5[0][0]
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 11, 11, 64)   256         separable_conv2d_6[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 6, 6, 64)     2048        add_2[0][0]
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 6, 6, 64)     0           batch_normalization_11[0][0]
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 6, 6, 64)     256         conv2d_5[0][0]
__________________________________________________________________________________________________
add_3 (Add)                     (None, 6, 6, 64)     0           max_pooling2d_3[0][0]
                                                                 batch_normalization_9[0][0]
__________________________________________________________________________________________________
separable_conv2d_7 (SeparableCo (None, 6, 6, 128)    8768        add_3[0][0]
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 6, 6, 128)    512         separable_conv2d_7[0][0]
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 6, 6, 128)    0           batch_normalization_13[0][0]
__________________________________________________________________________________________________
separable_conv2d_8 (SeparableCo (None, 6, 6, 128)    17536       activation_6[0][0]
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 6, 6, 128)    512         separable_conv2d_8[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 3, 3, 128)    8192        add_3[0][0]
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 3, 3, 128)    0           batch_normalization_14[0][0]
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 3, 3, 128)    512         conv2d_6[0][0]
__________________________________________________________________________________________________
add_4 (Add)                     (None, 3, 3, 128)    0           max_pooling2d_4[0][0]
                                                                 batch_normalization_12[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 3, 3, 7)      8071        add_4[0][0]
__________________________________________________________________________________________________
global_average_pooling2d_1 (Glo (None, 7)            0           conv2d_7[0][0]
__________________________________________________________________________________________________
predictions (Activation)        (None, 7)            0           global_average_pooling2d_1[0][0]
==================================================================================================
Total params: 58,423
Trainable params: 56,951
Non-trainable params: 1,472
__________________________________________________________________________________________________
WARNING:tensorflow:From C:\Users\Bogdan\AppData\Local\Programs\Python\Python37\lib\site-packages\keras\backend\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

Epoch 1/110
2019-11-10 17:50:18.765174: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2019-11-10 17:50:19.706489: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Internal: Invoking ptxas not supported on Windows
Relying on driver to perform ptx compilation. This message will be only logged once.
2019-11-10 17:50:19.761722: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll
718/717 [==============================] - 59s 82ms/step - loss: 0.1382 - accuracy: 0.2971 - val_loss: 0.1138 - val_accuracy: 0.3499

Epoch 00001: val_loss improved from inf to 0.11383, saving model to models/_mini_XCEPTION.01-0.35.hdf5
Epoch 2/110
718/717 [==============================] - 57s 79ms/step - loss: 0.1061 - accuracy: 0.3978 - val_loss: 0.1029 - val_accuracy: 0.4218

Epoch 00002: val_loss improved from 0.11383 to 0.10292, saving model to models/_mini_XCEPTION.02-0.42.hdf5
Epoch 3/110
718/717 [==============================] - 55s 76ms/step - loss: 0.0986 - accuracy: 0.4500 - val_loss: 0.0955 - val_accuracy: 0.4700

Epoch 00003: val_loss improved from 0.10292 to 0.09549, saving model to models/_mini_XCEPTION.03-0.47.hdf5
Epoch 4/110
718/717 [==============================] - 55s 76ms/step - loss: 0.0946 - accuracy: 0.4744 - val_loss: 0.1010 - val_accuracy: 0.4382

Epoch 00004: val_loss did not improve from 0.09549
Epoch 5/110
718/717 [==============================] - 56s 77ms/step - loss: 0.0916 - accuracy: 0.4974 - val_loss: 0.0933 - val_accuracy: 0.4896

Epoch 00005: val_loss improved from 0.09549 to 0.09328, saving model to models/_mini_XCEPTION.05-0.49.hdf5
Epoch 6/110
718/717 [==============================] - 55s 77ms/step - loss: 0.0900 - accuracy: 0.5099 - val_loss: 0.0923 - val_accuracy: 0.5075

Epoch 00006: val_loss improved from 0.09328 to 0.09233, saving model to models/_mini_XCEPTION.06-0.51.hdf5
Epoch 7/110
718/717 [==============================] - 55s 77ms/step - loss: 0.0877 - accuracy: 0.5220 - val_loss: 0.0903 - val_accuracy: 0.5028

Epoch 00007: val_loss improved from 0.09233 to 0.09030, saving model to models/_mini_XCEPTION.07-0.50.hdf5
Epoch 8/110
718/717 [==============================] - 55s 77ms/step - loss: 0.0871 - accuracy: 0.5274 - val_loss: 0.0911 - val_accuracy: 0.5024

Epoch 00008: val_loss did not improve from 0.09030
Epoch 9/110
718/717 [==============================] - 55s 77ms/step - loss: 0.0857 - accuracy: 0.5356 - val_loss: 0.0908 - val_accuracy: 0.5056

Epoch 00009: val_loss did not improve from 0.09030
Epoch 10/110
718/717 [==============================] - 55s 77ms/step - loss: 0.0840 - accuracy: 0.5460 - val_loss: 0.0944 - val_accuracy: 0.4882

Epoch 00010: val_loss did not improve from 0.09030
Epoch 11/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0839 - accuracy: 0.5466 - val_loss: 0.0861 - val_accuracy: 0.5247

Epoch 00011: val_loss improved from 0.09030 to 0.08606, saving model to models/_mini_XCEPTION.11-0.52.hdf5
Epoch 12/110
718/717 [==============================] - 56s 77ms/step - loss: 0.0826 - accuracy: 0.5522 - val_loss: 0.0829 - val_accuracy: 0.5601

Epoch 00012: val_loss improved from 0.08606 to 0.08290, saving model to models/_mini_XCEPTION.12-0.56.hdf5
Epoch 13/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0816 - accuracy: 0.5632 - val_loss: 0.0890 - val_accuracy: 0.5118

Epoch 00013: val_loss did not improve from 0.08290
Epoch 14/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0810 - accuracy: 0.5664 - val_loss: 0.0836 - val_accuracy: 0.5535

Epoch 00014: val_loss did not improve from 0.08290
Epoch 15/110
718/717 [==============================] - 57s 79ms/step - loss: 0.0802 - accuracy: 0.5712 - val_loss: 0.0823 - val_accuracy: 0.5651

Epoch 00015: val_loss improved from 0.08290 to 0.08232, saving model to models/_mini_XCEPTION.15-0.57.hdf5
Epoch 16/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0802 - accuracy: 0.5728 - val_loss: 0.0842 - val_accuracy: 0.5428

Epoch 00016: val_loss did not improve from 0.08232
Epoch 17/110
718/717 [==============================] - 56s 77ms/step - loss: 0.0788 - accuracy: 0.5778 - val_loss: 0.0815 - val_accuracy: 0.5596

Epoch 00017: val_loss improved from 0.08232 to 0.08154, saving model to models/_mini_XCEPTION.17-0.56.hdf5
Epoch 18/110
718/717 [==============================] - 57s 79ms/step - loss: 0.0786 - accuracy: 0.5802 - val_loss: 0.0864 - val_accuracy: 0.5359

Epoch 00018: val_loss did not improve from 0.08154
Epoch 19/110
718/717 [==============================] - 55s 77ms/step - loss: 0.0782 - accuracy: 0.5843 - val_loss: 0.0864 - val_accuracy: 0.5444

Epoch 00019: val_loss did not improve from 0.08154
Epoch 20/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0773 - accuracy: 0.5866 - val_loss: 0.0812 - val_accuracy: 0.5658

Epoch 00020: val_loss improved from 0.08154 to 0.08119, saving model to models/_mini_XCEPTION.20-0.57.hdf5
Epoch 21/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0775 - accuracy: 0.5853 - val_loss: 0.0807 - val_accuracy: 0.5775

Epoch 00021: val_loss improved from 0.08119 to 0.08070, saving model to models/_mini_XCEPTION.21-0.58.hdf5
Epoch 22/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0768 - accuracy: 0.5920 - val_loss: 0.0808 - val_accuracy: 0.5719

Epoch 00022: val_loss did not improve from 0.08070
Epoch 23/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0761 - accuracy: 0.5965 - val_loss: 0.0854 - val_accuracy: 0.5395

Epoch 00023: val_loss did not improve from 0.08070
Epoch 24/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0765 - accuracy: 0.5929 - val_loss: 0.0796 - val_accuracy: 0.5831

Epoch 00024: val_loss improved from 0.08070 to 0.07964, saving model to models/_mini_XCEPTION.24-0.58.hdf5
Epoch 25/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0760 - accuracy: 0.5981 - val_loss: 0.0825 - val_accuracy: 0.5601

Epoch 00025: val_loss did not improve from 0.07964
Epoch 26/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0753 - accuracy: 0.6002 - val_loss: 0.0777 - val_accuracy: 0.5897

Epoch 00026: val_loss improved from 0.07964 to 0.07766, saving model to models/_mini_XCEPTION.26-0.59.hdf5
Epoch 27/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0753 - accuracy: 0.6041 - val_loss: 0.0776 - val_accuracy: 0.5892

Epoch 00027: val_loss improved from 0.07766 to 0.07762, saving model to models/_mini_XCEPTION.27-0.59.hdf5
Epoch 28/110
718/717 [==============================] - 57s 79ms/step - loss: 0.0747 - accuracy: 0.6047 - val_loss: 0.0831 - val_accuracy: 0.5740

Epoch 00028: val_loss did not improve from 0.07762
Epoch 29/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0741 - accuracy: 0.6099 - val_loss: 0.0787 - val_accuracy: 0.5904

Epoch 00029: val_loss did not improve from 0.07762
Epoch 30/110
718/717 [==============================] - 56s 77ms/step - loss: 0.0744 - accuracy: 0.6095 - val_loss: 0.0802 - val_accuracy: 0.5768

Epoch 00030: val_loss did not improve from 0.07762
Epoch 31/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0737 - accuracy: 0.6145 - val_loss: 0.0801 - val_accuracy: 0.5792

Epoch 00031: val_loss did not improve from 0.07762
Epoch 32/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0737 - accuracy: 0.6151 - val_loss: 0.0784 - val_accuracy: 0.5958

Epoch 00032: val_loss did not improve from 0.07762
Epoch 33/110
718/717 [==============================] - 57s 79ms/step - loss: 0.0733 - accuracy: 0.6162 - val_loss: 0.0770 - val_accuracy: 0.5937

Epoch 00033: val_loss improved from 0.07762 to 0.07705, saving model to models/_mini_XCEPTION.33-0.59.hdf5
Epoch 34/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0731 - accuracy: 0.6190 - val_loss: 0.0796 - val_accuracy: 0.5820

Epoch 00034: val_loss did not improve from 0.07705
Epoch 35/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0727 - accuracy: 0.6189 - val_loss: 0.0783 - val_accuracy: 0.5918

Epoch 00035: val_loss did not improve from 0.07705
Epoch 36/110
718/717 [==============================] - 56s 77ms/step - loss: 0.0729 - accuracy: 0.6181 - val_loss: 0.0812 - val_accuracy: 0.5643

Epoch 00036: val_loss did not improve from 0.07705
Epoch 37/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0724 - accuracy: 0.6235 - val_loss: 0.0758 - val_accuracy: 0.6015

Epoch 00037: val_loss improved from 0.07705 to 0.07584, saving model to models/_mini_XCEPTION.37-0.60.hdf5
Epoch 38/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0720 - accuracy: 0.6245 - val_loss: 0.0759 - val_accuracy: 0.6055

Epoch 00038: val_loss did not improve from 0.07584
Epoch 39/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0722 - accuracy: 0.6213 - val_loss: 0.0755 - val_accuracy: 0.6040

Epoch 00039: val_loss improved from 0.07584 to 0.07551, saving model to models/_mini_XCEPTION.39-0.60.hdf5
Epoch 40/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0719 - accuracy: 0.6234 - val_loss: 0.0771 - val_accuracy: 0.5968

Epoch 00040: val_loss did not improve from 0.07551
Epoch 41/110
718/717 [==============================] - 56s 77ms/step - loss: 0.0719 - accuracy: 0.6230 - val_loss: 0.0753 - val_accuracy: 0.6024

Epoch 00041: val_loss improved from 0.07551 to 0.07526, saving model to models/_mini_XCEPTION.41-0.60.hdf5
Epoch 42/110
718/717 [==============================] - 56s 79ms/step - loss: 0.0716 - accuracy: 0.6277 - val_loss: 0.0764 - val_accuracy: 0.5967

Epoch 00042: val_loss did not improve from 0.07526
Epoch 43/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0714 - accuracy: 0.6288 - val_loss: 0.0774 - val_accuracy: 0.5982

Epoch 00043: val_loss did not improve from 0.07526
Epoch 44/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0711 - accuracy: 0.6294 - val_loss: 0.0827 - val_accuracy: 0.5674

Epoch 00044: val_loss did not improve from 0.07526
Epoch 45/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0708 - accuracy: 0.6327 - val_loss: 0.0761 - val_accuracy: 0.5984

Epoch 00045: val_loss did not improve from 0.07526
Epoch 46/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0707 - accuracy: 0.6306 - val_loss: 0.0756 - val_accuracy: 0.6021

Epoch 00046: val_loss did not improve from 0.07526
Epoch 47/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0705 - accuracy: 0.6344 - val_loss: 0.0768 - val_accuracy: 0.6028

Epoch 00047: val_loss did not improve from 0.07526
Epoch 48/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0698 - accuracy: 0.6385 - val_loss: 0.0794 - val_accuracy: 0.5907

Epoch 00048: val_loss did not improve from 0.07526
Epoch 49/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0706 - accuracy: 0.6316 - val_loss: 0.0740 - val_accuracy: 0.6153

Epoch 00049: val_loss improved from 0.07526 to 0.07397, saving model to models/_mini_XCEPTION.49-0.62.hdf5
Epoch 50/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0700 - accuracy: 0.6366 - val_loss: 0.0754 - val_accuracy: 0.6083

Epoch 00050: val_loss did not improve from 0.07397
Epoch 51/110
718/717 [==============================] - 56s 77ms/step - loss: 0.0697 - accuracy: 0.6370 - val_loss: 0.0773 - val_accuracy: 0.6021

Epoch 00051: val_loss did not improve from 0.07397
Epoch 52/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0693 - accuracy: 0.6414 - val_loss: 0.0835 - val_accuracy: 0.5763

Epoch 00052: val_loss did not improve from 0.07397
Epoch 53/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0693 - accuracy: 0.6426 - val_loss: 0.0773 - val_accuracy: 0.5977

Epoch 00053: val_loss did not improve from 0.07397
Epoch 54/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0691 - accuracy: 0.6430 - val_loss: 0.0775 - val_accuracy: 0.6033

Epoch 00054: val_loss did not improve from 0.07397
Epoch 55/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0691 - accuracy: 0.6426 - val_loss: 0.0837 - val_accuracy: 0.5672

Epoch 00055: val_loss did not improve from 0.07397
Epoch 56/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0696 - accuracy: 0.6374 - val_loss: 0.0765 - val_accuracy: 0.6024

Epoch 00056: val_loss did not improve from 0.07397
Epoch 57/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0684 - accuracy: 0.6508 - val_loss: 0.0773 - val_accuracy: 0.5940

Epoch 00057: val_loss did not improve from 0.07397
Epoch 58/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0687 - accuracy: 0.6459 - val_loss: 0.0767 - val_accuracy: 0.6101

Epoch 00058: val_loss did not improve from 0.07397
Epoch 59/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0683 - accuracy: 0.6464 - val_loss: 0.0755 - val_accuracy: 0.6068

Epoch 00059: val_loss did not improve from 0.07397
Epoch 60/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0683 - accuracy: 0.6481 - val_loss: 0.0762 - val_accuracy: 0.6106

Epoch 00060: val_loss did not improve from 0.07397
Epoch 61/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0687 - accuracy: 0.6468 - val_loss: 0.0759 - val_accuracy: 0.6111

Epoch 00061: val_loss did not improve from 0.07397

Epoch 00061: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.
Epoch 62/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0655 - accuracy: 0.6662 - val_loss: 0.0714 - val_accuracy: 0.6348

Epoch 00062: val_loss improved from 0.07397 to 0.07136, saving model to models/_mini_XCEPTION.62-0.63.hdf5
Epoch 63/110
718/717 [==============================] - 56s 79ms/step - loss: 0.0639 - accuracy: 0.6716 - val_loss: 0.0714 - val_accuracy: 0.6273

Epoch 00063: val_loss did not improve from 0.07136
Epoch 64/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0631 - accuracy: 0.6795 - val_loss: 0.0715 - val_accuracy: 0.6278

Epoch 00064: val_loss did not improve from 0.07136
Epoch 65/110
718/717 [==============================] - 57s 79ms/step - loss: 0.0631 - accuracy: 0.6770 - val_loss: 0.0717 - val_accuracy: 0.6285

Epoch 00065: val_loss did not improve from 0.07136
Epoch 66/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0630 - accuracy: 0.6796 - val_loss: 0.0713 - val_accuracy: 0.6329

Epoch 00066: val_loss improved from 0.07136 to 0.07132, saving model to models/_mini_XCEPTION.66-0.63.hdf5
Epoch 67/110
718/717 [==============================] - 56s 79ms/step - loss: 0.0624 - accuracy: 0.6815 - val_loss: 0.0717 - val_accuracy: 0.6324

Epoch 00067: val_loss did not improve from 0.07132
Epoch 68/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0627 - accuracy: 0.6815 - val_loss: 0.0711 - val_accuracy: 0.6353

Epoch 00068: val_loss improved from 0.07132 to 0.07105, saving model to models/_mini_XCEPTION.68-0.64.hdf5
Epoch 69/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0619 - accuracy: 0.6864 - val_loss: 0.0713 - val_accuracy: 0.6367

Epoch 00069: val_loss did not improve from 0.07105
Epoch 70/110
718/717 [==============================] - 56s 79ms/step - loss: 0.0625 - accuracy: 0.6800 - val_loss: 0.0713 - val_accuracy: 0.6327

Epoch 00070: val_loss did not improve from 0.07105
Epoch 71/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0619 - accuracy: 0.6857 - val_loss: 0.0712 - val_accuracy: 0.6350

Epoch 00071: val_loss did not improve from 0.07105
Epoch 72/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0618 - accuracy: 0.6870 - val_loss: 0.0710 - val_accuracy: 0.6348

Epoch 00072: val_loss improved from 0.07105 to 0.07099, saving model to models/_mini_XCEPTION.72-0.63.hdf5
Epoch 73/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0620 - accuracy: 0.6838 - val_loss: 0.0711 - val_accuracy: 0.6369

Epoch 00073: val_loss did not improve from 0.07099
Epoch 74/110
718/717 [==============================] - 56s 79ms/step - loss: 0.0621 - accuracy: 0.6838 - val_loss: 0.0707 - val_accuracy: 0.6362

Epoch 00074: val_loss improved from 0.07099 to 0.07067, saving model to models/_mini_XCEPTION.74-0.64.hdf5
Epoch 75/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0616 - accuracy: 0.6876 - val_loss: 0.0709 - val_accuracy: 0.6411

Epoch 00075: val_loss did not improve from 0.07067
Epoch 76/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0615 - accuracy: 0.6865 - val_loss: 0.0708 - val_accuracy: 0.6397

Epoch 00076: val_loss did not improve from 0.07067
Epoch 77/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0617 - accuracy: 0.6867 - val_loss: 0.0717 - val_accuracy: 0.6301

Epoch 00077: val_loss did not improve from 0.07067
Epoch 78/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0615 - accuracy: 0.6894 - val_loss: 0.0709 - val_accuracy: 0.6324

Epoch 00078: val_loss did not improve from 0.07067
Epoch 79/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0613 - accuracy: 0.6911 - val_loss: 0.0712 - val_accuracy: 0.6372

Epoch 00079: val_loss did not improve from 0.07067
Epoch 80/110
718/717 [==============================] - 56s 79ms/step - loss: 0.0610 - accuracy: 0.6899 - val_loss: 0.0710 - val_accuracy: 0.6355

Epoch 00080: val_loss did not improve from 0.07067
Epoch 81/110
718/717 [==============================] - 57s 79ms/step - loss: 0.0612 - accuracy: 0.6874 - val_loss: 0.0710 - val_accuracy: 0.6334

Epoch 00081: val_loss did not improve from 0.07067
Epoch 82/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0610 - accuracy: 0.6903 - val_loss: 0.0716 - val_accuracy: 0.6336

Epoch 00082: val_loss did not improve from 0.07067
Epoch 83/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0612 - accuracy: 0.6907 - val_loss: 0.0711 - val_accuracy: 0.6378

Epoch 00083: val_loss did not improve from 0.07067
Epoch 84/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0610 - accuracy: 0.6884 - val_loss: 0.0710 - val_accuracy: 0.6346

Epoch 00084: val_loss did not improve from 0.07067
Epoch 85/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0609 - accuracy: 0.6897 - val_loss: 0.0717 - val_accuracy: 0.6332

Epoch 00085: val_loss did not improve from 0.07067
Epoch 86/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0607 - accuracy: 0.6930 - val_loss: 0.0714 - val_accuracy: 0.6320

Epoch 00086: val_loss did not improve from 0.07067

Epoch 00086: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
Epoch 87/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0608 - accuracy: 0.6915 - val_loss: 0.0709 - val_accuracy: 0.6365

Epoch 00087: val_loss did not improve from 0.07067
Epoch 88/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0606 - accuracy: 0.6937 - val_loss: 0.0708 - val_accuracy: 0.6371

Epoch 00088: val_loss did not improve from 0.07067
Epoch 89/110
718/717 [==============================] - 56s 79ms/step - loss: 0.0605 - accuracy: 0.6945 - val_loss: 0.0708 - val_accuracy: 0.6378

Epoch 00089: val_loss did not improve from 0.07067
Epoch 90/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0603 - accuracy: 0.6960 - val_loss: 0.0709 - val_accuracy: 0.6364

Epoch 00090: val_loss did not improve from 0.07067
Epoch 91/110
718/717 [==============================] - 57s 79ms/step - loss: 0.0602 - accuracy: 0.6945 - val_loss: 0.0708 - val_accuracy: 0.6372

Epoch 00091: val_loss did not improve from 0.07067
Epoch 92/110
718/717 [==============================] - 57s 79ms/step - loss: 0.0602 - accuracy: 0.6943 - val_loss: 0.0707 - val_accuracy: 0.6383

Epoch 00092: val_loss did not improve from 0.07067
Epoch 93/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0605 - accuracy: 0.6948 - val_loss: 0.0707 - val_accuracy: 0.6392

Epoch 00093: val_loss did not improve from 0.07067
Epoch 94/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0601 - accuracy: 0.6961 - val_loss: 0.0707 - val_accuracy: 0.6379

Epoch 00094: val_loss did not improve from 0.07067
Epoch 95/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0604 - accuracy: 0.6946 - val_loss: 0.0707 - val_accuracy: 0.6378

Epoch 00095: val_loss did not improve from 0.07067
Epoch 96/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0602 - accuracy: 0.6948 - val_loss: 0.0706 - val_accuracy: 0.6393

Epoch 00096: val_loss improved from 0.07067 to 0.07065, saving model to models/_mini_XCEPTION.96-0.64.hdf5
Epoch 97/110
718/717 [==============================] - 57s 79ms/step - loss: 0.0605 - accuracy: 0.6941 - val_loss: 0.0705 - val_accuracy: 0.6404

Epoch 00097: val_loss improved from 0.07065 to 0.07053, saving model to models/_mini_XCEPTION.97-0.64.hdf5
Epoch 98/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0606 - accuracy: 0.6942 - val_loss: 0.0706 - val_accuracy: 0.6390

Epoch 00098: val_loss did not improve from 0.07053
Epoch 99/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0595 - accuracy: 0.6998 - val_loss: 0.0706 - val_accuracy: 0.6395

Epoch 00099: val_loss did not improve from 0.07053
Epoch 100/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0599 - accuracy: 0.6988 - val_loss: 0.0705 - val_accuracy: 0.6402

Epoch 00100: val_loss did not improve from 0.07053
Epoch 101/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0601 - accuracy: 0.6946 - val_loss: 0.0707 - val_accuracy: 0.6395

Epoch 00101: val_loss did not improve from 0.07053
Epoch 102/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0604 - accuracy: 0.6960 - val_loss: 0.0706 - val_accuracy: 0.6388

Epoch 00102: val_loss did not improve from 0.07053
Epoch 103/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0603 - accuracy: 0.6950 - val_loss: 0.0705 - val_accuracy: 0.6369

Epoch 00103: val_loss did not improve from 0.07053
Epoch 104/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0603 - accuracy: 0.6926 - val_loss: 0.0705 - val_accuracy: 0.6393

Epoch 00104: val_loss did not improve from 0.07053
Epoch 105/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0604 - accuracy: 0.6945 - val_loss: 0.0706 - val_accuracy: 0.6376

Epoch 00105: val_loss did not improve from 0.07053
Epoch 106/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0599 - accuracy: 0.6978 - val_loss: 0.0705 - val_accuracy: 0.6395

Epoch 00106: val_loss improved from 0.07053 to 0.07052, saving model to models/_mini_XCEPTION.106-0.64.hdf5
Epoch 107/110
718/717 [==============================] - 57s 79ms/step - loss: 0.0604 - accuracy: 0.6928 - val_loss: 0.0706 - val_accuracy: 0.6393

Epoch 00107: val_loss did not improve from 0.07052
Epoch 108/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0601 - accuracy: 0.6971 - val_loss: 0.0705 - val_accuracy: 0.6395

Epoch 00108: val_loss improved from 0.07052 to 0.07048, saving model to models/_mini_XCEPTION.108-0.64.hdf5
Epoch 109/110
718/717 [==============================] - 57s 79ms/step - loss: 0.0604 - accuracy: 0.6919 - val_loss: 0.0707 - val_accuracy: 0.6383

Epoch 00109: val_loss did not improve from 0.07048

Epoch 00109: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.
Epoch 110/110
718/717 [==============================] - 56s 78ms/step - loss: 0.0598 - accuracy: 0.6962 - val_loss: 0.0706 - val_accuracy: 0.6398

Epoch 00110: val_loss did not improve from 0.07048
Press any key to continue . . .