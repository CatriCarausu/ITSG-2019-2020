Using TensorFlow backend.
2019-11-09 23:17:11.922219: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
C:\Users\Bogdan\Documents\GitHub\Intelligent-Tools-for-Social-Good\facial-expression-recognition-tf\facial-expression-recognition-tf\data_loader.py:23: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.
  emotions = pd.get_dummies(data['emotion']).as_matrix()
WARNING:tensorflow:From C:\Users\Bogdan\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\ops\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2019-11-09 23:17:52.188986: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2019-11-09 23:17:52.745572: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:
name: Quadro M520 major: 5 minor: 0 memoryClockRate(GHz): 1.176
pciBusID: 0000:02:00.0
2019-11-09 23:17:52.751815: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2019-11-09 23:17:52.759756: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll
2019-11-09 23:17:52.768360: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_100.dll
2019-11-09 23:17:52.773723: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_100.dll
2019-11-09 23:17:52.781788: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_100.dll
2019-11-09 23:17:52.788769: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_100.dll
2019-11-09 23:17:52.806431: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2019-11-09 23:17:52.812865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-11-09 23:17:52.817702: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2019-11-09 23:17:52.824188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:
name: Quadro M520 major: 5 minor: 0 memoryClockRate(GHz): 1.176
pciBusID: 0000:02:00.0
2019-11-09 23:17:52.830675: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2019-11-09 23:17:52.835188: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll
2019-11-09 23:17:52.839431: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_100.dll
2019-11-09 23:17:52.845678: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_100.dll
2019-11-09 23:17:52.850448: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_100.dll
2019-11-09 23:17:52.854980: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_100.dll
2019-11-09 23:17:52.859486: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2019-11-09 23:17:52.868665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-11-09 23:17:54.255133: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-11-09 23:17:54.261823: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0
2019-11-09 23:17:54.265896: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N
2019-11-09 23:17:54.270136: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1384 MB memory) -> physical GPU (device: 0, name: Quadro M520, pci bus id: 0000:02:00.0, compute capability: 5.0)
WARNING:tensorflow:From C:\Users\Bogdan\AppData\Local\Programs\Python\Python37\lib\site-packages\keras\backend\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.

Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, 48, 48, 1)    0
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 46, 46, 8)    72          input_1[0][0]
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 46, 46, 8)    32          conv2d_1[0][0]
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 46, 46, 8)    0           batch_normalization_1[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 44, 44, 8)    576         activation_1[0][0]
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 44, 44, 8)    32          conv2d_2[0][0]
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 44, 44, 8)    0           batch_normalization_2[0][0]
__________________________________________________________________________________________________
separable_conv2d_1 (SeparableCo (None, 44, 44, 16)   200         activation_2[0][0]
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 44, 44, 16)   64          separable_conv2d_1[0][0]
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 44, 44, 16)   0           batch_normalization_4[0][0]
__________________________________________________________________________________________________
separable_conv2d_2 (SeparableCo (None, 44, 44, 16)   400         activation_3[0][0]
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 44, 44, 16)   64          separable_conv2d_2[0][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 22, 22, 16)   128         activation_2[0][0]
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 22, 22, 16)   0           batch_normalization_5[0][0]
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 22, 22, 16)   64          conv2d_3[0][0]
__________________________________________________________________________________________________
add_1 (Add)                     (None, 22, 22, 16)   0           max_pooling2d_1[0][0]
                                                                 batch_normalization_3[0][0]
__________________________________________________________________________________________________
separable_conv2d_3 (SeparableCo (None, 22, 22, 32)   656         add_1[0][0]
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 22, 22, 32)   128         separable_conv2d_3[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 22, 22, 32)   0           batch_normalization_7[0][0]
__________________________________________________________________________________________________
separable_conv2d_4 (SeparableCo (None, 22, 22, 32)   1312        activation_4[0][0]
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 22, 22, 32)   128         separable_conv2d_4[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 11, 11, 32)   512         add_1[0][0]
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 11, 11, 32)   0           batch_normalization_8[0][0]
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 11, 11, 32)   128         conv2d_4[0][0]
__________________________________________________________________________________________________
add_2 (Add)                     (None, 11, 11, 32)   0           max_pooling2d_2[0][0]
                                                                 batch_normalization_6[0][0]
__________________________________________________________________________________________________
separable_conv2d_5 (SeparableCo (None, 11, 11, 64)   2336        add_2[0][0]
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 11, 11, 64)   256         separable_conv2d_5[0][0]
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 11, 11, 64)   0           batch_normalization_10[0][0]
__________________________________________________________________________________________________
separable_conv2d_6 (SeparableCo (None, 11, 11, 64)   4672        activation_5[0][0]
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 11, 11, 64)   256         separable_conv2d_6[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 6, 6, 64)     2048        add_2[0][0]
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 6, 6, 64)     0           batch_normalization_11[0][0]
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 6, 6, 64)     256         conv2d_5[0][0]
__________________________________________________________________________________________________
add_3 (Add)                     (None, 6, 6, 64)     0           max_pooling2d_3[0][0]
                                                                 batch_normalization_9[0][0]
__________________________________________________________________________________________________
separable_conv2d_7 (SeparableCo (None, 6, 6, 128)    8768        add_3[0][0]
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 6, 6, 128)    512         separable_conv2d_7[0][0]
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 6, 6, 128)    0           batch_normalization_13[0][0]
__________________________________________________________________________________________________
separable_conv2d_8 (SeparableCo (None, 6, 6, 128)    17536       activation_6[0][0]
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 6, 6, 128)    512         separable_conv2d_8[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 3, 3, 128)    8192        add_3[0][0]
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 3, 3, 128)    0           batch_normalization_14[0][0]
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 3, 3, 128)    512         conv2d_6[0][0]
__________________________________________________________________________________________________
add_4 (Add)                     (None, 3, 3, 128)    0           max_pooling2d_4[0][0]
                                                                 batch_normalization_12[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 3, 3, 7)      8071        add_4[0][0]
__________________________________________________________________________________________________
global_average_pooling2d_1 (Glo (None, 7)            0           conv2d_7[0][0]
__________________________________________________________________________________________________
predictions (Activation)        (None, 7)            0           global_average_pooling2d_1[0][0]
==================================================================================================
Total params: 58,423
Trainable params: 56,951
Non-trainable params: 1,472
__________________________________________________________________________________________________
WARNING:tensorflow:From C:\Users\Bogdan\AppData\Local\Programs\Python\Python37\lib\site-packages\keras\backend\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

Epoch 1/110
2019-11-09 23:18:05.787049: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2019-11-09 23:18:06.833935: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Internal: Invoking ptxas not supported on Windows
Relying on driver to perform ptx compilation. This message will be only logged once.
2019-11-09 23:18:06.893714: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll
718/717 [==============================] - 59s 82ms/step - loss: 1.8108 - accuracy: 0.3134 - val_loss: 2.0787 - val_accuracy: 0.3347

Epoch 00001: val_loss improved from inf to 2.07872, saving model to models/_mini_XCEPTION.01-0.33.hdf5
Epoch 2/110
718/717 [==============================] - 54s 75ms/step - loss: 1.5525 - accuracy: 0.4221 - val_loss: 1.5308 - val_accuracy: 0.4500

Epoch 00002: val_loss improved from 2.07872 to 1.53076, saving model to models/_mini_XCEPTION.02-0.45.hdf5
Epoch 3/110
718/717 [==============================] - 53s 74ms/step - loss: 1.4353 - accuracy: 0.4650 - val_loss: 1.4763 - val_accuracy: 0.4493

Epoch 00003: val_loss improved from 1.53076 to 1.47631, saving model to models/_mini_XCEPTION.03-0.45.hdf5
Epoch 4/110
718/717 [==============================] - 54s 76ms/step - loss: 1.3612 - accuracy: 0.4921 - val_loss: 1.4872 - val_accuracy: 0.4648

Epoch 00004: val_loss did not improve from 1.47631
Epoch 5/110
718/717 [==============================] - 53s 74ms/step - loss: 1.3095 - accuracy: 0.5137 - val_loss: 1.3390 - val_accuracy: 0.5061

Epoch 00005: val_loss improved from 1.47631 to 1.33904, saving model to models/_mini_XCEPTION.05-0.51.hdf5
Epoch 6/110
718/717 [==============================] - 54s 75ms/step - loss: 1.2720 - accuracy: 0.5242 - val_loss: 1.3143 - val_accuracy: 0.5171

Epoch 00006: val_loss improved from 1.33904 to 1.31430, saving model to models/_mini_XCEPTION.06-0.52.hdf5
Epoch 7/110
718/717 [==============================] - 54s 75ms/step - loss: 1.2491 - accuracy: 0.5298 - val_loss: 1.2996 - val_accuracy: 0.5319

Epoch 00007: val_loss improved from 1.31430 to 1.29957, saving model to models/_mini_XCEPTION.07-0.53.hdf5
Epoch 8/110
718/717 [==============================] - 55s 76ms/step - loss: 1.2155 - accuracy: 0.5438 - val_loss: 1.3849 - val_accuracy: 0.4991

Epoch 00008: val_loss did not improve from 1.29957
Epoch 9/110
718/717 [==============================] - 53s 74ms/step - loss: 1.1949 - accuracy: 0.5529 - val_loss: 1.2561 - val_accuracy: 0.5348

Epoch 00009: val_loss improved from 1.29957 to 1.25608, saving model to models/_mini_XCEPTION.09-0.53.hdf5
Epoch 10/110
718/717 [==============================] - 53s 74ms/step - loss: 1.1822 - accuracy: 0.5551 - val_loss: 1.2750 - val_accuracy: 0.5291

Epoch 00010: val_loss did not improve from 1.25608
Epoch 11/110
718/717 [==============================] - 53s 74ms/step - loss: 1.1719 - accuracy: 0.5633 - val_loss: 1.2432 - val_accuracy: 0.5369

Epoch 00011: val_loss improved from 1.25608 to 1.24318, saving model to models/_mini_XCEPTION.11-0.54.hdf5
Epoch 12/110
718/717 [==============================] - 53s 74ms/step - loss: 1.1590 - accuracy: 0.5655 - val_loss: 1.2476 - val_accuracy: 0.5392

Epoch 00012: val_loss did not improve from 1.24318
Epoch 13/110
718/717 [==============================] - 54s 75ms/step - loss: 1.1422 - accuracy: 0.5724 - val_loss: 1.2539 - val_accuracy: 0.5524

Epoch 00013: val_loss did not improve from 1.24318
Epoch 14/110
718/717 [==============================] - 55s 77ms/step - loss: 1.1326 - accuracy: 0.5735 - val_loss: 1.2098 - val_accuracy: 0.5491

Epoch 00014: val_loss improved from 1.24318 to 1.20984, saving model to models/_mini_XCEPTION.14-0.55.hdf5
Epoch 15/110
718/717 [==============================] - 53s 74ms/step - loss: 1.1167 - accuracy: 0.5805 - val_loss: 1.1696 - val_accuracy: 0.5617

Epoch 00015: val_loss improved from 1.20984 to 1.16956, saving model to models/_mini_XCEPTION.15-0.56.hdf5
Epoch 16/110
718/717 [==============================] - 54s 75ms/step - loss: 1.1090 - accuracy: 0.5854 - val_loss: 1.2009 - val_accuracy: 0.5627

Epoch 00016: val_loss did not improve from 1.16956
Epoch 17/110
718/717 [==============================] - 53s 74ms/step - loss: 1.1004 - accuracy: 0.5896 - val_loss: 1.2433 - val_accuracy: 0.5484

Epoch 00017: val_loss did not improve from 1.16956
Epoch 18/110
718/717 [==============================] - 54s 75ms/step - loss: 1.0948 - accuracy: 0.5878 - val_loss: 1.1885 - val_accuracy: 0.5653

Epoch 00018: val_loss did not improve from 1.16956
Epoch 19/110
718/717 [==============================] - 54s 75ms/step - loss: 1.0897 - accuracy: 0.5908 - val_loss: 1.1768 - val_accuracy: 0.5653

Epoch 00019: val_loss did not improve from 1.16956
Epoch 20/110
718/717 [==============================] - 53s 74ms/step - loss: 1.0800 - accuracy: 0.5960 - val_loss: 1.1416 - val_accuracy: 0.5843

Epoch 00020: val_loss improved from 1.16956 to 1.14163, saving model to models/_mini_XCEPTION.20-0.58.hdf5
Epoch 21/110
718/717 [==============================] - 53s 74ms/step - loss: 1.0715 - accuracy: 0.5985 - val_loss: 1.1304 - val_accuracy: 0.5792

Epoch 00021: val_loss improved from 1.14163 to 1.13041, saving model to models/_mini_XCEPTION.21-0.58.hdf5
Epoch 22/110
718/717 [==============================] - 53s 74ms/step - loss: 1.0617 - accuracy: 0.5974 - val_loss: 1.1513 - val_accuracy: 0.5744

Epoch 00022: val_loss did not improve from 1.13041
Epoch 23/110
718/717 [==============================] - 54s 75ms/step - loss: 1.0585 - accuracy: 0.6048 - val_loss: 1.1308 - val_accuracy: 0.5839

Epoch 00023: val_loss did not improve from 1.13041
Epoch 24/110
718/717 [==============================] - 53s 74ms/step - loss: 1.0573 - accuracy: 0.6070 - val_loss: 1.1821 - val_accuracy: 0.5749

Epoch 00024: val_loss did not improve from 1.13041
Epoch 25/110
718/717 [==============================] - 53s 74ms/step - loss: 1.0482 - accuracy: 0.6071 - val_loss: 1.1482 - val_accuracy: 0.5759

Epoch 00025: val_loss did not improve from 1.13041
Epoch 26/110
718/717 [==============================] - 54s 75ms/step - loss: 1.0436 - accuracy: 0.6113 - val_loss: 1.1585 - val_accuracy: 0.5740

Epoch 00026: val_loss did not improve from 1.13041
Epoch 27/110
718/717 [==============================] - 54s 75ms/step - loss: 1.0448 - accuracy: 0.6110 - val_loss: 1.0962 - val_accuracy: 0.5937

Epoch 00027: val_loss improved from 1.13041 to 1.09624, saving model to models/_mini_XCEPTION.27-0.59.hdf5
Epoch 28/110
718/717 [==============================] - 53s 74ms/step - loss: 1.0351 - accuracy: 0.6158 - val_loss: 1.1480 - val_accuracy: 0.5728

Epoch 00028: val_loss did not improve from 1.09624
Epoch 29/110
718/717 [==============================] - 54s 75ms/step - loss: 1.0269 - accuracy: 0.6184 - val_loss: 1.0956 - val_accuracy: 0.5944

Epoch 00029: val_loss improved from 1.09624 to 1.09561, saving model to models/_mini_XCEPTION.29-0.59.hdf5
Epoch 30/110
718/717 [==============================] - 54s 75ms/step - loss: 1.0162 - accuracy: 0.6184 - val_loss: 1.1743 - val_accuracy: 0.5754

Epoch 00030: val_loss did not improve from 1.09561
Epoch 31/110
718/717 [==============================] - 54s 75ms/step - loss: 1.0229 - accuracy: 0.6160 - val_loss: 1.1307 - val_accuracy: 0.5785

Epoch 00031: val_loss did not improve from 1.09561
Epoch 32/110
718/717 [==============================] - 54s 75ms/step - loss: 1.0123 - accuracy: 0.6209 - val_loss: 1.1329 - val_accuracy: 0.5947

Epoch 00032: val_loss did not improve from 1.09561
Epoch 33/110
718/717 [==============================] - 54s 75ms/step - loss: 1.0065 - accuracy: 0.6208 - val_loss: 1.2011 - val_accuracy: 0.5592

Epoch 00033: val_loss did not improve from 1.09561
Epoch 34/110
718/717 [==============================] - 53s 74ms/step - loss: 1.0094 - accuracy: 0.6245 - val_loss: 1.1267 - val_accuracy: 0.5846

Epoch 00034: val_loss did not improve from 1.09561
Epoch 35/110
718/717 [==============================] - 53s 74ms/step - loss: 1.0007 - accuracy: 0.6280 - val_loss: 1.1008 - val_accuracy: 0.6050

Epoch 00035: val_loss did not improve from 1.09561
Epoch 36/110
718/717 [==============================] - 54s 75ms/step - loss: 0.9944 - accuracy: 0.6292 - val_loss: 1.1149 - val_accuracy: 0.5927

Epoch 00036: val_loss did not improve from 1.09561
Epoch 37/110
718/717 [==============================] - 53s 74ms/step - loss: 0.9960 - accuracy: 0.6286 - val_loss: 1.0931 - val_accuracy: 0.5958

Epoch 00037: val_loss improved from 1.09561 to 1.09306, saving model to models/_mini_XCEPTION.37-0.60.hdf5
Epoch 38/110
718/717 [==============================] - 53s 74ms/step - loss: 0.9896 - accuracy: 0.6330 - val_loss: 1.1567 - val_accuracy: 0.5914

Epoch 00038: val_loss did not improve from 1.09306
Epoch 39/110
718/717 [==============================] - 53s 74ms/step - loss: 0.9888 - accuracy: 0.6327 - val_loss: 1.1141 - val_accuracy: 0.5907

Epoch 00039: val_loss did not improve from 1.09306
Epoch 40/110
718/717 [==============================] - 53s 74ms/step - loss: 0.9854 - accuracy: 0.6347 - val_loss: 1.1586 - val_accuracy: 0.5796

Epoch 00040: val_loss did not improve from 1.09306
Epoch 41/110
718/717 [==============================] - 53s 74ms/step - loss: 0.9777 - accuracy: 0.6385 - val_loss: 1.1354 - val_accuracy: 0.5932

Epoch 00041: val_loss did not improve from 1.09306
Epoch 42/110
718/717 [==============================] - 54s 75ms/step - loss: 0.9763 - accuracy: 0.6311 - val_loss: 1.0988 - val_accuracy: 0.6010

Epoch 00042: val_loss did not improve from 1.09306
Epoch 43/110
718/717 [==============================] - 54s 75ms/step - loss: 0.9765 - accuracy: 0.6337 - val_loss: 1.1228 - val_accuracy: 0.5944

Epoch 00043: val_loss did not improve from 1.09306
Epoch 44/110
718/717 [==============================] - 53s 74ms/step - loss: 0.9675 - accuracy: 0.6376 - val_loss: 1.0730 - val_accuracy: 0.6022

Epoch 00044: val_loss improved from 1.09306 to 1.07302, saving model to models/_mini_XCEPTION.44-0.60.hdf5
Epoch 45/110
718/717 [==============================] - 53s 74ms/step - loss: 0.9663 - accuracy: 0.6415 - val_loss: 1.2383 - val_accuracy: 0.5462

Epoch 00045: val_loss did not improve from 1.07302
Epoch 46/110
718/717 [==============================] - 53s 74ms/step - loss: 0.9584 - accuracy: 0.6447 - val_loss: 1.1481 - val_accuracy: 0.5758

Epoch 00046: val_loss did not improve from 1.07302
Epoch 47/110
718/717 [==============================] - 54s 75ms/step - loss: 0.9569 - accuracy: 0.6422 - val_loss: 1.1032 - val_accuracy: 0.5937

Epoch 00047: val_loss did not improve from 1.07302
Epoch 48/110
718/717 [==============================] - 53s 74ms/step - loss: 0.9551 - accuracy: 0.6437 - val_loss: 1.1027 - val_accuracy: 0.6017

Epoch 00048: val_loss did not improve from 1.07302
Epoch 49/110
718/717 [==============================] - 52s 73ms/step - loss: 0.9594 - accuracy: 0.6426 - val_loss: 1.0867 - val_accuracy: 0.6061

Epoch 00049: val_loss did not improve from 1.07302
Epoch 50/110
718/717 [==============================] - 52s 73ms/step - loss: 0.9569 - accuracy: 0.6412 - val_loss: 1.0806 - val_accuracy: 0.6118

Epoch 00050: val_loss did not improve from 1.07302
Epoch 51/110
718/717 [==============================] - 53s 73ms/step - loss: 0.9482 - accuracy: 0.6480 - val_loss: 1.0952 - val_accuracy: 0.5974

Epoch 00051: val_loss did not improve from 1.07302
Epoch 52/110
718/717 [==============================] - 53s 73ms/step - loss: 0.9470 - accuracy: 0.6460 - val_loss: 1.0924 - val_accuracy: 0.6008

Epoch 00052: val_loss did not improve from 1.07302
Epoch 53/110
718/717 [==============================] - 53s 74ms/step - loss: 0.9450 - accuracy: 0.6492 - val_loss: 1.1128 - val_accuracy: 0.5953

Epoch 00053: val_loss did not improve from 1.07302
Epoch 54/110
718/717 [==============================] - 54s 75ms/step - loss: 0.9433 - accuracy: 0.6488 - val_loss: 1.1242 - val_accuracy: 0.5900

Epoch 00054: val_loss did not improve from 1.07302
Epoch 55/110
718/717 [==============================] - 53s 73ms/step - loss: 0.9425 - accuracy: 0.6502 - val_loss: 1.0849 - val_accuracy: 0.5974

Epoch 00055: val_loss did not improve from 1.07302
Epoch 56/110
718/717 [==============================] - 52s 73ms/step - loss: 0.9412 - accuracy: 0.6516 - val_loss: 1.1030 - val_accuracy: 0.6052

Epoch 00056: val_loss did not improve from 1.07302

Epoch 00056: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.
Epoch 57/110
718/717 [==============================] - 52s 73ms/step - loss: 0.9066 - accuracy: 0.6641 - val_loss: 1.0554 - val_accuracy: 0.6224

Epoch 00057: val_loss improved from 1.07302 to 1.05542, saving model to models/_mini_XCEPTION.57-0.62.hdf5
Epoch 58/110
718/717 [==============================] - 52s 73ms/step - loss: 0.8970 - accuracy: 0.6684 - val_loss: 1.0459 - val_accuracy: 0.6254

Epoch 00058: val_loss improved from 1.05542 to 1.04587, saving model to models/_mini_XCEPTION.58-0.63.hdf5
Epoch 59/110
718/717 [==============================] - 52s 73ms/step - loss: 0.8904 - accuracy: 0.6705 - val_loss: 1.0428 - val_accuracy: 0.6240

Epoch 00059: val_loss improved from 1.04587 to 1.04282, saving model to models/_mini_XCEPTION.59-0.62.hdf5
Epoch 60/110
718/717 [==============================] - 53s 73ms/step - loss: 0.8884 - accuracy: 0.6703 - val_loss: 1.0404 - val_accuracy: 0.6230

Epoch 00060: val_loss improved from 1.04282 to 1.04042, saving model to models/_mini_XCEPTION.60-0.62.hdf5
Epoch 61/110
718/717 [==============================] - 53s 73ms/step - loss: 0.8779 - accuracy: 0.6744 - val_loss: 1.0360 - val_accuracy: 0.6256

Epoch 00061: val_loss improved from 1.04042 to 1.03598, saving model to models/_mini_XCEPTION.61-0.63.hdf5
Epoch 62/110
718/717 [==============================] - 52s 73ms/step - loss: 0.8783 - accuracy: 0.6763 - val_loss: 1.0351 - val_accuracy: 0.6252

Epoch 00062: val_loss improved from 1.03598 to 1.03512, saving model to models/_mini_XCEPTION.62-0.63.hdf5
Epoch 63/110
718/717 [==============================] - 53s 74ms/step - loss: 0.8730 - accuracy: 0.6769 - val_loss: 1.0346 - val_accuracy: 0.6254

Epoch 00063: val_loss improved from 1.03512 to 1.03458, saving model to models/_mini_XCEPTION.63-0.63.hdf5
Epoch 64/110
718/717 [==============================] - 52s 73ms/step - loss: 0.8798 - accuracy: 0.6697 - val_loss: 1.0328 - val_accuracy: 0.6266

Epoch 00064: val_loss improved from 1.03458 to 1.03284, saving model to models/_mini_XCEPTION.64-0.63.hdf5
Epoch 65/110
718/717 [==============================] - 53s 74ms/step - loss: 0.8761 - accuracy: 0.6736 - val_loss: 1.0340 - val_accuracy: 0.6263

Epoch 00065: val_loss did not improve from 1.03284
Epoch 66/110
718/717 [==============================] - 53s 74ms/step - loss: 0.8733 - accuracy: 0.6751 - val_loss: 1.0326 - val_accuracy: 0.6252

Epoch 00066: val_loss improved from 1.03284 to 1.03263, saving model to models/_mini_XCEPTION.66-0.63.hdf5
Epoch 67/110
718/717 [==============================] - 52s 73ms/step - loss: 0.8707 - accuracy: 0.6759 - val_loss: 1.0322 - val_accuracy: 0.6261

Epoch 00067: val_loss improved from 1.03263 to 1.03219, saving model to models/_mini_XCEPTION.67-0.63.hdf5
Epoch 68/110
718/717 [==============================] - 52s 73ms/step - loss: 0.8717 - accuracy: 0.6743 - val_loss: 1.0317 - val_accuracy: 0.6254

Epoch 00068: val_loss improved from 1.03219 to 1.03169, saving model to models/_mini_XCEPTION.68-0.63.hdf5
Epoch 69/110
718/717 [==============================] - 52s 73ms/step - loss: 0.8663 - accuracy: 0.6778 - val_loss: 1.0322 - val_accuracy: 0.6268

Epoch 00069: val_loss did not improve from 1.03169
Epoch 70/110
718/717 [==============================] - 52s 73ms/step - loss: 0.8661 - accuracy: 0.6765 - val_loss: 1.0313 - val_accuracy: 0.6268

Epoch 00070: val_loss improved from 1.03169 to 1.03131, saving model to models/_mini_XCEPTION.70-0.63.hdf5
Epoch 71/110
718/717 [==============================] - 54s 75ms/step - loss: 0.8648 - accuracy: 0.6795 - val_loss: 1.0318 - val_accuracy: 0.6264

Epoch 00071: val_loss did not improve from 1.03131
Epoch 72/110
718/717 [==============================] - 52s 73ms/step - loss: 0.8666 - accuracy: 0.6788 - val_loss: 1.0301 - val_accuracy: 0.6271

Epoch 00072: val_loss improved from 1.03131 to 1.03012, saving model to models/_mini_XCEPTION.72-0.63.hdf5
Epoch 73/110
718/717 [==============================] - 53s 73ms/step - loss: 0.8677 - accuracy: 0.6767 - val_loss: 1.0297 - val_accuracy: 0.6271

Epoch 00073: val_loss improved from 1.03012 to 1.02969, saving model to models/_mini_XCEPTION.73-0.63.hdf5
Epoch 74/110
718/717 [==============================] - 52s 73ms/step - loss: 0.8671 - accuracy: 0.6805 - val_loss: 1.0315 - val_accuracy: 0.6273

Epoch 00074: val_loss did not improve from 1.02969
Epoch 75/110
718/717 [==============================] - 52s 73ms/step - loss: 0.8645 - accuracy: 0.6804 - val_loss: 1.0290 - val_accuracy: 0.6284

Epoch 00075: val_loss improved from 1.02969 to 1.02899, saving model to models/_mini_XCEPTION.75-0.63.hdf5
Epoch 76/110
718/717 [==============================] - 53s 74ms/step - loss: 0.8580 - accuracy: 0.6830 - val_loss: 1.0294 - val_accuracy: 0.6282

Epoch 00076: val_loss did not improve from 1.02899
Epoch 77/110
718/717 [==============================] - 53s 73ms/step - loss: 0.8633 - accuracy: 0.6825 - val_loss: 1.0297 - val_accuracy: 0.6264

Epoch 00077: val_loss did not improve from 1.02899
Epoch 78/110
718/717 [==============================] - 53s 73ms/step - loss: 0.8657 - accuracy: 0.6796 - val_loss: 1.0296 - val_accuracy: 0.6273

Epoch 00078: val_loss did not improve from 1.02899
Epoch 79/110
718/717 [==============================] - 53s 73ms/step - loss: 0.8623 - accuracy: 0.6788 - val_loss: 1.0302 - val_accuracy: 0.6275

Epoch 00079: val_loss did not improve from 1.02899
Epoch 80/110
718/717 [==============================] - 53s 74ms/step - loss: 0.8544 - accuracy: 0.6825 - val_loss: 1.0299 - val_accuracy: 0.6275

Epoch 00080: val_loss did not improve from 1.02899
Epoch 81/110
718/717 [==============================] - 53s 73ms/step - loss: 0.8584 - accuracy: 0.6837 - val_loss: 1.0299 - val_accuracy: 0.6271

Epoch 00081: val_loss did not improve from 1.02899
Epoch 82/110
718/717 [==============================] - 52s 73ms/step - loss: 0.8584 - accuracy: 0.6796 - val_loss: 1.0300 - val_accuracy: 0.6290

Epoch 00082: val_loss did not improve from 1.02899
Epoch 83/110
718/717 [==============================] - 52s 73ms/step - loss: 0.8579 - accuracy: 0.6809 - val_loss: 1.0308 - val_accuracy: 0.6275

Epoch 00083: val_loss did not improve from 1.02899
Epoch 84/110
718/717 [==============================] - 52s 73ms/step - loss: 0.8529 - accuracy: 0.6846 - val_loss: 1.0298 - val_accuracy: 0.6280

Epoch 00084: val_loss did not improve from 1.02899
Epoch 85/110
718/717 [==============================] - 52s 73ms/step - loss: 0.8572 - accuracy: 0.6832 - val_loss: 1.0300 - val_accuracy: 0.6290

Epoch 00085: val_loss did not improve from 1.02899
Epoch 86/110
718/717 [==============================] - 52s 73ms/step - loss: 0.8535 - accuracy: 0.6828 - val_loss: 1.0307 - val_accuracy: 0.6297

Epoch 00086: val_loss did not improve from 1.02899
Epoch 87/110
718/717 [==============================] - 53s 73ms/step - loss: 0.8540 - accuracy: 0.6858 - val_loss: 1.0301 - val_accuracy: 0.6292

Epoch 00087: val_loss did not improve from 1.02899

Epoch 00087: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-07.
Epoch 88/110
718/717 [==============================] - 52s 73ms/step - loss: 0.8539 - accuracy: 0.6849 - val_loss: 1.0309 - val_accuracy: 0.6290

Epoch 00088: val_loss did not improve from 1.02899
Epoch 89/110
718/717 [==============================] - 52s 73ms/step - loss: 0.8605 - accuracy: 0.6792 - val_loss: 1.0305 - val_accuracy: 0.6299

Epoch 00089: val_loss did not improve from 1.02899
Epoch 90/110
718/717 [==============================] - 53s 73ms/step - loss: 0.8611 - accuracy: 0.6815 - val_loss: 1.0305 - val_accuracy: 0.6303

Epoch 00090: val_loss did not improve from 1.02899
Epoch 91/110
718/717 [==============================] - 52s 73ms/step - loss: 0.8543 - accuracy: 0.6825 - val_loss: 1.0304 - val_accuracy: 0.6303

Epoch 00091: val_loss did not improve from 1.02899
Epoch 92/110
718/717 [==============================] - 52s 73ms/step - loss: 0.8535 - accuracy: 0.6831 - val_loss: 1.0301 - val_accuracy: 0.6296

Epoch 00092: val_loss did not improve from 1.02899
Epoch 93/110
718/717 [==============================] - 53s 74ms/step - loss: 0.8542 - accuracy: 0.6838 - val_loss: 1.0309 - val_accuracy: 0.6301

Epoch 00093: val_loss did not improve from 1.02899
Epoch 94/110
718/717 [==============================] - 53s 73ms/step - loss: 0.8573 - accuracy: 0.6814 - val_loss: 1.0303 - val_accuracy: 0.6294

Epoch 00094: val_loss did not improve from 1.02899
Epoch 95/110
718/717 [==============================] - 53s 73ms/step - loss: 0.8535 - accuracy: 0.6828 - val_loss: 1.0304 - val_accuracy: 0.6292

Epoch 00095: val_loss did not improve from 1.02899
Epoch 96/110
718/717 [==============================] - 52s 73ms/step - loss: 0.8531 - accuracy: 0.6858 - val_loss: 1.0298 - val_accuracy: 0.6287

Epoch 00096: val_loss did not improve from 1.02899
Epoch 97/110
718/717 [==============================] - 53s 73ms/step - loss: 0.8548 - accuracy: 0.6839 - val_loss: 1.0307 - val_accuracy: 0.6296

Epoch 00097: val_loss did not improve from 1.02899
Epoch 98/110
718/717 [==============================] - 52s 73ms/step - loss: 0.8520 - accuracy: 0.6872 - val_loss: 1.0304 - val_accuracy: 0.6292

Epoch 00098: val_loss did not improve from 1.02899
Epoch 99/110
718/717 [==============================] - 53s 73ms/step - loss: 0.8561 - accuracy: 0.6825 - val_loss: 1.0301 - val_accuracy: 0.6299

Epoch 00099: val_loss did not improve from 1.02899

Epoch 00099: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.
Epoch 100/110
718/717 [==============================] - 53s 73ms/step - loss: 0.8538 - accuracy: 0.6827 - val_loss: 1.0297 - val_accuracy: 0.6301

Epoch 00100: val_loss did not improve from 1.02899
Epoch 101/110
718/717 [==============================] - 52s 73ms/step - loss: 0.8556 - accuracy: 0.6808 - val_loss: 1.0305 - val_accuracy: 0.6287

Epoch 00101: val_loss did not improve from 1.02899
Epoch 102/110
718/717 [==============================] - 53s 73ms/step - loss: 0.8548 - accuracy: 0.6789 - val_loss: 1.0304 - val_accuracy: 0.6310

Epoch 00102: val_loss did not improve from 1.02899
Epoch 103/110
718/717 [==============================] - 52s 73ms/step - loss: 0.8561 - accuracy: 0.6817 - val_loss: 1.0306 - val_accuracy: 0.6289

Epoch 00103: val_loss did not improve from 1.02899
Epoch 104/110
718/717 [==============================] - 52s 73ms/step - loss: 0.8558 - accuracy: 0.6845 - val_loss: 1.0299 - val_accuracy: 0.6294

Epoch 00104: val_loss did not improve from 1.02899
Epoch 105/110
718/717 [==============================] - 52s 73ms/step - loss: 0.8546 - accuracy: 0.6845 - val_loss: 1.0308 - val_accuracy: 0.6299

Epoch 00105: val_loss did not improve from 1.02899
Epoch 106/110
718/717 [==============================] - 52s 73ms/step - loss: 0.8566 - accuracy: 0.6818 - val_loss: 1.0309 - val_accuracy: 0.6296

Epoch 00106: val_loss did not improve from 1.02899
Epoch 107/110
718/717 [==============================] - 52s 73ms/step - loss: 0.8569 - accuracy: 0.6827 - val_loss: 1.0304 - val_accuracy: 0.6303

Epoch 00107: val_loss did not improve from 1.02899
Epoch 108/110
718/717 [==============================] - 52s 73ms/step - loss: 0.8552 - accuracy: 0.6830 - val_loss: 1.0311 - val_accuracy: 0.6290

Epoch 00108: val_loss did not improve from 1.02899
Epoch 109/110
718/717 [==============================] - 52s 73ms/step - loss: 0.8531 - accuracy: 0.6842 - val_loss: 1.0300 - val_accuracy: 0.6296

Epoch 00109: val_loss did not improve from 1.02899
Epoch 110/110
718/717 [==============================] - 53s 73ms/step - loss: 0.8573 - accuracy: 0.6810 - val_loss: 1.0305 - val_accuracy: 0.6294

Epoch 00110: val_loss did not improve from 1.02899
Press any key to continue . . .