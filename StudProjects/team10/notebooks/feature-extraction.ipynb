{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'append-df.ipynb', 'CNN-Pets-Adoption-classification.ipynb', 'CNN-Pets-Adoption-regression.ipynb', 'CNNTest-classification.ipynb', 'CNNTest-regression.ipynb', 'combine-all.ipynb', 'draft_models.ipynb', 'extract-pureBreed.ipynb', 'feature-extraction.ipynb', 'feature-importance.ipynb', 'features-test.csv', 'features-train.csv', 'image-feature-extraction-and-image-quality.ipynb', 'merge-image-features-to-csv.ipynb', 'Model_Combined_Csv_Senti.ipynb', 'petfinder-adoption-prediction', 'petfinder-adoption-prediction.zip', 'score_solution.py', 'split_dataset.py', 'voting_solution-V2.ipynb']\n"
     ]
    }
   ],
   "source": [
    "############################################################################################################\n",
    "# Feature extraction from sentiment and metadata \n",
    "# df_v2 = train.csv + basic sentiment data + image features\n",
    "# df_v3 = df_v2 + countRescuer\n",
    "# df_v4 = df_v3 + pure breed\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numbers\n",
    "\n",
    "# Set the correct path for yourself:\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"./\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "############################ Feature Merging ##############################################\n",
    "############################################################################################\n",
    "\n",
    "#helper function: get sum of var\n",
    "def summ_var(obs,var_type):\n",
    "    ''' obs: a dictionary read from json file. \n",
    "        var_type: either 'magnitude' or 'score'. '''    \n",
    "    sum_var=0\n",
    "    for i in obs['sentences']:\n",
    "        sum_var+=i['sentiment'][var_type]\n",
    "    return sum_var\n",
    "\n",
    "#helper function: get entities (labels, tags) from description as string\n",
    "def get_tags(obs):\n",
    "    final_tags = []\n",
    "    ent = obs['entities'] # list of entities\n",
    "    for i in ent:\n",
    "        final_tags.append(i['name'])\n",
    "    joined = ' '.join(final_tags)\n",
    "    return joined \n",
    "\n",
    "def merge_sentiment(folder):\n",
    "    \n",
    "    files = [f for f in os.listdir(folder)]\n",
    "    record=[]\n",
    "    for j in files:\n",
    "        file = folder+j\n",
    "        \n",
    "        with open (file, 'r',encoding=\"utf8\") as f: #read all the files\n",
    "            obs = json.load(f)\n",
    "        if type(obs)==list: data=obs[0] #sometimes json's are as list, sometimes dictionary,\n",
    "        else: data=obs                 #here it is handled\n",
    "        j=j[:-5]    #We want PetID without \".json\" part\n",
    "        doc_mag=data['documentSentiment']['magnitude'] #whole description magnitude!\n",
    "        doc_score=data['documentSentiment']['score']  #whole desription score!\n",
    "        sent_count = len(data['sentences']) #how many sentences?\n",
    "        sen1_mag=data['sentences'][0]['sentiment']['magnitude'] #magnitude of the 1st sentence\n",
    "        sen1_score=data['sentences'][0]['sentiment']['score'] #score of the 1st sentence\n",
    "        sum_mag=summ_var(data,'magnitude') #sum magnitude of all sentences\n",
    "        sum_score=summ_var(data,'score') #sum score of all sentences\n",
    "        tags = get_tags(data) #returns a string with comma-separated tags\n",
    "        record.append([j,doc_mag,doc_score,sent_count,sen1_mag,sen1_score, sum_mag, sum_score, tags])\n",
    "    \n",
    "    #Our DataFrame with columns as below:\n",
    "    rec_df = pd.DataFrame(record,columns=['PetID','docMagnitude','doc_score','sent_count', 'sen1_magnitude','sen1_score', 'sum_mag', 'sum_score','sent_tags'])   \n",
    "    \n",
    "    # Correct the magnitude and sum of magnitude, dividing it by sentences number\n",
    "    rec_df['doc_mag_corr']= rec_df.docMagnitude / rec_df.sent_count\n",
    "    rec_df['sum_mag_corr']= rec_df.sum_mag / rec_df.sent_count\n",
    "    \n",
    "    del rec_df['docMagnitude']\n",
    "    # does pet have english description\n",
    "    sentiment = rec_df #we ignore Adoption Speed merging\n",
    "    sentiment['has_eng_description']=np.where(sentiment.doc_mag_corr.isnull()==True, 0,1)\n",
    "    \n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_metadata(folder, pics):\n",
    "    \n",
    "    files = [f for f in os.listdir(folder)]\n",
    "    record_img=[]\n",
    "    for j in files:\n",
    "        file = folder+j\n",
    "      \n",
    "        with open (file, 'r',encoding=\"utf8\") as f: #read all the files\n",
    "            obs = json.load(f)\n",
    "        if type(obs)==list: data=obs[0] #sometimes json's are as list, sometimes dictionary,\n",
    "        else: data=obs    #here it is handled        \n",
    "        \n",
    "        if pics != \"ALL\": condition = \"j[-6] == '1'\" #do we want just 1st pic or all\n",
    "        else: condition = 'True'\n",
    "        flag=eval(condition)\n",
    "            \n",
    "        if flag==True:\n",
    "        \n",
    "        #1. CropHintsAnnotation:\n",
    "    \n",
    "        #boundingPoly \t: The bounding polygon for the crop region. \n",
    "        #The coordinates of the bounding box are in the original image's scale.\n",
    "        #confidence \t:Confidence of this being a salient region. Range [0, 1].\n",
    "        #importanceFraction \t: Fraction of importance of this salient region with respect to the original image. \n",
    "\n",
    "            img_bound_polygon_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n",
    "            img_bound_polygon_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n",
    "            img_confidence = data['cropHintsAnnotation'] ['cropHints'] [0] ['confidence']\n",
    "            try: \n",
    "                img_imp_fract = data['cropHintsAnnotation'] ['cropHints'] [0] ['importanceFraction']\n",
    "            except KeyError:\n",
    "                img_imp_fract = 0\n",
    "                \n",
    "        #2. imagePropertiesAnnotation:\n",
    "        \n",
    "            try:\n",
    "                domcol_r = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['red']\n",
    "                domcol_g = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['green']\n",
    "                domcol_b = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['blue']\n",
    "            except KeyError:\n",
    "                domcol_r,domcol_g,domcol_b = 0,0,0\n",
    "        \n",
    "        # 3. labelAnnotations: tags, like 'dog', 'puppy' , with topicality score.\n",
    "            file_keys = list(data.keys())\n",
    "        \n",
    "            if 'labelAnnotations' in file_keys:\n",
    "                file_annots = data['labelAnnotations'][:int(len(data['labelAnnotations']) * 0.3)]\n",
    "                file_top_score = np.asarray([x['score'] for x in file_annots]).mean()\n",
    "                file_top_desc = [x['description'] for x in file_annots]\n",
    "            else:\n",
    "                file_top_score = np.nan\n",
    "                file_top_desc = ['']\n",
    "            meta_tags = ' '.join(file_top_desc)  \n",
    "            \n",
    "        # 4. imagePropertiesAnnotation\n",
    "            file_colors = data['imagePropertiesAnnotation']['dominantColors']['colors']\n",
    "            file_crops = data['cropHintsAnnotation']['cropHints']\n",
    "\n",
    "            file_color_score = np.asarray([x['score'] for x in file_colors]).mean()\n",
    "            file_color_pixelfrac = np.asarray([x['pixelFraction'] for x in file_colors]).mean()\n",
    "\n",
    "            file_crop_conf = np.asarray([x['confidence'] for x in file_crops]).mean()\n",
    "        \n",
    "            if 'importanceFraction' in file_crops[0].keys():\n",
    "                file_crop_importance = np.asarray([x['importanceFraction'] for x in file_crops]).mean()\n",
    "            else:\n",
    "                file_crop_importance = np.nan\n",
    "  \n",
    "            PetID = j[:-7] #PetID\n",
    "            PetID=PetID.replace('-','') #just in case\n",
    "            if pics == \"ALL\": \n",
    "                PetID_pic=j[:-5]    #We want PetID with \"-picture number.json\" part\n",
    "                pic_no = int(j[-6])\n",
    "            row = [PetID, img_bound_polygon_x,img_bound_polygon_y, img_confidence, \n",
    "                   img_imp_fract, domcol_r, domcol_g, domcol_b, file_top_score, meta_tags,\n",
    "                   file_color_score, file_color_pixelfrac, file_crop_conf, file_crop_importance]\n",
    "            if pics == \"ALL\": \n",
    "                row.append(PetID_pic) \n",
    "                row.append(pic_no) \n",
    "            record_img.append(row)\n",
    "            row=[]  #clear\n",
    "    \n",
    "    columns =  ['PetID', 'img_bound_polygon_x','img_bound_polygon_y','img_confidence',\n",
    "               'img_imp_fract','domcol_r','domcol_g','domcol_b','file_top_score', 'img_tags',\n",
    "               'file_color_score', 'file_color_pixelfrac', 'file_crop_conf', 'file_crop_importance']\n",
    "    \n",
    "    if pics == \"ALL\":  \n",
    "        columns.append('PetID_pic')\n",
    "        columns.append('pic_no')\n",
    "    \n",
    "    rec_img_df = pd.DataFrame(record_img, columns = columns)\n",
    "    \n",
    "    rec_img_df.set_index('PetID')\n",
    "    \n",
    "    return rec_img_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_data(ds_type, directory, pics):\n",
    "    ''' Extracts features: text sentiment and metadata images.\n",
    "        Merges it with CSV and returns the final file.\n",
    "        ds_type - train or test, which file are you separating\n",
    "        directory - dir to place with test.csv, train.csv and folders with metadata\n",
    "    '''\n",
    "    #-1.Set the directory\n",
    "    os.chdir(directory)\n",
    "    folder_sentiment= ds_type + \"_sentiment/\" \n",
    "    folder_meta= ds_type + \"_metadata/\"\n",
    "    \n",
    "    dataset = pd.read_csv(ds_type + '/' + ds_type + '.csv')\n",
    "    \n",
    "    # 2. GET DESCRIPTION (TEXT) SENTIMENT DATA\n",
    "    sentiment_df = merge_sentiment(folder_sentiment)\n",
    "\n",
    "    # 3. GET IMAGES METADATA & MERGE IT WITH THE REST\n",
    "    \n",
    "    metadata_df = merge_metadata(folder_meta, pics)\n",
    "    # 4. MERGE ALL FILES\n",
    "    dataset_meta = pd.merge(dataset, metadata_df,  how=\"outer\", left_on='PetID' , right_on='PetID', suffixes=('_img','_dataset'))\n",
    "    \n",
    "    dataset_meta_senti = pd.merge(dataset_meta, sentiment_df, how=\"left\", left_on='PetID' , right_on='PetID', suffixes=('_every','_sent'))\n",
    "      \n",
    "    # 5. CLEAR FINAL DATASET\n",
    "    df = dataset_meta_senti\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "############################ Feature Aggregation ###########################################\n",
    "############################################################################################\n",
    "\n",
    "def merge_tags(array):\n",
    "    try:\n",
    "        res = ' '.join(array)\n",
    "    except TypeError:\n",
    "        res = ' '.join(str(array))\n",
    "    return res\n",
    "\n",
    "def aggregate_features(data):\n",
    "    ''' returns features per pet, not per picture '''\n",
    "    #we separate columns connected to image: only those will be summed. Rest: only averaged\n",
    "    img_cols = ['img_bound_polygon_x','img_bound_polygon_y','img_confidence',\n",
    "               'img_imp_fract','domcol_r','domcol_g','domcol_b','file_top_score', 'img_tags',\n",
    "               'file_color_score', 'file_color_pixelfrac', 'file_crop_conf', 'file_crop_importance']\n",
    "    sent_cols = [ 'docMagnitude', 'doc_score', 'sent_count', 'sen1_magnitude', 'sen1_score', 'sum_mag',\n",
    "                 'sum_score', 'sent_tags', 'doc_mag_corr', 'sum_mag_corr', 'has_eng_description']\n",
    "    added_cols = img_cols + sent_cols\n",
    "    final_df = pd.DataFrame()\n",
    "    cols = list(data)\n",
    "    for col in cols:\n",
    "        if isinstance(data[col][0], numbers.Number): #if numeric, we aggregate:\n",
    "            column = data.groupby(['PetID'])[col].mean()\n",
    "            if col in added_cols: #name: either 'normal' or with suffix\n",
    "                final_df[col+'_Mean'] = column\n",
    "            else:\n",
    "                final_df[col] = column\n",
    "            if col in img_cols: #also sum\n",
    "                column2 = data.groupby(['PetID'])[col].sum()\n",
    "                final_df[col+'_Sum'] = column2\n",
    "        \n",
    "        else:  # text object   \n",
    "            column = data.groupby(['PetID'])[col].unique()\n",
    "            if col=='PetID': column = [x[0] for x in column]\n",
    "            else:\n",
    "                column = column.map(merge_tags)\n",
    "            final_df[col]= column\n",
    "            \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_rescuer(df):\n",
    "    rescuer_count = df.groupby(['RescuerID'])['PetID'].count().reset_index()\n",
    "    rescuer_count.columns = ['RescuerID', 'RescuerID_COUNT']\n",
    "    df = df.merge(rescuer_count, how='left', on='RescuerID')\n",
    "    del df[\"RescuerID\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pure_breed(df):\n",
    "    extra_df = pd.DataFrame() \n",
    "    count = 0\n",
    "    \n",
    "    for i in range (0, len(df)):\n",
    "        pure_breed_1 = df.iloc[i]['Breed1']\n",
    "        pure_breed_2 = df.iloc[i]['Breed2']\n",
    "        \n",
    "        if (pure_breed_1 != pure_breed_2 \n",
    "        or int(pure_breed_2) == 307 # mixed breed\n",
    "        or int(pure_breed_1) == 307\n",
    "        or pure_breed_2 == None):\n",
    "            is_pure_breed = 0\n",
    "        else:\n",
    "            count = count + 1\n",
    "            is_pure_breed = 1\n",
    "            \n",
    "        extra_df = extra_df.append({'PureBreed': is_pure_breed}, ignore_index = True)\n",
    "    \n",
    "    #print('Total pure breed: ', count)\n",
    "    assert(len(df) == len(extra_df))\n",
    "    df = df.join(extra_df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_PetID(ID_source, y):\n",
    "    ID_source=ID_source.reset_index()\n",
    "    try:\n",
    "        y=y.reset_index(drop=True)\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    y_df = pd.DataFrame(y)\n",
    "    y_final = pd.concat([ID_source['PetID'],y_df],ignore_index=True, axis=1)\n",
    "    y_final.columns=[\"PetID\",\"AdoptionSpeed\"]\n",
    "    return y_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\eprog\\python\\lib\\site-packages\\ipykernel_launcher.py:48: RuntimeWarning: Mean of empty slice.\n",
      "c:\\eprog\\python\\lib\\site-packages\\numpy\\core\\_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# TRAIN\n",
    "\n",
    "# Obtain CSV file merged with other features (text sentiment and metadata images).\n",
    "df=get_all_data(ds_type=\"train\",directory=\"./petfinder-adoption-prediction/\",pics=\"ALL\")\n",
    "# Return features per pet, not per picture.\n",
    "df_v2 = aggregate_features(df)\n",
    "df_v3 = count_rescuer(df_v2)\n",
    "df_v4 = pure_breed(df_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE FINAL TRAIN/TEST CSV DATA\n",
    "\n",
    "df_v4.to_csv('final_train.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
